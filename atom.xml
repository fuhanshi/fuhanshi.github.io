<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>一个大学狗的日常</title>
  
  <subtitle>活着就要折腾，一秒不折腾就会天崩地裂</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-01-11T11:05:27.011Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>凡希</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>股价数据分析及预测</title>
    <link href="http://yoursite.com/2019/01/11/%E8%82%A1%E4%BB%B7%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%8F%8A%E9%A2%84%E6%B5%8B/"/>
    <id>http://yoursite.com/2019/01/11/股价数据分析及预测/</id>
    <published>2019-01-11T10:40:04.000Z</published>
    <updated>2019-01-11T11:05:27.011Z</updated>
    
    <content type="html"><![CDATA[<h3 id="一、导入处理完毕的数据"><a href="#一、导入处理完毕的数据" class="headerlink" title="一、导入处理完毕的数据"></a>一、导入处理完毕的数据</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data=pd.read_csv(<span class="string">'/home/fanxi/桌面/mydata.csv'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data</span><br></pre></td></tr></table></figure><div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>code1</th><br>      <th>交易时间</th><br>      <th>op1</th><br>      <th>cp1</th><br>      <th>code2</th><br>      <th>op2</th><br>      <th>cp2</th><br>      <th>code3</th><br>      <th>op3</th><br>      <th>cp3</th><br>      <th>…</th><br>      <th>cp13</th><br>      <th>code14</th><br>      <th>op14</th><br>      <th>cp14</th><br>      <th>code15</th><br>      <th>op15</th><br>      <th>cp15</th><br>      <th>code16</th><br>      <th>op16</th><br>      <th>cp16</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>33</td><br>      <td>2010-01-04</td><br>      <td>2713.646</td><br>      <td>2682.361</td><br>      <td>40</td><br>      <td>2466.720</td><br>      <td>2449.466</td><br>      <td>34</td><br>      <td>2451.738</td><br>      <td>2426.070</td><br>      <td>…</td><br>      <td>3462.933</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>33</td><br>      <td>2010-01-05</td><br>      <td>2696.381</td><br>      <td>2717.703</td><br>      <td>40</td><br>      <td>2454.346</td><br>      <td>2519.827</td><br>      <td>34</td><br>      <td>2428.609</td><br>      <td>2443.498</td><br>      <td>…</td><br>      <td>3557.329</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>33</td><br>      <td>2010-01-06</td><br>      <td>2711.618</td><br>      <td>2726.328</td><br>      <td>40</td><br>      <td>2509.441</td><br>      <td>2486.248</td><br>      <td>34</td><br>      <td>2443.152</td><br>      <td>2441.108</td><br>      <td>…</td><br>      <td>3556.618</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>3</th><br>      <td>33</td><br>      <td>2010-01-07</td><br>      <td>2742.441</td><br>      <td>2669.174</td><br>      <td>40</td><br>      <td>2481.467</td><br>      <td>2418.760</td><br>      <td>34</td><br>      <td>2438.390</td><br>      <td>2407.755</td><br>      <td>…</td><br>      <td>3499.733</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>4</th><br>      <td>33</td><br>      <td>2010-01-08</td><br>      <td>2645.567</td><br>      <td>2650.290</td><br>      <td>40</td><br>      <td>2408.463</td><br>      <td>2458.201</td><br>      <td>34</td><br>      <td>2402.440</td><br>      <td>2419.262</td><br>      <td>…</td><br>      <td>3466.122</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>5</th><br>      <td>33</td><br>      <td>2010-01-11</td><br>      <td>2698.755</td><br>      <td>2662.427</td><br>      <td>40</td><br>      <td>2550.694</td><br>      <td>2494.197</td><br>      <td>34</td><br>      <td>2471.038</td><br>      <td>2440.857</td><br>      <td>…</td><br>      <td>3474.241</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>6</th><br>      <td>33</td><br>      <td>2010-01-12</td><br>      <td>2659.914</td><br>      <td>2696.089</td><br>      <td>40</td><br>      <td>2494.118</td><br>      <td>2612.378</td><br>      <td>34</td><br>      <td>2446.162</td><br>      <td>2496.178</td><br>      <td>…</td><br>      <td>3521.755</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>7</th><br>      <td>33</td><br>      <td>2010-01-13</td><br>      <td>2625.201</td><br>      <td>2610.793</td><br>      <td>40</td><br>      <td>2553.876</td><br>      <td>2640.460</td><br>      <td>34</td><br>      <td>2448.513</td><br>      <td>2453.958</td><br>      <td>…</td><br>      <td>3364.907</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>8</th><br>      <td>33</td><br>      <td>2010-01-14</td><br>      <td>2619.730</td><br>      <td>2647.404</td><br>      <td>40</td><br>      <td>2671.731</td><br>      <td>2693.953</td><br>      <td>34</td><br>      <td>2458.305</td><br>      <td>2498.126</td><br>      <td>…</td><br>      <td>3404.564</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>9</th><br>      <td>33</td><br>      <td>2010-01-15</td><br>      <td>2647.356</td><br>      <td>2654.593</td><br>      <td>40</td><br>      <td>2706.267</td><br>      <td>2685.700</td><br>      <td>34</td><br>      <td>2503.585</td><br>      <td>2512.508</td><br>      <td>…</td><br>      <td>3403.340</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>10</th><br>      <td>33</td><br>      <td>2010-01-18</td><br>      <td>2646.040</td><br>      <td>2673.146</td><br>      <td>40</td><br>      <td>2677.088</td><br>      <td>2701.576</td><br>      <td>34</td><br>      <td>2514.841</td><br>      <td>2554.061</td><br>      <td>…</td><br>      <td>3411.960</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>11</th><br>      <td>33</td><br>      <td>2010-01-19</td><br>      <td>2675.846</td><br>      <td>2686.841</td><br>      <td>40</td><br>      <td>2690.431</td><br>      <td>2667.143</td><br>      <td>34</td><br>      <td>2559.853</td><br>      <td>2549.841</td><br>      <td>…</td><br>      <td>3434.423</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>12</th><br>      <td>33</td><br>      <td>2010-01-20</td><br>      <td>2692.322</td><br>      <td>2587.099</td><br>      <td>40</td><br>      <td>2665.330</td><br>      <td>2527.851</td><br>      <td>34</td><br>      <td>2553.230</td><br>      <td>2485.197</td><br>      <td>…</td><br>      <td>3311.985</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>13</th><br>      <td>33</td><br>      <td>2010-01-21</td><br>      <td>2582.791</td><br>      <td>2555.272</td><br>      <td>40</td><br>      <td>2525.867</td><br>      <td>2552.840</td><br>      <td>34</td><br>      <td>2488.962</td><br>      <td>2486.646</td><br>      <td>…</td><br>      <td>3254.948</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>14</th><br>      <td>33</td><br>      <td>2010-01-22</td><br>      <td>2520.985</td><br>      <td>2476.312</td><br>      <td>40</td><br>      <td>2514.170</td><br>      <td>2514.909</td><br>      <td>34</td><br>      <td>2458.909</td><br>      <td>2443.763</td><br>      <td>…</td><br>      <td>3166.966</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>15</th><br>      <td>33</td><br>      <td>2010-01-25</td><br>      <td>2458.770</td><br>      <td>2453.876</td><br>      <td>40</td><br>      <td>2492.953</td><br>      <td>2439.092</td><br>      <td>34</td><br>      <td>2429.138</td><br>      <td>2404.856</td><br>      <td>…</td><br>      <td>3150.376</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>16</th><br>      <td>33</td><br>      <td>2010-01-26</td><br>      <td>2457.599</td><br>      <td>2368.312</td><br>      <td>40</td><br>      <td>2443.537</td><br>      <td>2400.311</td><br>      <td>34</td><br>      <td>2402.696</td><br>      <td>2325.771</td><br>      <td>…</td><br>      <td>3030.856</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>17</th><br>      <td>33</td><br>      <td>2010-01-27</td><br>      <td>2367.634</td><br>      <td>2332.614</td><br>      <td>40</td><br>      <td>2396.002</td><br>      <td>2423.848</td><br>      <td>34</td><br>      <td>2324.705</td><br>      <td>2307.867</td><br>      <td>…</td><br>      <td>3002.380</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>18</th><br>      <td>33</td><br>      <td>2010-01-28</td><br>      <td>2323.293</td><br>      <td>2340.692</td><br>      <td>40</td><br>      <td>2425.680</td><br>      <td>2452.100</td><br>      <td>34</td><br>      <td>2309.120</td><br>      <td>2332.425</td><br>      <td>…</td><br>      <td>3008.012</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>19</th><br>      <td>33</td><br>      <td>2010-01-29</td><br>      <td>2322.601</td><br>      <td>2338.576</td><br>      <td>40</td><br>      <td>2444.205</td><br>      <td>2457.172</td><br>      <td>34</td><br>      <td>2321.328</td><br>      <td>2328.314</td><br>      <td>…</td><br>      <td>2991.117</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>20</th><br>      <td>33</td><br>      <td>2010-02-01</td><br>      <td>2335.854</td><br>      <td>2270.268</td><br>      <td>40</td><br>      <td>2434.279</td><br>      <td>2363.383</td><br>      <td>34</td><br>      <td>2323.903</td><br>      <td>2279.203</td><br>      <td>…</td><br>      <td>2881.514</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>21</th><br>      <td>33</td><br>      <td>2010-02-02</td><br>      <td>2292.856</td><br>      <td>2292.992</td><br>      <td>40</td><br>      <td>2378.921</td><br>      <td>2344.431</td><br>      <td>34</td><br>      <td>2290.957</td><br>      <td>2263.588</td><br>      <td>…</td><br>      <td>2912.715</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>22</th><br>      <td>33</td><br>      <td>2010-02-03</td><br>      <td>2309.751</td><br>      <td>2356.738</td><br>      <td>40</td><br>      <td>2346.423</td><br>      <td>2356.974</td><br>      <td>34</td><br>      <td>2271.374</td><br>      <td>2304.093</td><br>      <td>…</td><br>      <td>3038.974</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>23</th><br>      <td>33</td><br>      <td>2010-02-04</td><br>      <td>2332.797</td><br>      <td>2331.941</td><br>      <td>40</td><br>      <td>2337.631</td><br>      <td>2345.271</td><br>      <td>34</td><br>      <td>2291.879</td><br>      <td>2310.058</td><br>      <td>…</td><br>      <td>2994.403</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>24</th><br>      <td>33</td><br>      <td>2010-02-05</td><br>      <td>2265.393</td><br>      <td>2264.205</td><br>      <td>40</td><br>      <td>2291.995</td><br>      <td>2285.270</td><br>      <td>34</td><br>      <td>2263.165</td><br>      <td>2272.085</td><br>      <td>…</td><br>      <td>2901.972</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>25</th><br>      <td>33</td><br>      <td>2010-02-08</td><br>      <td>2267.282</td><br>      <td>2267.392</td><br>      <td>40</td><br>      <td>2283.685</td><br>      <td>2249.282</td><br>      <td>34</td><br>      <td>2273.349</td><br>      <td>2277.823</td><br>      <td>…</td><br>      <td>2910.316</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>26</th><br>      <td>33</td><br>      <td>2010-02-09</td><br>      <td>2265.561</td><br>      <td>2300.005</td><br>      <td>40</td><br>      <td>2247.759</td><br>      <td>2250.444</td><br>      <td>34</td><br>      <td>2273.436</td><br>      <td>2293.953</td><br>      <td>…</td><br>      <td>2949.124</td><br>      <td>65.0</td><br>      <td>2799.228</td><br>      <td>2818.825</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>27</th><br>      <td>33</td><br>      <td>2010-02-10</td><br>      <td>2327.039</td><br>      <td>2329.774</td><br>      <td>40</td><br>      <td>2267.313</td><br>      <td>2285.868</td><br>      <td>34</td><br>      <td>2309.165</td><br>      <td>2321.366</td><br>      <td>…</td><br>      <td>2982.261</td><br>      <td>65.0</td><br>      <td>2837.942</td><br>      <td>2853.171</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>28</th><br>      <td>33</td><br>      <td>2010-02-11</td><br>      <td>2332.464</td><br>      <td>2360.296</td><br>      <td>40</td><br>      <td>2287.648</td><br>      <td>2307.545</td><br>      <td>34</td><br>      <td>2324.817</td><br>      <td>2321.032</td><br>      <td>…</td><br>      <td>2991.857</td><br>      <td>65.0</td><br>      <td>2857.127</td><br>      <td>2866.994</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>29</th><br>      <td>33</td><br>      <td>2010-02-12</td><br>      <td>2374.444</td><br>      <td>2397.680</td><br>      <td>40</td><br>      <td>2315.770</td><br>      <td>2346.850</td><br>      <td>34</td><br>      <td>2322.024</td><br>      <td>2326.739</td><br>      <td>…</td><br>      <td>3047.581</td><br>      <td>65.0</td><br>      <td>2879.211</td><br>      <td>2898.345</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>    <tr><br>      <th>…</th><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>    </tr><br>    <tr><br>      <th>2157</th><br>      <td>33</td><br>      <td>2018-11-19</td><br>      <td>1754.323</td><br>      <td>1766.487</td><br>      <td>40</td><br>      <td>2533.729</td><br>      <td>2529.920</td><br>      <td>34</td><br>      <td>1903.059</td><br>      <td>1913.073</td><br>      <td>…</td><br>      <td>1720.738</td><br>      <td>65.0</td><br>      <td>2783.358</td><br>      <td>2801.159</td><br>      <td>134.0</td><br>      <td>798.968</td><br>      <td>805.857</td><br>      <td>950070.0</td><br>      <td>3784.413</td><br>      <td>3784.413</td><br>    </tr><br>    <tr><br>      <th>2158</th><br>      <td>33</td><br>      <td>2018-11-20</td><br>      <td>1752.559</td><br>      <td>1731.626</td><br>      <td>40</td><br>      <td>2504.580</td><br>      <td>2442.358</td><br>      <td>34</td><br>      <td>1899.651</td><br>      <td>1878.983</td><br>      <td>…</td><br>      <td>1693.719</td><br>      <td>65.0</td><br>      <td>2776.358</td><br>      <td>2726.113</td><br>      <td>134.0</td><br>      <td>802.293</td><br>      <td>794.817</td><br>      <td>950070.0</td><br>      <td>3779.864</td><br>      <td>3779.864</td><br>    </tr><br>    <tr><br>      <th>2159</th><br>      <td>33</td><br>      <td>2018-11-21</td><br>      <td>1709.068</td><br>      <td>1725.332</td><br>      <td>40</td><br>      <td>2408.692</td><br>      <td>2462.195</td><br>      <td>34</td><br>      <td>1865.588</td><br>      <td>1885.438</td><br>      <td>…</td><br>      <td>1681.127</td><br>      <td>65.0</td><br>      <td>2698.996</td><br>      <td>2737.598</td><br>      <td>134.0</td><br>      <td>790.110</td><br>      <td>795.328</td><br>      <td>950070.0</td><br>      <td>3828.820</td><br>      <td>3828.820</td><br>    </tr><br>    <tr><br>      <th>2160</th><br>      <td>33</td><br>      <td>2018-11-22</td><br>      <td>1724.401</td><br>      <td>1715.715</td><br>      <td>40</td><br>      <td>2464.601</td><br>      <td>2458.634</td><br>      <td>34</td><br>      <td>1888.406</td><br>      <td>1883.209</td><br>      <td>…</td><br>      <td>1674.748</td><br>      <td>65.0</td><br>      <td>2744.605</td><br>      <td>2733.319</td><br>      <td>134.0</td><br>      <td>796.486</td><br>      <td>792.511</td><br>      <td>950070.0</td><br>      <td>3706.437</td><br>      <td>3706.437</td><br>    </tr><br>    <tr><br>      <th>2161</th><br>      <td>33</td><br>      <td>2018-11-23</td><br>      <td>1711.269</td><br>      <td>1654.161</td><br>      <td>40</td><br>      <td>2453.379</td><br>      <td>2372.747</td><br>      <td>34</td><br>      <td>1881.252</td><br>      <td>1841.670</td><br>      <td>…</td><br>      <td>1617.033</td><br>      <td>65.0</td><br>      <td>2729.671</td><br>      <td>2660.184</td><br>      <td>134.0</td><br>      <td>791.309</td><br>      <td>783.052</td><br>      <td>950070.0</td><br>      <td>3689.881</td><br>      <td>3689.881</td><br>    </tr><br>    <tr><br>      <th>2162</th><br>      <td>33</td><br>      <td>2018-11-26</td><br>      <td>1648.513</td><br>      <td>1638.650</td><br>      <td>40</td><br>      <td>2358.844</td><br>      <td>2397.097</td><br>      <td>34</td><br>      <td>1850.095</td><br>      <td>1843.309</td><br>      <td>…</td><br>      <td>1602.049</td><br>      <td>65.0</td><br>      <td>2661.906</td><br>      <td>2656.397</td><br>      <td>134.0</td><br>      <td>787.311</td><br>      <td>786.533</td><br>      <td>950070.0</td><br>      <td>3697.513</td><br>      <td>3697.513</td><br>    </tr><br>    <tr><br>      <th>2163</th><br>      <td>33</td><br>      <td>2018-11-27</td><br>      <td>1647.911</td><br>      <td>1639.499</td><br>      <td>40</td><br>      <td>2414.186</td><br>      <td>2447.015</td><br>      <td>34</td><br>      <td>1848.543</td><br>      <td>1841.800</td><br>      <td>…</td><br>      <td>1605.554</td><br>      <td>65.0</td><br>      <td>2669.267</td><br>      <td>2654.555</td><br>      <td>134.0</td><br>      <td>787.767</td><br>      <td>783.061</td><br>      <td>950070.0</td><br>      <td>3721.933</td><br>      <td>3721.933</td><br>    </tr><br>    <tr><br>      <th>2164</th><br>      <td>33</td><br>      <td>2018-11-28</td><br>      <td>1638.380</td><br>      <td>1652.918</td><br>      <td>40</td><br>      <td>2453.709</td><br>      <td>2508.172</td><br>      <td>34</td><br>      <td>1841.607</td><br>      <td>1864.742</td><br>      <td>…</td><br>      <td>1620.326</td><br>      <td>65.0</td><br>      <td>2654.625</td><br>      <td>2692.905</td><br>      <td>134.0</td><br>      <td>783.639</td><br>      <td>788.297</td><br>      <td>950070.0</td><br>      <td>3672.047</td><br>      <td>3672.047</td><br>    </tr><br>    <tr><br>      <th>2165</th><br>      <td>33</td><br>      <td>2018-11-29</td><br>      <td>1665.882</td><br>      <td>1637.879</td><br>      <td>40</td><br>      <td>2526.459</td><br>      <td>2432.162</td><br>      <td>34</td><br>      <td>1876.933</td><br>      <td>1852.254</td><br>      <td>…</td><br>      <td>1608.826</td><br>      <td>65.0</td><br>      <td>2708.103</td><br>      <td>2649.690</td><br>      <td>134.0</td><br>      <td>792.254</td><br>      <td>784.479</td><br>      <td>950070.0</td><br>      <td>3698.409</td><br>      <td>3698.409</td><br>    </tr><br>    <tr><br>      <th>2166</th><br>      <td>33</td><br>      <td>2018-11-30</td><br>      <td>1635.363</td><br>      <td>1652.398</td><br>      <td>40</td><br>      <td>2410.602</td><br>      <td>2447.328</td><br>      <td>34</td><br>      <td>1847.350</td><br>      <td>1875.414</td><br>      <td>…</td><br>      <td>1620.026</td><br>      <td>65.0</td><br>      <td>2645.322</td><br>      <td>2670.178</td><br>      <td>134.0</td><br>      <td>784.946</td><br>      <td>790.818</td><br>      <td>950070.0</td><br>      <td>3835.628</td><br>      <td>3835.628</td><br>    </tr><br>    <tr><br>      <th>2167</th><br>      <td>33</td><br>      <td>2018-12-03</td><br>      <td>1688.989</td><br>      <td>1698.744</td><br>      <td>40</td><br>      <td>2527.108</td><br>      <td>2551.565</td><br>      <td>34</td><br>      <td>1916.829</td><br>      <td>1912.219</td><br>      <td>…</td><br>      <td>1660.124</td><br>      <td>65.0</td><br>      <td>2750.849</td><br>      <td>2756.625</td><br>      <td>134.0</td><br>      <td>806.416</td><br>      <td>804.746</td><br>      <td>950070.0</td><br>      <td>3843.151</td><br>      <td>3843.151</td><br>    </tr><br>    <tr><br>      <th>2168</th><br>      <td>33</td><br>      <td>2018-12-04</td><br>      <td>1695.224</td><br>      <td>1714.395</td><br>      <td>40</td><br>      <td>2542.772</td><br>      <td>2546.331</td><br>      <td>34</td><br>      <td>1910.574</td><br>      <td>1929.953</td><br>      <td>…</td><br>      <td>1665.881</td><br>      <td>65.0</td><br>      <td>2751.997</td><br>      <td>2768.926</td><br>      <td>134.0</td><br>      <td>805.068</td><br>      <td>807.706</td><br>      <td>950070.0</td><br>      <td>3822.606</td><br>      <td>3822.606</td><br>    </tr><br>    <tr><br>      <th>2169</th><br>      <td>33</td><br>      <td>2018-12-05</td><br>      <td>1687.966</td><br>      <td>1703.822</td><br>      <td>40</td><br>      <td>2493.064</td><br>      <td>2509.245</td><br>      <td>34</td><br>      <td>1905.420</td><br>      <td>1919.090</td><br>      <td>…</td><br>      <td>1656.932</td><br>      <td>65.0</td><br>      <td>2727.755</td><br>      <td>2757.008</td><br>      <td>134.0</td><br>      <td>801.085</td><br>      <td>802.449</td><br>      <td>950070.0</td><br>      <td>3769.869</td><br>      <td>3769.869</td><br>    </tr><br>    <tr><br>      <th>2170</th><br>      <td>33</td><br>      <td>2018-12-06</td><br>      <td>1692.681</td><br>      <td>1677.408</td><br>      <td>40</td><br>      <td>2460.441</td><br>      <td>2397.791</td><br>      <td>34</td><br>      <td>1905.064</td><br>      <td>1888.891</td><br>      <td>…</td><br>      <td>1636.267</td><br>      <td>65.0</td><br>      <td>2729.767</td><br>      <td>2693.094</td><br>      <td>134.0</td><br>      <td>794.541</td><br>      <td>793.555</td><br>      <td>950070.0</td><br>      <td>3770.980</td><br>      <td>3770.980</td><br>    </tr><br>    <tr><br>      <th>2171</th><br>      <td>33</td><br>      <td>2018-12-07</td><br>      <td>1681.396</td><br>      <td>1684.223</td><br>      <td>40</td><br>      <td>2400.515</td><br>      <td>2415.807</td><br>      <td>34</td><br>      <td>1894.020</td><br>      <td>1907.082</td><br>      <td>…</td><br>      <td>1641.699</td><br>      <td>65.0</td><br>      <td>2702.189</td><br>      <td>2696.011</td><br>      <td>134.0</td><br>      <td>795.621</td><br>      <td>792.377</td><br>      <td>950070.0</td><br>      <td>3722.674</td><br>      <td>3722.674</td><br>    </tr><br>    <tr><br>      <th>2172</th><br>      <td>33</td><br>      <td>2018-12-10</td><br>      <td>1676.905</td><br>      <td>1669.241</td><br>      <td>40</td><br>      <td>2388.698</td><br>      <td>2423.082</td><br>      <td>34</td><br>      <td>1895.022</td><br>      <td>1898.475</td><br>      <td>…</td><br>      <td>1632.750</td><br>      <td>65.0</td><br>      <td>2676.193</td><br>      <td>2674.154</td><br>      <td>134.0</td><br>      <td>786.796</td><br>      <td>785.573</td><br>      <td>950070.0</td><br>      <td>3735.064</td><br>      <td>3735.064</td><br>    </tr><br>    <tr><br>      <th>2173</th><br>      <td>33</td><br>      <td>2018-12-11</td><br>      <td>1669.483</td><br>      <td>1676.685</td><br>      <td>40</td><br>      <td>2430.099</td><br>      <td>2482.301</td><br>      <td>34</td><br>      <td>1903.826</td><br>      <td>1900.236</td><br>      <td>…</td><br>      <td>1639.168</td><br>      <td>65.0</td><br>      <td>2679.515</td><br>      <td>2690.659</td><br>      <td>134.0</td><br>      <td>785.747</td><br>      <td>785.384</td><br>      <td>950070.0</td><br>      <td>3768.554</td><br>      <td>3768.554</td><br>    </tr><br>    <tr><br>      <th>2174</th><br>      <td>33</td><br>      <td>2018-12-12</td><br>      <td>1682.059</td><br>      <td>1678.936</td><br>      <td>40</td><br>      <td>2481.788</td><br>      <td>2440.667</td><br>      <td>34</td><br>      <td>1912.010</td><br>      <td>1904.690</td><br>      <td>…</td><br>      <td>1640.032</td><br>      <td>65.0</td><br>      <td>2708.227</td><br>      <td>2699.243</td><br>      <td>134.0</td><br>      <td>791.468</td><br>      <td>786.472</td><br>      <td>950070.0</td><br>      <td>3810.051</td><br>      <td>3810.051</td><br>    </tr><br>    <tr><br>      <th>2175</th><br>      <td>33</td><br>      <td>2018-12-13</td><br>      <td>1683.666</td><br>      <td>1702.764</td><br>      <td>40</td><br>      <td>2448.517</td><br>      <td>2487.152</td><br>      <td>34</td><br>      <td>1910.566</td><br>      <td>1951.742</td><br>      <td>…</td><br>      <td>1650.048</td><br>      <td>65.0</td><br>      <td>2706.955</td><br>      <td>2744.017</td><br>      <td>134.0</td><br>      <td>787.342</td><br>      <td>791.427</td><br>      <td>950070.0</td><br>      <td>3749.893</td><br>      <td>3749.893</td><br>    </tr><br>    <tr><br>      <th>2176</th><br>      <td>33</td><br>      <td>2018-12-14</td><br>      <td>1698.709</td><br>      <td>1670.207</td><br>      <td>40</td><br>      <td>2472.207</td><br>      <td>2428.973</td><br>      <td>34</td><br>      <td>1942.950</td><br>      <td>1924.022</td><br>      <td>…</td><br>      <td>1619.260</td><br>      <td>65.0</td><br>      <td>2737.008</td><br>      <td>2693.074</td><br>      <td>134.0</td><br>      <td>788.415</td><br>      <td>783.047</td><br>      <td>950070.0</td><br>      <td>3766.160</td><br>      <td>3766.160</td><br>    </tr><br>    <tr><br>      <th>2177</th><br>      <td>33</td><br>      <td>2018-12-17</td><br>      <td>1665.904</td><br>      <td>1678.581</td><br>      <td>40</td><br>      <td>2415.319</td><br>      <td>2412.400</td><br>      <td>34</td><br>      <td>1922.974</td><br>      <td>1931.527</td><br>      <td>…</td><br>      <td>1624.877</td><br>      <td>65.0</td><br>      <td>2686.233</td><br>      <td>2683.499</td><br>      <td>134.0</td><br>      <td>782.717</td><br>      <td>785.535</td><br>      <td>950070.0</td><br>      <td>3750.959</td><br>      <td>3750.959</td><br>    </tr><br>    <tr><br>      <th>2178</th><br>      <td>33</td><br>      <td>2018-12-18</td><br>      <td>1670.814</td><br>      <td>1667.029</td><br>      <td>40</td><br>      <td>2393.651</td><br>      <td>2398.800</td><br>      <td>34</td><br>      <td>1925.150</td><br>      <td>1917.040</td><br>      <td>…</td><br>      <td>1615.604</td><br>      <td>65.0</td><br>      <td>2667.987</td><br>      <td>2656.420</td><br>      <td>134.0</td><br>      <td>780.641</td><br>      <td>777.662</td><br>      <td>950070.0</td><br>      <td>3738.553</td><br>      <td>3738.553</td><br>    </tr><br>    <tr><br>      <th>2179</th><br>      <td>33</td><br>      <td>2018-12-19</td><br>      <td>1667.242</td><br>      <td>1648.755</td><br>      <td>40</td><br>      <td>2404.342</td><br>      <td>2364.009</td><br>      <td>34</td><br>      <td>1922.566</td><br>      <td>1902.693</td><br>      <td>…</td><br>      <td>1595.598</td><br>      <td>65.0</td><br>      <td>2660.946</td><br>      <td>2622.330</td><br>      <td>134.0</td><br>      <td>779.511</td><br>      <td>769.746</td><br>      <td>950070.0</td><br>      <td>3719.836</td><br>      <td>3719.836</td><br>    </tr><br>    <tr><br>      <th>2180</th><br>      <td>33</td><br>      <td>2018-12-20</td><br>      <td>1643.743</td><br>      <td>1643.127</td><br>      <td>40</td><br>      <td>2351.063</td><br>      <td>2372.549</td><br>      <td>34</td><br>      <td>1895.237</td><br>      <td>1875.376</td><br>      <td>…</td><br>      <td>1593.566</td><br>      <td>65.0</td><br>      <td>2616.527</td><br>      <td>2615.110</td><br>      <td>134.0</td><br>      <td>768.152</td><br>      <td>756.181</td><br>      <td>950070.0</td><br>      <td>3724.058</td><br>      <td>3724.058</td><br>    </tr><br>    <tr><br>      <th>2181</th><br>      <td>33</td><br>      <td>2018-12-21</td><br>      <td>1641.157</td><br>      <td>1627.991</td><br>      <td>40</td><br>      <td>2362.176</td><br>      <td>2370.390</td><br>      <td>34</td><br>      <td>1868.794</td><br>      <td>1861.734</td><br>      <td>…</td><br>      <td>1581.223</td><br>      <td>65.0</td><br>      <td>2606.714</td><br>      <td>2590.908</td><br>      <td>134.0</td><br>      <td>754.020</td><br>      <td>748.978</td><br>      <td>950070.0</td><br>      <td>3749.456</td><br>      <td>3749.456</td><br>    </tr><br>    <tr><br>      <th>2182</th><br>      <td>33</td><br>      <td>2018-12-24</td><br>      <td>1625.416</td><br>      <td>1634.661</td><br>      <td>40</td><br>      <td>2369.313</td><br>      <td>2438.950</td><br>      <td>34</td><br>      <td>1855.511</td><br>      <td>1869.084</td><br>      <td>…</td><br>      <td>1585.971</td><br>      <td>65.0</td><br>      <td>2581.226</td><br>      <td>2611.945</td><br>      <td>134.0</td><br>      <td>744.198</td><br>      <td>746.910</td><br>      <td>950070.0</td><br>      <td>3723.477</td><br>      <td>3723.477</td><br>    </tr><br>    <tr><br>      <th>2183</th><br>      <td>33</td><br>      <td>2018-12-25</td><br>      <td>1622.469</td><br>      <td>1605.203</td><br>      <td>40</td><br>      <td>2412.039</td><br>      <td>2470.138</td><br>      <td>34</td><br>      <td>1854.376</td><br>      <td>1846.477</td><br>      <td>…</td><br>      <td>1556.440</td><br>      <td>65.0</td><br>      <td>2586.229</td><br>      <td>2588.309</td><br>      <td>134.0</td><br>      <td>739.827</td><br>      <td>744.314</td><br>      <td>950070.0</td><br>      <td>3739.416</td><br>      <td>3739.416</td><br>    </tr><br>    <tr><br>      <th>2184</th><br>      <td>33</td><br>      <td>2018-12-26</td><br>      <td>1600.658</td><br>      <td>1598.941</td><br>      <td>40</td><br>      <td>2459.625</td><br>      <td>2425.858</td><br>      <td>34</td><br>      <td>1844.705</td><br>      <td>1843.400</td><br>      <td>…</td><br>      <td>1548.941</td><br>      <td>65.0</td><br>      <td>2585.119</td><br>      <td>2574.757</td><br>      <td>134.0</td><br>      <td>743.132</td><br>      <td>741.972</td><br>      <td>950070.0</td><br>      <td>3731.144</td><br>      <td>3731.144</td><br>    </tr><br>    <tr><br>      <th>2185</th><br>      <td>33</td><br>      <td>2018-12-27</td><br>      <td>1619.408</td><br>      <td>1587.707</td><br>      <td>40</td><br>      <td>2458.034</td><br>      <td>2411.189</td><br>      <td>34</td><br>      <td>1864.291</td><br>      <td>1831.414</td><br>      <td>…</td><br>      <td>1530.117</td><br>      <td>65.0</td><br>      <td>2607.803</td><br>      <td>2555.645</td><br>      <td>134.0</td><br>      <td>748.851</td><br>      <td>743.572</td><br>      <td>950070.0</td><br>      <td>3741.382</td><br>      <td>3741.382</td><br>    </tr><br>    <tr><br>      <th>2186</th><br>      <td>33</td><br>      <td>2018-12-28</td><br>      <td>1590.878</td><br>      <td>1592.215</td><br>      <td>40</td><br>      <td>2406.970</td><br>      <td>2420.020</td><br>      <td>34</td><br>      <td>1832.470</td><br>      <td>1848.191</td><br>      <td>…</td><br>      <td>1532.148</td><br>      <td>65.0</td><br>      <td>2556.683</td><br>      <td>2560.488</td><br>      <td>134.0</td><br>      <td>745.710</td><br>      <td>750.410</td><br>      <td>NaN</td><br>      <td>NaN</td><br>      <td>NaN</td><br>    </tr><br>  </tbody><br></table><br><p>2187 rows × 49 columns</p><br></div><h3 id="二、选择要进行分析的数据"><a href="#二、选择要进行分析的数据" class="headerlink" title="二、选择要进行分析的数据"></a>二、选择要进行分析的数据</h3><p><strong>观察上表，发现data中有数据缺失，为了方便起见，这里选取前13个无数据缺失的行业股价进行演示</strong></p><p><strong>首先计算13个行业的开/收盘价的收益率</strong></p><p><strong>计算日收益率的方法：（后一天的价格-前一天的价格）/前一天的价格</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#一次性计算13个行业开盘价的日收益率并将其存入名为op的list里面</span></span><br><span class="line">op=[]</span><br><span class="line"><span class="keyword">for</span> cloumn <span class="keyword">in</span> range(<span class="number">2</span>,<span class="number">38</span>+<span class="number">1</span>,<span class="number">3</span>):</span><br><span class="line">    print(cloumn)<span class="comment">#充当进度条</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,len(data[<span class="string">'code1'</span>])):</span><br><span class="line">        opp=(data.iloc[:,cloumn][i]-data.iloc[:,cloumn][i<span class="number">-1</span>])/data.iloc[:,cloumn][i<span class="number">-1</span>]</span><br><span class="line">        op.append(opp)</span><br></pre></td></tr></table></figure><pre><code>25811141720232629323538</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">op<span class="comment">#查看13个行业开盘价日收益率</span></span><br></pre></td></tr></table></figure><pre><code>[-0.0063622889647359585, 0.0056509076425030735, 0.011367014085317277, -0.03532400514723919, 0.020104574936110144, -0.014392191955179294, -0.013050421930934851, -0.002084030898967184, 0.010545361544891935, -0.0004970997478237923, 0.01126437997913848, 0.006157305016806113, -0.040682726657509743, -0.023929926966603197, -0.024678845768618275, -0.0004762543873562073, -0.03660686710891408, -0.01872797907108968, -0.00029785309041931114, 0.005706102770126983, -0.018407828571477325, 0.007368539498337436, 0.009977698894815857, -0.028894070079822632, 0.0008338509035739598, -0.0007590586437858208, 0.02713588378331001, 0.002331288818107553, 0.01799813416198493, 0.013006834442084132, -0.012117682079117665, -0.014750559826881952, 0.028928325668234868, 0.007222930718980248, 0.005154033685659631, 0.00826403582148286, -0.009709570973201992, 0.009677052400392893, -0.031637071946955204, 0.009172890012684681, 0.004866691444770236, 0.002268517598096469, 0.001288107548095419, -0.004114000064384368, -0.017975298572966786, -0.018485789186716277, 0.011986413277151818, 0.018726435816624035, -0.0052878652272084995, 0.015594058887037184, -0.0003377000397975001, -0.007480228788535725, -0.00847904392933665, -0.00763756321447403, 0.017732885253846056, 0.017480833658468434, -0.00021847756664190963, -0.00649040592199499, 0.0192740403373049, 0.015340711998128405, -0.004153336990759343, -0.0032790835161029333, -0.006977568841156929, 0.013336992935516984, -0.007008493637229652, 0.005704125667804469, 0.014714907870028384, -0.004602398459534969, -0.026189278239295715, -0.03781859674497436, 0.008730950624489942, 0.009719208045369944, -0.0006457869118040601, -0.002518253305439979, -0.016658038529433854, -0.038345025140581884, 0.015539865009092832, -0.024439983581800398, -0.015043478453224509, -0.004143624074253702, 0.009999156550915624, -0.048705016308988394, -0.0005980678746009809, 0.015217112534577497, -0.024380990276752886, -0.004917908395064019, 0.016387942332896446, -0.010329001292297536, -0.04345477701212147, 0.005932461964285292, -0.002367951847214158, -0.030265145158583636, 0.04797782506701907, 0.024226264627817833, -0.004408486561868903, -0.0016026938592774, 0.035350689930708806, -0.016303065711726437, -0.03885908067889357, 8.089735636219567e-05, 0.008347834392142244, -0.02282339751024514, -0.01603569047680275, 0.0036750607023324825, 0.009653775102794951, 0.016007205573606357, 0.0014893890185506947, 0.008482400067851423, -0.012958301433001545, -0.024217803532970966, 0.02278341006424525, -0.006790711641065821, -0.006517020231810578, -0.009588542479883666, -0.006420442592818529, -0.010924342146818386, -0.0585652277868717, -0.018389090492710918, -0.029265212951747527, -0.011743828382124103, -0.009566755797335366, 0.029031017426756237, 0.012274860989701342, -0.003775939334730873, 0.030498625473290776, -0.0025077288540620007, -0.012842897733006314, 0.0052901925333911325, -0.022196535223251556, -0.0068720119395937, 0.04222659942379415, 0.04169479393461169, -0.001225687618052649, 0.021364411751809656, 0.0018408686119636474, 0.008970625219674485, -0.011507316954787766, 0.03473662889900936, 0.020181154369741168, 0.0013891493241038652, 0.0217131493380816, -0.026555281670964187, 0.006694653083086948, -0.007412716101527194, 0.01943335331116782, 0.02192746595041329, -0.03207913370169064, -0.003978506408823295, 0.0017523967600776298, 0.015170064998664488, 0.02512092124662682, 0.003374945886011768, 0.010214981756974969, -0.0016950684289262457, -0.02829252287747487, 0.0011222038892664472, 0.0032386032437047843, -0.010589890470840501, -0.005283328106477628, 0.015616500615962531, 0.022814900792183394, 0.014979965919034605, -0.0015731731996991133, 0.012404339473694915, -0.00039593240060261054, 0.01775620393834829, 0.010646381121735401, 0.027286920667575282, -0.024916944227576072, -0.0027633592510859224, 0.012756291715490965, 0.002162997296253257, -0.02309351566876971, -0.017468063159565162, 0.0045111675511784605, -0.008013513368659136, 0.0030175243333487433, 0.027586850818412157, 0.013614895089254675, -0.005575018607098732, 0.06105403666007197, 0.044580058261024416, 0.0032039889936682545, 0.05214857475607378, 0.011140150217175923, -0.008196748338613322, 0.038062903657165206, -0.03940409193054499, -0.020309450169720685, 0.03536137100040095, 0.0049022579676759525, 0.0150878650534662, 0.05277394288136606, -0.01946970972278194, -0.016598137842651264, 0.004589542276657001, 0.015475506710499866, 0.039177141055408954, 0.0011734098084683936, -0.04409701003257587, 0.060647723907006866, -0.02200222545971792, 0.006043717586735627, 0.002918094484301672, -0.0015524530851268345, -0.007554111798060269, -0.062363051844293735, -0.014110815229184637, -0.08347369203518, 0.017419222111006046, 0.016250725704501957, -0.015410016721320138, 0.011125632831576037, -0.031014476599718947, 0.0320563470115331, 0.008442647302422815, -0.023282731348585033, -0.003427521231784817, -0.0008543457892147433, 0.022105016068929794, -0.009139541918705769, -0.0003304456101617323, -0.00816478197508437, 0.019428349764931036, -0.01447578213974671, -0.01865591780022702, 0.029594483496762084, 0.04234127212963478, 0.000695242036435927, -0.02181948723467864, -0.005381746823910076, 0.015951885222798952, -0.01808303092405235, 0.017619710014160985, -0.009785054216910184, -0.01844786352100904, -0.00668835454575824, -0.02743450502820336, -0.010076581863185906, 0.003958604082180029, 0.008582613150169331, 0.047402580152483956, -0.0014407645515893213, 0.0020983021380693626, -0.0038696639380915317, -0.009515588345986575, -0.02275222125557452, 0.010610662373855126, 0.004707661810830962, -0.018383439718353652, -0.03709092283274153, -0.026009268525604486, 0.006768923142271186, 0.0239744888025806, -0.0411320369083347, 0.014565330835505702, -0.018923870937501086, -0.011995568635598064, 0.008630102859185843, 0.020579461119908063, 0.015611066790484624, 0.02498925191245761, -0.004332218984966552, -0.014825279418174136, 0.028647983431382914, 0.006297756791023245, 0.040332206876518735, 0.008178871718505225, 0.029133112000226915, -0.006732532473357233, -0.013726222470290709, 0.034549014927882434, -0.03343441891509523, 0.021876700555811975, 0.0032906552003002854, 0.008808040933852901, 0.008119717665910449, 0.0021337088200137923, -0.0017175558124826124, -0.012812445244615247, 0.02598662582786772, 0.016039938929301927, 0.005777462964801735, -0.010840951523180245, -0.023342083715503805, 0.0022647065596611378, 0.003231779862730279, -0.013071200774102116, 0.019935553277685376, -0.002976624274152177, -0.006287650938031113, -0.0008048337388599559, 0.004740601033947486, 0.019287893750565215, -0.006323299322471208, 0.028335087749633895, -0.002101508632005129, -0.015006673033071377, 0.018430462369262702, -0.010421562325920158, 0.0032791496519930185, 0.022078661966238387, 0.00643585729564943, 0.02057844678997788, -0.014006614718933642, -0.011652804208272326, 0.00882627605872636, -0.002981141960367659, -0.0012438712413747343, 0.0018706503096751915, -0.012749855553502457, 0.010236571908376676, 0.007385206521845163, -0.003038562103283165, -0.03688153864180391, -0.007535933856190828, -0.005131333919164361, -0.03622922586877498, 0.01575115217809806, 0.00031789847530984666, -0.029115854425760788, -0.012649519227920849, 0.019450259404296268, 0.007384972840222029, 0.013516489174648467, -0.01546129326195033, -0.01446202884866967, 0.0017653063460050383, -0.012671942874320948, -0.003123609721484452, 0.015347997280874097, -0.0064094527558793005, -0.002450407617318079, -0.03308354920001491, 0.018199382375127242, 0.0016132024120491804, -0.01817356575014532, -0.017509628362969466, -0.0046879278383285365, 0.02071794815038505, -0.0068294462363113735, 0.012015388868375346, 0.009239742063943072, 0.023603278324296546, -0.002991416971675981, -0.017948114645588317, -0.0062959560223908645, 0.01449283139764641, 0.01680107730022049, -0.01605636691858527, -0.00859406776684526, -0.027244880894832565, 0.0007988734318192712, 0.007352377255602897, -0.008563073865786032, 0.01687953326717292, 0.02695279148545323, 0.005718800683869942, 0.00917832983046701, -0.016809067943780745, 0.020246688370110125, 0.0014460255486625675, 0.03137951603068749, -0.00015626979015951549, 0.0005395361915668106, -0.009367602987105022, -0.00921569396897307, 0.00031649076780809055, -0.0062787739299931455, 0.028365929593832002, 0.006454834994526084, 0.011565307468145478, -0.011396769367472765, 1.695212754506324e-05, -0.010569119185916015, -0.01764405216403066, -0.0012564530381599934, -0.03081927898443443, 0.004715805436896157, 0.011074567835183824, 0.002253692874761027, -0.014483889092895927, -0.0013834440874227125, -0.011938152525286105, 0.012617754715306972, -0.03174685191779231, -0.0012420895101056834, -0.04893228272312516, 0.04444235464239345, -0.020276433031642456, 0.02590430949425502, -0.005010022760138954, 0.012139710146735072, -0.006160134223690945, -0.007318485064292442, -0.0349312678426869, 0.006088621105682766, -0.00970318005575808, 0.011648949946375036, -0.015493797585484368, 0.024525959432349607, 0.001900419150432694, -0.0029685125631691046, -0.014523546201707276, 0.0003762392962749238, -0.01112659669874208, -0.015338188969462038, -0.024568824779667212, 0.0031478721558138617, 0.021051823755052376, -0.012728368998124543, -0.02556327791405471, 0.0006224276837877475, 0.004748990262607219, 0.001052298957661613, -0.00717736079844338, -0.021320556982541816, 0.00898988191372756, 0.02377211041185292, -0.040149898194854934, -0.0036638124840292583, -0.001007567254555772, 0.0066436802070303285, -0.03064119328393464, -0.015508548379703918, -0.0004837361752250537, 0.012577172067042324, -0.03357212878485731, 0.03548378344543882, 0.013925759621364907, 0.0029034037921064943, -0.013121521673384975, -0.02992031845135154, -0.01433740668739008, -0.023573525522855344, -0.015161588026947632, 0.021153553947515804, 0.024601216937205763, 0.020730305566903792, 0.014412937637233555, 0.0013906031281212176, -0.017888796025018108, -0.010209172544554586, 0.02936653894429917, 0.018393341884676274, -0.0009382291892924165, 0.004908001472265059, -0.010612813207934309, -0.008002798257349791, -0.003685649200649844, 0.004248277508227866, 0.010054004392570612, 0.0021834742899863257, -0.028427178468766173, -0.015707008246225722, -0.019681706254677488, -0.009223224280069445, 0.007831706305666066, -0.022911699345869174, 0.00383238038066114, 0.002294190460593757, 0.007637647922388806, 0.004218388140403455, -0.013295407874058557, -0.010817388022440807, -0.009535188736424668, -0.025619624541109533, -0.0010537478315033155, 0.000638481084933578, -0.011009989792881156, -0.0033263851389524457, -0.0196659900784477, -0.030210395155763268, -0.03042169866482603, -0.03396756842540785, 0.01687573339591627, -0.007624210763788486, 0.01756156723251687, -0.04234884473023515, -0.0016777672569281738, 0.010314213569603633, 0.0014735433600929352, -0.019598056437960573, 0.0007808193180483427, 0.00561558638268448, 0.025791732493464403, -0.03209742387228042, -0.021541715631506163, 0.015100667005181171, 0.04352080792530252, 0.05381064962081976, -0.0034861170838506685, 0.014531154094161515, -0.032740758780094, -0.018928817799896903, 0.09586194793126164, -0.02179897747064434, 0.0333750765575742, 0.00894266121047973, -0.01939846682146031, -0.0010279882668632779, -0.0197678812529136, 0.015514819421806708, 0.013156415039251773, -0.006929737526681016, -0.010984541984532536, 0.042231707737651634, -0.00020566693868325118, -0.01086134636701693, 0.011456775074307443, -0.005443639298841299, 0.012954887148788808, 0.001743890573392317, 0.004570500077158444, -0.00835443025265603, 0.008968008282055588, 0.012947362611448701, 0.007725589746765009, 0.015725456039699704, 0.0002622836583246831, 0.003254947854554951, -0.012616848343689037, 0.006926255548647427, 0.01675564434731041, -0.007938275980668826, -0.03316266424575573, 0.0032337141808462624, 0.021968646530103875, 0.015079252663584251, -0.002014724308681163, 0.015533107512145938, -0.038999121564943985, -0.010763909843791483, 0.016746612015163472, 0.016045402459494932, -0.004966811902104018, 0.007670037829074435, -0.01852923036847788, -0.006668357987588144, 0.005790240921959196, -0.006616853883751488, -0.04593877550017182, -0.00796939406430343, -0.013729573014122736, 0.039894696874346706, -0.00399614109806527, -0.003420339196771886, -0.007977910621074981, 0.015249948570253056, 0.015571524964894558, -0.0008893695153262142, 0.00024163657858599586, -0.00669381260319485, 0.01997305637308221, -0.006165543994100045, 0.009447711164449347, -0.015971042633152136, -0.0033315866956109145, 0.021373701039568975, -0.00210504539279887, 0.008050828098510106, 0.01741558659912149, -0.0012928158429576354, 0.010365507397630123, 0.006929636514504108, -0.004311706287284372, -0.016483937925888353, 0.008063580489697632, -0.005662870566301467, -0.032285161418602826, 0.004051791865744932, -0.00571308527859817, 0.007548531779746315, -0.002519858627394949, 0.013586803284932631, 0.0023810906556710146, -0.013993105932987434, -0.0007021532204982316, -0.005460087746821885, 0.019605242042263958, 0.0159858726324648, -0.009005306013017713, 0.007004588579031126, -0.007322090268132267, -0.016271959447191987, -0.0024145530714433814, 0.018819994671771208, -0.01655620320112413, -0.017146610586356824, 0.006578413425554407, 0.0008749729370572133, 0.004693273819558143, -0.007558895486589128, 0.00251953864314971, 0.0011277591420871182, -0.006730111784254734, -0.0064490340706707654, -0.020605605300670238, -0.03596543090551075, -0.002078784769515091, -0.00905230553495127, -0.03150466187455413, 0.03083171574015601, 0.0021812312558880487, -0.001313005110802404, -0.006874814474997884, -0.014968110636743914, 0.005124770422532873, -0.0213562198590669, -0.002788075941557533, 0.008892680959673833, 0.012356518879021493, 0.0038057110446019084, -0.0316545483869906, 0.01286622659424281, 0.004970146123190921, 0.0012455746757638086, -0.015036695406723493, -0.01292306546241757, 0.005628773121850805, -0.002784704479064634, 0.0027502913347578, -0.0036939539009108783, -0.01640736325437705, -0.014978196443200533, 0.01863930977125098, -0.004606219147554628, 0.01287414279365777, 0.035823645028065405, 0.01617056807114271, -0.0008096927744455859, 0.004526174924118505, -0.00996266173310786, -0.01414247534991558, -0.007661571559356512, -0.00856930352059429, -0.004207771062881815, -0.004398706276218245, 0.003616967529405497, 0.011265768268805586, -0.009569345731381816, 0.009391213386747716, -0.021431706561805756, -0.02274473679072006, -0.0006246936767835354, -0.023214555581625402, -0.01072727415021312, -0.003266649384095372, 0.016616948713784472, -0.015038528685268124, 0.007127692395868045, 0.019784207505636284, 0.05647715487123093, -0.0025798164065309057, 0.00850876595006294, 0.0024037816966240265, 0.010444722717745714, -0.007339749087357365, -0.02845541780820772, -0.010055369440323649, 0.015772067000378462, -0.026915773256207552, 0.00209876038129533, 0.011111623053993132, -0.009242333961871081, -0.0197795800026782, 0.02947929980946229, 0.026945304562597437, -0.009411834765642297, 0.012296113972006101, 0.0027637077495306474, -0.00037768443683124895, -0.010632256824083412, -0.015067042151676011, 0.004498463995081287, 0.005981178843242814, 0.010676568649598504, -0.012961273988943227, 0.006566590783786103, -0.013286612091975736, 0.0029394426688375457, -0.003823613411778123, -0.034290593036950624, -0.006892280799879793, 0.005542286088383608, 0.00720509672298308, 0.024633659251598317, -0.0069193506756881264, 0.0014010542425546663, -0.0010891498085008773, -0.000967964380766737, -0.013619781628359395, -0.002449522013622604, 0.004621388894624815, -0.015524630926638675, 0.003999181551746975, -0.012321105708872111, -0.006010310952830691, 0.015262186054872017, -0.00935776677354961, 0.010111689519863153, -0.005105909952264827, 0.003574097404183802, -0.005531487594873982, -0.025040647479856296, -0.02051281420131164, -0.0021825734639526273, 0.01349368005162999, -0.01970669509510107, 0.008076378304556217, 0.04089825791375887, -0.005865672948871643, 0.024151545877548572, 0.010755871031577624, -0.005349464085229909, 0.00510740538378126, -0.015172737679415306, 0.04630651810558376, 0.008357602854916159, 0.0030379251937052635, -0.006254030929373802, 0.02063390755864296, -0.011167171581136625, -0.0014422696201743703, 0.018218080394429837, 0.012127489159554868, -0.00841890405800353, 0.00820803146494634, 0.021579415130102104, -0.009311069559952556, 0.00017908222517209958, 0.002892307260861553, -0.003763191065567096, 0.006624765059154968, -0.03516980031283744, 0.03500844782357388, 0.01209813829358792, 0.005156515280135851, -0.00695691096262432, 0.004096018846151667, 0.0003255613286078463, -0.011368182636029867, 0.002000344720899229, -0.019252476741732392, 0.006960165065614464, 0.025037135126421632, 0.0068646009185521365, 0.0015802325745946297, -0.0030759376690904284, 0.011611561484952945, -0.003696544996510385, 0.01748624146585473, -0.0014275798810024691, 0.0010979733626125335, 0.006977950434432282, -0.012633749898939154, -0.026511607067907932, -0.0027614503645167526, -0.027812450433427397, 0.004061305357998356, -0.0034764320514319358, -0.012272401058401832, 0.007361401587010675, 0.020690728661572632, -0.01177959585198405, -0.029234640565848652, 0.017475591209644967, -0.000628792421483357, 0.002340601565428525, -0.005538533861942522, -0.0034675396538754654, -0.012105745865232501, -0.008379004526917559, -0.0031729904912110164, -0.0132686462473697, -0.014547290772701945, 0.009170392582446396, 0.0246992473811014, 0.0003104063444799054, 0.001871451534370096, 0.005311031244474152, -0.0073068712261975585, -0.007160330112153404, -0.016297267988150736, -0.013881169103550972, 0.0030485071806610944, 0.0033774548173418186, -0.014696208117495361, 0.010329233230801345, 0.010258719502210661, 0.005237620564259708, -0.01096081310644662, -0.01831889775699335, -0.023141028188319007, 0.015397356220507566, -0.006336417929724766, 0.004628116878154356, 0.020779998682232303, -0.006724047079478636, -0.027654849612133763, 0.010588057312940923, -0.007132645319231589, -0.016766839804925816, -0.005954609726416734, 0.01769762377525767, 0.012140519066087685, 0.011590112978039522, -0.0018187828721667174, -0.013527237638004947, 0.006474752811532919, -0.005717977884006189, -0.013434967480572425, -0.004733185692678719, 0.010966550974859017, 0.01754299587030553, 0.007989753999207894, -0.0006086308236249603, -0.0021175699002649716, -0.012740700595913024, 0.004420485434789444, -0.0009576316893389802, 0.015126249023754918, -0.004496197229954827, 0.000726534537101301, -0.012032499492862744, -0.004070487882193581, -0.016937453733509827, -0.00015636775488396702, -0.009345507802438174, -0.02825272182249727, -0.0319709844665218, 0.004716810391066916, -0.009589120059529901, -0.004654801926207195, -0.007223500597030111, -0.040025353428669676, 0.007346558579688447, -0.06479588376081608, -0.0005898478089042987, -0.00455272848987589, -0.01595460966739107, 0.0038720777983739835, 0.0062217322350791495, -0.005028484015722207, -0.01228535603312306, 0.02796435403492116, -0.0011216002880307697, -0.024536783296238297, 0.002589604024758583, 0.048803903948311356, 0.02873130632145145, -0.018048057398417375, 0.004789993980297469, 0.004257816980119879, -0.01715271098096952, -0.008867849768362003, -0.03278355470115341, 0.02566818670153925, 0.022936052550256852, -0.003953071182548642, -0.011998492894197839, -0.010012862430328645, -0.008341287939050077, 0.013479859682897527, -0.0023260790544420862, 0.023378930094596996, -0.0037059075597501243, 0.010615856566701679, 0.0056503176254122656, 0.006246637257661117, 0.0038196790257914973, 0.0280557479649637, 0.030587279590959058, 0.0025693897517183283, -0.009161086063732502, -0.0064559148101968, -0.01803456042436776, 0.023439742583839508, -0.008059690727983543, -0.005774268320022359, -0.0013888080086348043, -0.006227475072130603, 0.014341850377770696, 0.007308000188350543, 0.01971145691286681, -0.015854918004336714, -0.009075295406255651, 0.005491823039467082, 0.008234817181047715, 0.0009227420475598352, -0.012563003631973895, 0.008015865515759826, 0.015593739631822036, 0.014122847562021585, 0.011622787480649068, -0.011954461624857181, -0.001048248494959331, -0.01053680348929367, -0.02262449056670366, 0.005820479718242186, 0.010481508635330048, -0.01144828275758636, -0.0029497798810770694, -0.015178281236339623, 0.006459535065273123, -0.0021431745646992234, 0.010719319944642086, 0.007713553061220613, -0.002434682600201237, 0.010979158724951019, -0.0007869701910405385, 0.0004516480222154934, -0.009446156252238328, -0.0027086230138090393, -0.006429082201120824, 0.01213395284067311, -0.0021226577341735544, -0.014304890376680082, -0.011384705395781331, -0.022328929449426112, -0.004349305370498042, -0.012076254900453656, 0.012835031820551622, -0.0025666024608655504, 0.010810969773396791, -0.008762007150085922, 0.007650754383390194, -0.003406573744137883, -0.012023610200653474, -0.014180180992415333, 0.0025214419018059473, 0.00421487750556792, -0.009275825580413273, 0.007231535943849612, 0.018622196429181014, 0.019889711837614774, 0.000991281786224122, -0.0036985794527770476, -0.004006109385457768, -0.005248325394796066, 0.00012679065823073692, 0.002800124973415381, 0.00943871330089706, 0.02822177753809839, -0.014443076308695827, -0.0070984298497382865, 0.020188554377893672, 0.011940628873120706, -0.0074797523571466734, 0.002563595352869461, -0.0027166068567341384, -0.000970630783867339, -0.01326643518133938, -0.010981861676764492, 0.004558374293329384, -0.019633270065075897, -0.004399002376512342, 0.004511502734612445, -0.014584778041042046, -0.027452226796774924, -0.003605672983207303, 0.0024472342271046987, 0.005300711937833058, -0.015441815686809624, 0.017692211705245993, -0.005927054069131332, 0.016109859228077103, 0.0025960289346841055, -0.013381272449334574, -0.03739500864043721, 0.0038081321663410825, -0.013451689185174514, -0.015878512665470256, 0.0018351585472950074, 0.003863648337753071, 0.006817509353823786, -0.0017368429800266668, 0.007205416814556236, -0.014030481903811592, -0.007009522067648873, 0.012297613922151526, 0.021849207113349724, -0.007084558422567296, 0.004853875628403442, -0.004512911880357612, 0.0034831285261727723, -0.0012304581949409187, -0.015853933932501483, 0.018306263569906934, 0.027656267430186197, 0.004011324048115713, 0.0023454263825149007, -0.0005242209494224163, 0.016225557699585374, -0.0006902860120441876, -0.014779490790891478, 0.00688253170164136, -0.010497124325003833, -0.013782799649222844, -0.015311779355810271, ...]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#计算13个行业的收盘价的日收益率并将其存入名为cp的list里面</span></span><br><span class="line">cp=[]</span><br><span class="line"><span class="keyword">for</span> cloumn <span class="keyword">in</span> range(<span class="number">3</span>,<span class="number">39</span>+<span class="number">1</span>,<span class="number">3</span>):</span><br><span class="line">    print(cloumn)<span class="comment">#充当进度条</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,len(data[<span class="string">'code1'</span>])):</span><br><span class="line">        cpp=(data.iloc[:,cloumn][i]-data.iloc[:,cloumn][i<span class="number">-1</span>])/data.iloc[:,cloumn][i<span class="number">-1</span>]</span><br><span class="line">        cp.append(cpp)</span><br></pre></td></tr></table></figure><pre><code>36912151821242730333639</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp<span class="comment">#查看13个行业收盘价日收益率</span></span><br></pre></td></tr></table></figure><pre><code>[0.013175706029128853, 0.0031736359712595526, -0.020963728502219666, -0.007074847874286377, 0.004579498847296021, 0.012643351348224856, -0.031636937801385906, 0.01402294245464914, 0.0027154903445032927, 0.006989018655590475, 0.005123176960779608, -0.03712240508463275, -0.012302196398359794, -0.030900819951848586, -0.009060247658614785, -0.03486891758181762, -0.015073182925222634, 0.003463067614273075, -0.0009040061656980011, -0.029209228179883823, 0.010009390961771985, 0.027800358658032664, -0.010521746583625152, -0.029047047073661094, 0.001407558061218111, 0.014383485519927663, 0.0129430153412709, 0.013100841540853487, 0.01583869141836429, -0.005371025324480207, -0.007695817095088005, 0.018640165074337064, 0.008911207354307804, -0.00993527658456435, 0.020714919941957344, -0.012183059594581649, 0.011128117722362703, -0.03004198659704071, 0.00533076755409188, 0.010660980810234725, 0.0012792977791885737, 0.002433133214249796, -0.006255855470012706, -0.015293643761909068, -0.021404442155064214, 0.006204286019004688, 0.0223138747432957, -0.0029661817652773203, 0.018915085063372316, -0.0031115495236790727, -0.008569123908425307, -0.0041646194715321706, -0.010855230973659273, 0.011479074234390388, 0.021690434677948983, 0.0020431503537778816, -0.0089478554741731, 0.017328238520917316, 0.012740601943642352, 0.0023419194384509427, -0.0023727427733849452, -0.009822986347767983, 0.009520306406977543, 0.0006959206650442796, 0.0023852679329742295, 0.014537121913817258, -0.002986890693948995, -0.011037695537619144, -0.052376964594549436, 0.007011308358342619, 0.015391915339035613, -0.0030876639884423866, -0.0098644168786915, -0.008128183927666647, -0.029851498793301557, 0.0008003464703334588, -0.02029860899548551, -0.0015415596077814025, -0.0058209622769970935, 0.001291776372729434, -0.03619951328684263, -0.020587068989366953, 0.0002933017967206916, -0.003939822517372962, -0.0102667619252825, 0.024807411000393385, -0.003091897895161376, -0.049804644146731226, 0.007615466404899927, 2.147549328957654e-05, -0.014888542143042314, 0.01459631606640169, 0.03715560781640422, -0.010053033613065702, -0.0005868968849757839, 0.02596283341661325, -0.003749697922962487, -0.03648391883316264, -0.004803180730064169, 0.00012575733592552765, -0.00775810632295365, -0.004016046572048996, -0.019367820581029684, 0.006454867526180157, 0.026586655503861877, -0.01024016384262143, 0.007402465136990367, -0.006449438792331352, -0.02512780229004079, 0.02787152070382661, -0.0008034698173754534, -0.015389165210377725, -0.007381929588435911, -0.01090791417713743, -0.00783033680308871, -0.0540401003017206, -0.022896486030271748, -0.022860662390711866, -0.010128700920324071, -0.01562300472481163, 0.024349366059375415, 0.006242706853974337, 0.002979504738103547, 0.030281284993969648, 0.010251823497908052, -0.02747895631956256, 0.012380858037956124, -0.023764660221772118, 0.0037477301119166148, 0.02630965304640158, 0.039453486804628395, 0.004568796017402987, 0.015182266845804128, 0.0020454071149813434, 0.01572910206262157, -0.010624952769591247, 0.02863085963357893, 0.019439261774369124, 0.003245189272762717, 0.01613183487823811, -0.019127661277108136, 0.004828060442538217, -0.006810029606302815, 0.017865145740057573, 0.021887610390591398, -0.02825603573418484, 0.004900917694431072, -0.011759452931048109, 0.016920723135006824, 0.022394527339467188, 0.0037129134130686053, 0.011970882265497359, 0.004939384661007076, -0.030061635259772065, -0.0011679607640535672, 0.008769005681135233, -0.022043705073716734, 0.00019834489548656086, 0.006022754966001259, 0.03486572676829425, 0.0070986047886436535, -0.005224217985200893, 0.018092847479384742, -0.004234929460591938, 0.022698617421554948, 0.015475133116811262, 0.022612878366247324, -0.0227792158166021, -0.0036951802848242572, 0.011171235903198334, -0.0016524181797984273, -0.017003094699180408, -0.0215808706781772, 0.008190470874424557, -0.010505076843601358, -0.0021668198601989125, 0.03451513080929325, 0.011628842221400456, 0.003983101417850952, 0.030658124936787007, 0.05063286910543966, 0.026705686569175123, 0.03706102419815643, 0.0046784005566448935, 0.016452159438813327, 0.025357492725863795, -0.039281078581764176, 0.023991578178666005, -0.015256423218347609, 0.024175166644709763, 0.003104392751674045, 0.048202355976965476, -0.008246752625442449, -0.018507141574877928, -0.004306097308812239, 0.007869890309503549, 0.04460965502379857, 0.004178368129067559, -0.04194531328422767, 0.02089950727427469, 0.014028746929165887, 0.010631211585555673, 0.007316685276397907, -0.0006242595175629545, -0.0030267406154680728, -0.07161617999966198, -0.012972298577471258, -0.06085608844874988, -0.026654879119863233, 0.026723102465009665, 0.0049157996336567705, 0.00838536869960997, -0.03760771607309827, 0.023218651125379745, 0.017806077691452225, -0.018170121922292407, -0.014032088918472692, 0.0020657090725755196, 0.0029229641853748514, 0.0058503462927520945, -0.006904188005515025, 0.004739453096168005, 0.023062700149139116, -0.014899487254303057, -0.01833940314406302, 0.016616673247332896, 0.03948323369260662, 0.008717755013573705, -0.013725989561584398, -0.011457377695482747, 0.006831623096420361, -0.010888644174006626, 0.0145671754195238, -0.004213945567032327, -0.014989128429560687, -0.014360504036603165, -0.022077510451871556, -0.019186454441519847, 0.006636389752329622, 0.012161189368408522, 0.035269845104138435, 0.02472430583978188, -0.013305974324986715, 0.0006525931023367785, -0.012206685302904323, -0.020376317726339993, 4.532133411941837e-05, 0.0051652258912171425, -0.007888565078718452, -0.033014540015404145, -0.031424021106272404, 0.005984419678187294, 0.02661979239214732, -0.03745064875522273, 0.0038959525265203805, -0.014991860268217168, -0.014009429546479569, 0.012556875143765581, 0.021146237394155012, 0.007126498478408708, 0.01925508543379571, 0.009089275273218969, -0.014768291228398123, 0.02311780572045556, 0.006257969867405289, 0.038444899523888146, 0.012753051692762099, 0.022408209179044274, -0.00023903637675512732, -0.008718599111473865, 0.021007201968640036, -0.019801286105070363, 0.012953609130664795, 0.004333748260157227, 0.008290677153130206, 0.00783358746858417, 0.006500429674762224, -0.0056189670207875695, -0.01116463260971297, 0.0173700107183632, 0.02630405547098172, 0.001480873474756215, -0.005976013082886791, -0.019732502972870183, -0.007163813682959324, 0.005523645001079665, -0.010942580176867955, 0.03136780205364802, -0.0274324434326902, 0.002423632837302677, -0.0015259538111057178, 0.009097986915854159, 0.01397418408617539, -0.001778622918202553, 0.024309868777667346, 0.005361837005391688, -0.017641880215056707, 0.01747765321813205, -0.014162073058427762, 0.008990664690844287, 0.017603960184456445, 0.010957700518874305, 0.011678209966549368, -0.0008819165706261875, -0.010014925932174663, 0.0015763306651558364, -0.0030564376238343886, 0.000929835479812438, 0.0041075087913955794, -0.017848170442421016, 0.002600392827467826, 0.013473079760022266, -0.0009140126089608904, -0.029764374570014466, -0.017626008700744458, -0.009197395744650925, -0.03234513859139813, 0.017584839168459588, 0.009574462342859085, -0.029306427731769964, -0.0023764926977560847, -0.0014822208947137471, 0.007447908871950496, 0.01509523985500427, -0.004384164238729516, -0.026224956012367188, 0.006093609591839647, -0.014515391747438003, 9.974452651913591e-05, 0.0061647967682995335, -0.004750102570465963, 0.0029272413165620736, -0.032278898764724145, 0.018215907381188383, -0.011655908959333943, -0.005763663351458053, -0.017999274353405235, -0.009075856729071536, 0.02303039284367087, 0.005200096828854972, 0.0015716866752643252, 0.010696012125968456, 0.018225605144369487, 0.0028212398577230928, -0.017531048097898613, -0.002108831328780005, 0.006506402030398981, 0.019353361764049146, -0.008098228262416511, -0.01668885969671008, -0.025476775254497954, -0.008960553213373211, 0.011521294822793892, -0.003682470537902006, 0.019995456021786113, 0.0219694716847828, 0.003383570713819145, 0.00999107599734416, -0.01917806700516616, 0.019538362941537397, 0.000687356084819788, 0.03236885225015438, 0.003541007579236951, 0.00011584672729440606, -0.012056999967210165, -0.0029715789773684803, 0.004466492262156773, -0.01825049693195408, 0.02584272961102531, 0.013489872848603921, 0.007479047233190306, -0.0050421969602494465, -0.00756500865749353, -0.006625805698611562, -0.019629305572915787, 0.0033105236987670188, -0.03209957763809165, 0.007060005356616326, 0.01867818286181273, -0.006503516857367286, -0.01597546154972943, 0.005829140541195429, -0.013541839811603236, 0.0050817926993474705, 0.0028864131178774634, -0.021347197306188743, -0.03538539316954912, -0.00011963717451842268, 0.01056142301278476, 0.006910776608665838, -0.0016329988813529204, 0.010314002605995784, -0.0014734419519071972, -0.009694531599139515, -0.019078800720944693, -0.011701049759508066, -0.012486691439515115, 0.012928389242177388, -0.0102791397953351, 0.026386542180151785, 0.0034873610900857427, -0.016882960969086837, -0.007110012785972072, -0.00020066778679213307, -0.007263932316061792, -0.009775812171361094, -0.026218891976633744, -0.00610663660270462, 0.01958423376887398, -0.010220993207557224, -0.006754692186734152, -0.019026653439097374, 0.006131442556493698, -0.0020511352180046702, 0.0011433455479746117, -0.020050673175232108, 0.0035260807020192826, 0.036622025723894745, -0.03243507918819444, -0.011746793473518861, -0.022785323099986925, 0.009280110441581165, -0.012243745194033491, -0.030838567422209168, -0.0006045471483656515, -0.010033225762082543, -0.0016185802617327842, 0.03204020397353641, 0.017177326589356664, -0.00638975763019388, -0.00040297749543150344, -0.041572445565891666, -0.005623509985377689, -0.02759537266614736, -0.01965870374044442, 0.022654350753671035, 0.028990379376714136, 0.016429306448495638, -0.0030549379598584303, 0.02299513104928473, -0.00804535043518099, -0.005930904477941334, 0.007757617088963183, 0.011198118465325051, 0.013105487136354437, -0.003358182594597746, -0.008286716601877228, 0.008981496151177621, -0.02092684740488879, -0.0019047267917588473, 0.022467230479117407, 0.0005423615532945967, -0.029635433615680808, -0.005420639080975155, -0.029250186014253565, 0.0009543621238193405, -0.0041791142472514435, -0.011967926999608703, -0.004505769302263591, -0.0033851631674199413, 0.0016403966535316593, 0.01664143894028794, -0.045915803112571395, 0.02725539870650264, -0.016590932220387662, -0.02212855955128993, -0.0037615345334762787, 0.0015083352928800151, -0.001784621056365858, -0.013569005350539419, -0.01056590829109539, -0.036013129891537425, -0.019745310927244332, -0.045925306988991124, 0.023115111873694475, -0.00964230812643344, 0.000945906855655187, -0.021591956697020867, -0.006341646689642647, 0.011534782404492658, -0.0008381111349470192, -0.019025669713103235, 0.00493861213965557, -0.007184152380406814, 0.01919259365659956, -0.018112931616547964, -0.024176485461137567, 0.012105244046619145, 0.04669985199166664, 0.05216638587114396, 0.003141747474178495, 0.008125346766594252, -0.017829746017845004, -0.03581597540705662, 0.08252016044177408, -0.004734795527668782, 0.029086546308421016, 0.0031725594666398237, -0.008916862002722318, -0.0016553762855972276, -0.027765801003464635, 0.023248884032012394, 0.009281001488730789, 0.0017280988773096225, -0.018145508886283263, 0.046920158384578976, -0.00315686316239655, 0.0006648882093158667, -0.000773155551582054, -0.00498669570531062, 0.01489455734522105, -0.006889930763394938, -0.003915970620842245, 0.005214848501195361, 0.006951655340462014, 0.013219075046859109, 0.008880845717325269, 0.012616488936041222, 0.007475286382641342, 0.0017848068676978043, -0.002791793158013464, -0.007435541667169458, 0.016382809476298762, -0.00020474922525396956, -0.023781133758290836, -0.014512558124488685, 0.021019842596129434, 0.016298997460649337, 0.0026103040385896774, 0.008241255229365665, -0.02850833937085829, -0.01941962226689655, 0.02129491647578694, 0.020189560710014258, -0.014258628242383067, 0.014710094253351406, -0.013575568275437263, -0.013237565422845777, -0.005483621997096349, 0.010019275807849858, -0.042793550804244414, -0.02062212886955158, -0.007672535657099994, 0.03584536912864452, 0.0006828121926839271, -0.004640157319569601, 0.005068422946382728, -0.0023799034873180605, 0.015205286853418794, 0.009963367326174729, -0.007015650563969918, -0.011047005234103823, 0.02164766872810946, -0.0012468747949219302, 0.007248510478524225, -0.010348183157642873, -0.010316625646647657, 0.019942430710031305, 0.00045710728175338695, -0.004854607165502281, 0.03211967885745852, -0.0038812341397617203, 0.015493330672765376, 0.004008373845337199, 0.005733564049654772, -0.026403104959861933, 0.007466974935833393, -0.014470829663126185, -0.014181009786614667, -0.0046986266874338815, -0.008846336515883851, 0.014844637111742099, -0.008982037396145632, 0.009499820898121717, 0.009720167202755078, -0.013848453813409267, -0.004426836849356088, -0.0030103252822334162, 0.016289169579136613, 0.020730288148370368, -0.005257293311363434, -0.002254255622303725, 0.002940380356621668, -0.023848341814867433, -0.0071599139275429335, 0.009606313102283645, -0.007180230660615629, -0.012676299329332067, 0.012792487330180333, -0.009357746824229124, 0.011076472874967342, -0.012189036678161598, 0.0003822349200828588, 0.005101510774406709, -0.009380997293732927, -0.0011532549982743705, -0.018218762759336166, -0.037856435481494866, -0.003146356064577454, -0.013718487798321778, -0.022667371811302493, 0.017031031101923895, 0.00859134478592098, -0.00407406788099391, 0.0005296885561275247, -0.023940831874483766, 0.021224334056709503, -0.028284626442321454, -0.004829246646465208, 0.011921388919308145, 0.013192237274910435, -0.004274061382598267, -0.023995233959032136, 0.009825609936868776, 0.005495977504836248, 0.0019011712880394856, -0.012050222001101417, -0.013391124699696574, 0.002803360622332426, -0.0071169092344674926, -0.00025004381475513947, 0.0002403990281255723, -0.01560905098888169, -0.01515541652603255, 0.020051218907538127, -0.009460517291941441, 0.017424457882442977, 0.033890644887447405, 0.015687615066980576, 0.00015818969890498013, 0.006375672745280722, -0.007524297889359785, -0.018118992780142177, -0.0036609726853510396, -0.012564935952930604, -0.004373504150787298, 0.004312094587881298, -0.005536711665875277, 0.008574712669814476, -0.010686446362042846, 0.01595898020225049, -0.017529112379448583, -0.03192859829577922, 0.006219104939436383, -0.022174343084426965, -0.012397761463689748, -0.008332119800009691, 0.016520349399271148, -0.009134148895956887, 0.002039882806971184, 0.012940647978122407, 0.05537271744119987, 0.014829890926495361, -0.008491524959640747, 0.011893349294080882, -0.018710567120553245, 0.021205406552840048, -0.018572461557016525, -0.02103174355974359, 0.02276722727138637, -0.03092092537100691, 0.012448660728616364, 0.004155218869582078, -0.011605183782710167, -0.018860658694216374, 0.028611650318758845, 0.025070530831556, -0.01379541507799406, 0.020845668272865686, 0.004260672874013394, -0.008672333827932576, -0.004345251732751088, -0.014833381155726596, -0.004395660876790782, 0.007918671429915218, 0.015469341599735622, -0.003407431900399828, -0.003736809400226744, -0.006804545018373027, -0.004366444924995236, -0.003988305081649847, -0.03171269166735919, -0.00888579799354853, 0.005073044071286641, 0.004642217141444651, 0.02775949303454789, -0.0023328587820568487, -0.005173821960538232, -0.0010366671730177705, 0.01034666883346779, -0.023883887667545383, -0.003419215875519324, 0.004981073691545429, -0.016221218736136366, 0.011490593636870277, -0.017207792016071123, -0.0066272675709712296, 0.009179788859412539, -0.002303289874694466, 0.011036019800076813, -0.0123950467346915, 0.007023433504797852, -0.001913582312118686, -0.020955081972512175, -0.02406987471975334, -0.006675044667356603, 0.016823401272334733, -0.01840207310961472, 0.00795276112966363, 0.04074271195557102, -0.00898460582120039, 0.02327539101793829, 0.01276706200168567, -0.005409827893105421, 0.007585116589545611, -0.01695749477375579, 0.043288450558995765, 0.011323192315088581, 0.007717247543255483, -0.0071934067250821725, 0.018306982032537365, -0.012107087353798281, 0.00228850912768676, 0.013733264122574588, 0.008840563076882295, -0.008452429786419424, 0.0092531507598892, 0.011264320197456209, 0.004665237334221037, -0.0013174365879413624, 0.0038745701505659208, -0.005505304892477527, 0.003856162495137149, -0.027267831343950867, 0.028578674789798354, 0.01326003890342003, 0.004865293865976134, -0.014078866979635988, 0.010979772666584396, 0.0001847316389189508, -0.010794485112004048, 0.002169936640632085, -0.01814241138992166, 0.0037651216528476034, 0.02457459341611706, 0.009049749833998134, -0.002841257383444529, 0.005659299751633903, 1.201665613027668e-05, 0.010728655148578986, 0.007915979174678832, 0.0019313989256785021, 0.0001980909559962547, 0.008780293085845405, -0.014044294010933586, -0.026172776237607328, 0.007562448451660866, -0.03810160077275063, 0.002955902427961353, -0.00022231668114345687, -0.01995586385974294, 0.009924531400129962, 0.023828697045966706, 0.0007103484463004697, -0.04379695882152997, 0.015285044978772283, 0.007603951691242659, -0.0016250689215570838, -0.004804638628262346, -0.004360607031802904, -0.01186371000090876, -0.004148577266200218, -0.00911376811267803, -0.005216723288655112, -0.023141124647005542, 0.010350703008147345, 0.02381701973003442, 0.0026143561787552614, 0.0005948008206559025, 0.009961921569025223, -0.011209313794954065, 0.00022230422276463255, -0.02387208502092803, -0.006563720620588963, -0.004498962516207914, 0.0034528720076618105, -0.002668238182476675, -0.00410240001494745, 0.0125150237742066, 0.0012148402757187874, -0.006279661830551914, -0.008015872276826742, -0.02043952188486207, 0.0025083777300618257, -0.00016788216941982773, -0.005264378146078393, 0.025553522492313793, -0.006217634182871668, -0.03097087985213008, 0.012414237749987886, -0.012543782179526404, -0.006140138998826424, -0.013388274026809678, 0.010928558265478578, 0.021208989100586544, 0.00806218472344013, -0.0015716380152763496, -0.00808792573726774, 0.003234698994792947, -0.004477251887874882, -0.014336064155740605, -0.0018670727381694268, 0.008385376873245582, 0.015882727867625363, 0.008182048179583897, 0.0009604700430048352, 0.002559190921095222, -0.019003056332283018, 0.0070078491043583805, -0.002242297376386307, 0.013650755379783082, 0.0002502859354096624, -0.004919283757963686, -0.008818350539242707, -0.004184509458178949, -0.017540140318713333, 0.0018139574801990098, -0.012451971883473148, -0.018370356600675364, -0.04064441236381961, 0.0030023410759476682, -0.00788125247232171, -0.002361866826945424, -0.000610422517101624, -0.03245637886555196, -0.006380703666385617, -0.05892574679864604, -0.0093102116183759, -0.0071616722160561305, -0.005666925767296861, 0.002872846021299276, 0.0018900638989574354, -0.0002401947218327969, -0.012947516782229128, 0.023013763659343336, 0.014716824706709167, -0.04268874347722894, 0.0033461555722734937, 0.03583434037733432, 0.04628382013146372, -0.022763854563718378, 0.006471701962412403, 0.0015212873547521889, -0.011155494484034295, -0.01439019415774508, -0.02882097406221634, 0.017350360084118565, 0.03123725579992476, -0.00885818958828417, -0.006039769693646441, -0.006313837961279192, -0.021459393634794356, 0.013014217620293433, -0.0005910235171350246, 0.022017216181499846, 0.0005634818996770975, 0.015022288625806245, 0.007347054180795004, -0.003640387283045425, 0.0018756825892064666, 0.030603990620776386, 0.03660778673803202, 0.002744362319310253, -0.010646022233785312, -0.006638457057001698, -0.014611750195774027, 0.023237205678859185, -0.014956205530861228, 0.001805266351105405, -0.007933039562013768, -0.00841455232175155, 0.01918904188476292, 0.010554036464902316, 0.015233721065681504, -0.015595874931402956, -0.01046406611323651, 0.009112796347612369, 0.004437546189699791, 0.009449467263871239, -0.015426261483355127, 0.002182155897867672, 0.017388568026555452, 0.013274286991172578, 0.01727424997898196, -0.011487596002799704, -0.00575974110748331, -0.008865602613237539, -0.02371189392985165, 0.0019038247625565055, 0.01339018831265877, -0.01075350939162635, -0.0027034581902404798, -0.014991411170683425, 0.0023780584473920567, 0.005660680876834399, 0.009694336703455039, 0.004298733344738102, -0.005075025136519254, 0.01398703887272292, -0.0024321660863280982, 0.0034022700240647972, -0.013253791767198332, -0.0054623908536217715, -0.0006688251884916588, 0.011600505702539229, -0.0057082670632726835, -0.009913072639972686, -0.010955223439245357, -0.0237146361434157, -0.004865096240512547, -0.010117675858089644, 0.012607140914104323, -0.004848752958513869, 0.006194770309696312, 0.0005156657872474521, 0.007018494502632181, -0.006147919397173227, -0.007561765553256105, -0.01580674567000906, -0.00190159580152733, 0.009368336820404775, -0.014786285557487735, 0.0072542988504096925, 0.01758823156317907, 0.023327410845785827, -0.0037332329592117126, 0.004750145114700852, -0.009395895149932273, 0.0011360209038887186, -0.004725751311732079, 0.002390372568414127, 0.008535344077603145, 0.03197303754476684, -0.004184769907368288, -0.015877344281815763, 0.016350972420479122, 0.011112562775214439, -0.0050650519327692265, -0.002450334022006685, 1.3284764236485276e-06, 0.0034201580087758286, -0.015629851022914275, -0.0041034979933073455, -0.004018442564834624, -0.02051633557541129, -0.004213302323649879, 0.002754717531973417, -0.011622877249605952, -0.02903173544468127, -0.0021980244505841002, 0.0014166971916529099, 0.007186958511254297, -0.016047014206036093, 0.01346683949426789, -0.0006276158156307078, 0.012900131291532765, -0.0011923849476510547, -0.007234749577860075, -0.03369819695464968, -0.001800250818127124, -0.010410626510968109, -0.01663511523783768, -0.002799812989298804, 0.007335191837291307, 0.005831506868067018, -0.0016293737912013815, 0.011429532148020545, -0.01594721500287152, -0.010205952825716605, 0.012253809135775938, 0.026111819921625666, -0.00670469372967922, 0.008040480327610022, -0.01269814284394335, 0.004393963895976265, 0.002865310290593317, -0.01325247564123618, 0.00939755750881877, 0.030143538884597854, 0.005292480924684844, 0.002641676664829123, -0.0021603813655249457, 0.008203599068002075, 0.009166276731401805, -0.016067833468777586, 0.008587433396309872, -0.009051455408974044, -0.011184975363252398, -0.021731929673344086, -0.022126886913554463, ...]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(cp)==len(op)</span><br></pre></td></tr></table></figure><pre><code>True</code></pre><h3 id="三、检验13个行业总体的开-收盘价的收益率是否符合正态分布"><a href="#三、检验13个行业总体的开-收盘价的收益率是否符合正态分布" class="headerlink" title="三、检验13个行业总体的开/收盘价的收益率是否符合正态分布"></a>三、检验13个行业总体的开/收盘价的收益率是否符合正态分布</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.hist(op)</span><br><span class="line">plt.hist(cp)</span><br></pre></td></tr></table></figure><pre><code>(array([   54.,   146.,   314.,  1319.,  7790., 14480.,  3561.,   607.,          123.,    24.]), array([-0.09991755, -0.08084738, -0.06177721, -0.04270704, -0.02363687,        -0.0045667 ,  0.01450346,  0.03357363,  0.0526438 ,  0.07171397,         0.09078414]), &lt;a list of 10 Patch objects&gt;)</code></pre><p><img src="output_15_1.png" alt="png"></p><p>上图中蓝色和黄色的直方图分别是开盘价和收盘价的，可见13个行业总体的开/收盘价<strong>近似服从正态分布</strong></p><h3 id="四、将13个行业的收益率（以开盘价收益率为例）分为13组，便于后续对单个行业的收益率进行分析"><a href="#四、将13个行业的收益率（以开盘价收益率为例）分为13组，便于后续对单个行业的收益率进行分析" class="headerlink" title="四、将13个行业的收益率（以开盘价收益率为例）分为13组，便于后续对单个行业的收益率进行分析"></a>四、将13个行业的收益率（以开盘价收益率为例）分为13组，便于后续对单个行业的收益率进行分析</h3><p>由于一共选取了13个行业的数据，且每一个行业都总共有2187个样本，根据计算日收益率的公式可知，每一个行业在经过计算后会得到2187-1=2186个数值 <br><br>来验证下，以op（开盘价）为例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(op)</span><br></pre></td></tr></table></figure><pre><code>28418</code></pre><p>len(op)，即13个行业总共计算所得到的日收益率的个数为28418，那么它除以（2188-1）应该等于13</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(op)/<span class="number">2186</span></span><br></pre></td></tr></table></figure><pre><code>13.0</code></pre><p>结果为13，从而验证成功</p><p>下面分析各行业本身的开/收盘价收益率,即分别从op和cp中每2186个作一次断点，并将它们分别存入一个数据框，以op（开盘价日收益率）为例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">op_df=pd.DataFrame(columns=(<span class="string">'code1'</span>,<span class="string">'code2'</span>,<span class="string">'code3'</span>,<span class="string">'code4'</span>,<span class="string">'code5'</span>,<span class="string">'code6'</span>,<span class="string">'code7'</span>,<span class="string">'code8'</span>,<span class="string">'code9'</span>,<span class="string">'code10'</span>,<span class="string">'code11'</span>,<span class="string">'code12'</span>,<span class="string">'code13'</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">op_df</span><br></pre></td></tr></table></figure><div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>code1</th><br>      <th>code2</th><br>      <th>code3</th><br>      <th>code4</th><br>      <th>code5</th><br>      <th>code6</th><br>      <th>code7</th><br>      <th>code8</th><br>      <th>code9</th><br>      <th>code10</th><br>      <th>code11</th><br>      <th>code12</th><br>      <th>code13</th><br>    </tr><br>  </thead><br>  <tbody><br>  </tbody><br></table><br></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">r1=r2=r3=r4=r5=r6=r7=r8=r9=r10=r11=r12=r13=[]</span><br><span class="line">r=[[],[],[],[],[],[],[],[],[],[],[],[],[]]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">13</span>):</span><br><span class="line">    r[i]=op[<span class="number">2186</span>*i:<span class="number">21876</span>+<span class="number">2186</span>*i]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">op_df[<span class="string">'code1'</span>]=r[<span class="number">0</span>]</span><br><span class="line">op_df[<span class="string">'code2'</span>]=r[<span class="number">0</span>]</span><br><span class="line">op_df[<span class="string">'code3'</span>]=r[<span class="number">0</span>]</span><br><span class="line">op_df[<span class="string">'code4'</span>]=r[<span class="number">0</span>]</span><br><span class="line">op_df[<span class="string">'code5'</span>]=r[<span class="number">0</span>]</span><br><span class="line">op_df[<span class="string">'code6'</span>]=r[<span class="number">0</span>]</span><br><span class="line">op_df[<span class="string">'code7'</span>]=r[<span class="number">0</span>]</span><br><span class="line">op_df[<span class="string">'code8'</span>]=r[<span class="number">0</span>]</span><br><span class="line">op_df[<span class="string">'code9'</span>]=r[<span class="number">0</span>]</span><br><span class="line">op_df[<span class="string">'code10'</span>]=r[<span class="number">0</span>]</span><br><span class="line">op_df[<span class="string">'code11'</span>]=r[<span class="number">0</span>]</span><br><span class="line">op_df[<span class="string">'code12'</span>]=r[<span class="number">0</span>]</span><br><span class="line">op_df[<span class="string">'code13'</span>]=r[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">op_df</span><br></pre></td></tr></table></figure><div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>code1</th><br>      <th>code2</th><br>      <th>code3</th><br>      <th>code4</th><br>      <th>code5</th><br>      <th>code6</th><br>      <th>code7</th><br>      <th>code8</th><br>      <th>code9</th><br>      <th>code10</th><br>      <th>code11</th><br>      <th>code12</th><br>      <th>code13</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>-0.006362</td><br>      <td>-0.006362</td><br>      <td>-0.006362</td><br>      <td>-0.006362</td><br>      <td>-0.006362</td><br>      <td>-0.006362</td><br>      <td>-0.006362</td><br>      <td>-0.006362</td><br>      <td>-0.006362</td><br>      <td>-0.006362</td><br>      <td>-0.006362</td><br>      <td>-0.006362</td><br>      <td>-0.006362</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>0.005651</td><br>      <td>0.005651</td><br>      <td>0.005651</td><br>      <td>0.005651</td><br>      <td>0.005651</td><br>      <td>0.005651</td><br>      <td>0.005651</td><br>      <td>0.005651</td><br>      <td>0.005651</td><br>      <td>0.005651</td><br>      <td>0.005651</td><br>      <td>0.005651</td><br>      <td>0.005651</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>0.011367</td><br>      <td>0.011367</td><br>      <td>0.011367</td><br>      <td>0.011367</td><br>      <td>0.011367</td><br>      <td>0.011367</td><br>      <td>0.011367</td><br>      <td>0.011367</td><br>      <td>0.011367</td><br>      <td>0.011367</td><br>      <td>0.011367</td><br>      <td>0.011367</td><br>      <td>0.011367</td><br>    </tr><br>    <tr><br>      <th>3</th><br>      <td>-0.035324</td><br>      <td>-0.035324</td><br>      <td>-0.035324</td><br>      <td>-0.035324</td><br>      <td>-0.035324</td><br>      <td>-0.035324</td><br>      <td>-0.035324</td><br>      <td>-0.035324</td><br>      <td>-0.035324</td><br>      <td>-0.035324</td><br>      <td>-0.035324</td><br>      <td>-0.035324</td><br>      <td>-0.035324</td><br>    </tr><br>    <tr><br>      <th>4</th><br>      <td>0.020105</td><br>      <td>0.020105</td><br>      <td>0.020105</td><br>      <td>0.020105</td><br>      <td>0.020105</td><br>      <td>0.020105</td><br>      <td>0.020105</td><br>      <td>0.020105</td><br>      <td>0.020105</td><br>      <td>0.020105</td><br>      <td>0.020105</td><br>      <td>0.020105</td><br>      <td>0.020105</td><br>    </tr><br>    <tr><br>      <th>5</th><br>      <td>-0.014392</td><br>      <td>-0.014392</td><br>      <td>-0.014392</td><br>      <td>-0.014392</td><br>      <td>-0.014392</td><br>      <td>-0.014392</td><br>      <td>-0.014392</td><br>      <td>-0.014392</td><br>      <td>-0.014392</td><br>      <td>-0.014392</td><br>      <td>-0.014392</td><br>      <td>-0.014392</td><br>      <td>-0.014392</td><br>    </tr><br>    <tr><br>      <th>6</th><br>      <td>-0.013050</td><br>      <td>-0.013050</td><br>      <td>-0.013050</td><br>      <td>-0.013050</td><br>      <td>-0.013050</td><br>      <td>-0.013050</td><br>      <td>-0.013050</td><br>      <td>-0.013050</td><br>      <td>-0.013050</td><br>      <td>-0.013050</td><br>      <td>-0.013050</td><br>      <td>-0.013050</td><br>      <td>-0.013050</td><br>    </tr><br>    <tr><br>      <th>7</th><br>      <td>-0.002084</td><br>      <td>-0.002084</td><br>      <td>-0.002084</td><br>      <td>-0.002084</td><br>      <td>-0.002084</td><br>      <td>-0.002084</td><br>      <td>-0.002084</td><br>      <td>-0.002084</td><br>      <td>-0.002084</td><br>      <td>-0.002084</td><br>      <td>-0.002084</td><br>      <td>-0.002084</td><br>      <td>-0.002084</td><br>    </tr><br>    <tr><br>      <th>8</th><br>      <td>0.010545</td><br>      <td>0.010545</td><br>      <td>0.010545</td><br>      <td>0.010545</td><br>      <td>0.010545</td><br>      <td>0.010545</td><br>      <td>0.010545</td><br>      <td>0.010545</td><br>      <td>0.010545</td><br>      <td>0.010545</td><br>      <td>0.010545</td><br>      <td>0.010545</td><br>      <td>0.010545</td><br>    </tr><br>    <tr><br>      <th>9</th><br>      <td>-0.000497</td><br>      <td>-0.000497</td><br>      <td>-0.000497</td><br>      <td>-0.000497</td><br>      <td>-0.000497</td><br>      <td>-0.000497</td><br>      <td>-0.000497</td><br>      <td>-0.000497</td><br>      <td>-0.000497</td><br>      <td>-0.000497</td><br>      <td>-0.000497</td><br>      <td>-0.000497</td><br>      <td>-0.000497</td><br>    </tr><br>    <tr><br>      <th>10</th><br>      <td>0.011264</td><br>      <td>0.011264</td><br>      <td>0.011264</td><br>      <td>0.011264</td><br>      <td>0.011264</td><br>      <td>0.011264</td><br>      <td>0.011264</td><br>      <td>0.011264</td><br>      <td>0.011264</td><br>      <td>0.011264</td><br>      <td>0.011264</td><br>      <td>0.011264</td><br>      <td>0.011264</td><br>    </tr><br>    <tr><br>      <th>11</th><br>      <td>0.006157</td><br>      <td>0.006157</td><br>      <td>0.006157</td><br>      <td>0.006157</td><br>      <td>0.006157</td><br>      <td>0.006157</td><br>      <td>0.006157</td><br>      <td>0.006157</td><br>      <td>0.006157</td><br>      <td>0.006157</td><br>      <td>0.006157</td><br>      <td>0.006157</td><br>      <td>0.006157</td><br>    </tr><br>    <tr><br>      <th>12</th><br>      <td>-0.040683</td><br>      <td>-0.040683</td><br>      <td>-0.040683</td><br>      <td>-0.040683</td><br>      <td>-0.040683</td><br>      <td>-0.040683</td><br>      <td>-0.040683</td><br>      <td>-0.040683</td><br>      <td>-0.040683</td><br>      <td>-0.040683</td><br>      <td>-0.040683</td><br>      <td>-0.040683</td><br>      <td>-0.040683</td><br>    </tr><br>    <tr><br>      <th>13</th><br>      <td>-0.023930</td><br>      <td>-0.023930</td><br>      <td>-0.023930</td><br>      <td>-0.023930</td><br>      <td>-0.023930</td><br>      <td>-0.023930</td><br>      <td>-0.023930</td><br>      <td>-0.023930</td><br>      <td>-0.023930</td><br>      <td>-0.023930</td><br>      <td>-0.023930</td><br>      <td>-0.023930</td><br>      <td>-0.023930</td><br>    </tr><br>    <tr><br>      <th>14</th><br>      <td>-0.024679</td><br>      <td>-0.024679</td><br>      <td>-0.024679</td><br>      <td>-0.024679</td><br>      <td>-0.024679</td><br>      <td>-0.024679</td><br>      <td>-0.024679</td><br>      <td>-0.024679</td><br>      <td>-0.024679</td><br>      <td>-0.024679</td><br>      <td>-0.024679</td><br>      <td>-0.024679</td><br>      <td>-0.024679</td><br>    </tr><br>    <tr><br>      <th>15</th><br>      <td>-0.000476</td><br>      <td>-0.000476</td><br>      <td>-0.000476</td><br>      <td>-0.000476</td><br>      <td>-0.000476</td><br>      <td>-0.000476</td><br>      <td>-0.000476</td><br>      <td>-0.000476</td><br>      <td>-0.000476</td><br>      <td>-0.000476</td><br>      <td>-0.000476</td><br>      <td>-0.000476</td><br>      <td>-0.000476</td><br>    </tr><br>    <tr><br>      <th>16</th><br>      <td>-0.036607</td><br>      <td>-0.036607</td><br>      <td>-0.036607</td><br>      <td>-0.036607</td><br>      <td>-0.036607</td><br>      <td>-0.036607</td><br>      <td>-0.036607</td><br>      <td>-0.036607</td><br>      <td>-0.036607</td><br>      <td>-0.036607</td><br>      <td>-0.036607</td><br>      <td>-0.036607</td><br>      <td>-0.036607</td><br>    </tr><br>    <tr><br>      <th>17</th><br>      <td>-0.018728</td><br>      <td>-0.018728</td><br>      <td>-0.018728</td><br>      <td>-0.018728</td><br>      <td>-0.018728</td><br>      <td>-0.018728</td><br>      <td>-0.018728</td><br>      <td>-0.018728</td><br>      <td>-0.018728</td><br>      <td>-0.018728</td><br>      <td>-0.018728</td><br>      <td>-0.018728</td><br>      <td>-0.018728</td><br>    </tr><br>    <tr><br>      <th>18</th><br>      <td>-0.000298</td><br>      <td>-0.000298</td><br>      <td>-0.000298</td><br>      <td>-0.000298</td><br>      <td>-0.000298</td><br>      <td>-0.000298</td><br>      <td>-0.000298</td><br>      <td>-0.000298</td><br>      <td>-0.000298</td><br>      <td>-0.000298</td><br>      <td>-0.000298</td><br>      <td>-0.000298</td><br>      <td>-0.000298</td><br>    </tr><br>    <tr><br>      <th>19</th><br>      <td>0.005706</td><br>      <td>0.005706</td><br>      <td>0.005706</td><br>      <td>0.005706</td><br>      <td>0.005706</td><br>      <td>0.005706</td><br>      <td>0.005706</td><br>      <td>0.005706</td><br>      <td>0.005706</td><br>      <td>0.005706</td><br>      <td>0.005706</td><br>      <td>0.005706</td><br>      <td>0.005706</td><br>    </tr><br>    <tr><br>      <th>20</th><br>      <td>-0.018408</td><br>      <td>-0.018408</td><br>      <td>-0.018408</td><br>      <td>-0.018408</td><br>      <td>-0.018408</td><br>      <td>-0.018408</td><br>      <td>-0.018408</td><br>      <td>-0.018408</td><br>      <td>-0.018408</td><br>      <td>-0.018408</td><br>      <td>-0.018408</td><br>      <td>-0.018408</td><br>      <td>-0.018408</td><br>    </tr><br>    <tr><br>      <th>21</th><br>      <td>0.007369</td><br>      <td>0.007369</td><br>      <td>0.007369</td><br>      <td>0.007369</td><br>      <td>0.007369</td><br>      <td>0.007369</td><br>      <td>0.007369</td><br>      <td>0.007369</td><br>      <td>0.007369</td><br>      <td>0.007369</td><br>      <td>0.007369</td><br>      <td>0.007369</td><br>      <td>0.007369</td><br>    </tr><br>    <tr><br>      <th>22</th><br>      <td>0.009978</td><br>      <td>0.009978</td><br>      <td>0.009978</td><br>      <td>0.009978</td><br>      <td>0.009978</td><br>      <td>0.009978</td><br>      <td>0.009978</td><br>      <td>0.009978</td><br>      <td>0.009978</td><br>      <td>0.009978</td><br>      <td>0.009978</td><br>      <td>0.009978</td><br>      <td>0.009978</td><br>    </tr><br>    <tr><br>      <th>23</th><br>      <td>-0.028894</td><br>      <td>-0.028894</td><br>      <td>-0.028894</td><br>      <td>-0.028894</td><br>      <td>-0.028894</td><br>      <td>-0.028894</td><br>      <td>-0.028894</td><br>      <td>-0.028894</td><br>      <td>-0.028894</td><br>      <td>-0.028894</td><br>      <td>-0.028894</td><br>      <td>-0.028894</td><br>      <td>-0.028894</td><br>    </tr><br>    <tr><br>      <th>24</th><br>      <td>0.000834</td><br>      <td>0.000834</td><br>      <td>0.000834</td><br>      <td>0.000834</td><br>      <td>0.000834</td><br>      <td>0.000834</td><br>      <td>0.000834</td><br>      <td>0.000834</td><br>      <td>0.000834</td><br>      <td>0.000834</td><br>      <td>0.000834</td><br>      <td>0.000834</td><br>      <td>0.000834</td><br>    </tr><br>    <tr><br>      <th>25</th><br>      <td>-0.000759</td><br>      <td>-0.000759</td><br>      <td>-0.000759</td><br>      <td>-0.000759</td><br>      <td>-0.000759</td><br>      <td>-0.000759</td><br>      <td>-0.000759</td><br>      <td>-0.000759</td><br>      <td>-0.000759</td><br>      <td>-0.000759</td><br>      <td>-0.000759</td><br>      <td>-0.000759</td><br>      <td>-0.000759</td><br>    </tr><br>    <tr><br>      <th>26</th><br>      <td>0.027136</td><br>      <td>0.027136</td><br>      <td>0.027136</td><br>      <td>0.027136</td><br>      <td>0.027136</td><br>      <td>0.027136</td><br>      <td>0.027136</td><br>      <td>0.027136</td><br>      <td>0.027136</td><br>      <td>0.027136</td><br>      <td>0.027136</td><br>      <td>0.027136</td><br>      <td>0.027136</td><br>    </tr><br>    <tr><br>      <th>27</th><br>      <td>0.002331</td><br>      <td>0.002331</td><br>      <td>0.002331</td><br>      <td>0.002331</td><br>      <td>0.002331</td><br>      <td>0.002331</td><br>      <td>0.002331</td><br>      <td>0.002331</td><br>      <td>0.002331</td><br>      <td>0.002331</td><br>      <td>0.002331</td><br>      <td>0.002331</td><br>      <td>0.002331</td><br>    </tr><br>    <tr><br>      <th>28</th><br>      <td>0.017998</td><br>      <td>0.017998</td><br>      <td>0.017998</td><br>      <td>0.017998</td><br>      <td>0.017998</td><br>      <td>0.017998</td><br>      <td>0.017998</td><br>      <td>0.017998</td><br>      <td>0.017998</td><br>      <td>0.017998</td><br>      <td>0.017998</td><br>      <td>0.017998</td><br>      <td>0.017998</td><br>    </tr><br>    <tr><br>      <th>29</th><br>      <td>0.013007</td><br>      <td>0.013007</td><br>      <td>0.013007</td><br>      <td>0.013007</td><br>      <td>0.013007</td><br>      <td>0.013007</td><br>      <td>0.013007</td><br>      <td>0.013007</td><br>      <td>0.013007</td><br>      <td>0.013007</td><br>      <td>0.013007</td><br>      <td>0.013007</td><br>      <td>0.013007</td><br>    </tr><br>    <tr><br>      <th>…</th><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>      <td>…</td><br>    </tr><br>    <tr><br>      <th>21846</th><br>      <td>0.007553</td><br>      <td>0.007553</td><br>      <td>0.007553</td><br>      <td>0.007553</td><br>      <td>0.007553</td><br>      <td>0.007553</td><br>      <td>0.007553</td><br>      <td>0.007553</td><br>      <td>0.007553</td><br>      <td>0.007553</td><br>      <td>0.007553</td><br>      <td>0.007553</td><br>      <td>0.007553</td><br>    </tr><br>    <tr><br>      <th>21847</th><br>      <td>0.005566</td><br>      <td>0.005566</td><br>      <td>0.005566</td><br>      <td>0.005566</td><br>      <td>0.005566</td><br>      <td>0.005566</td><br>      <td>0.005566</td><br>      <td>0.005566</td><br>      <td>0.005566</td><br>      <td>0.005566</td><br>      <td>0.005566</td><br>      <td>0.005566</td><br>      <td>0.005566</td><br>    </tr><br>    <tr><br>      <th>21848</th><br>      <td>-0.006884</td><br>      <td>-0.006884</td><br>      <td>-0.006884</td><br>      <td>-0.006884</td><br>      <td>-0.006884</td><br>      <td>-0.006884</td><br>      <td>-0.006884</td><br>      <td>-0.006884</td><br>      <td>-0.006884</td><br>      <td>-0.006884</td><br>      <td>-0.006884</td><br>      <td>-0.006884</td><br>      <td>-0.006884</td><br>    </tr><br>    <tr><br>      <th>21849</th><br>      <td>0.010154</td><br>      <td>0.010154</td><br>      <td>0.010154</td><br>      <td>0.010154</td><br>      <td>0.010154</td><br>      <td>0.010154</td><br>      <td>0.010154</td><br>      <td>0.010154</td><br>      <td>0.010154</td><br>      <td>0.010154</td><br>      <td>0.010154</td><br>      <td>0.010154</td><br>      <td>0.010154</td><br>    </tr><br>    <tr><br>      <th>21850</th><br>      <td>-0.029378</td><br>      <td>-0.029378</td><br>      <td>-0.029378</td><br>      <td>-0.029378</td><br>      <td>-0.029378</td><br>      <td>-0.029378</td><br>      <td>-0.029378</td><br>      <td>-0.029378</td><br>      <td>-0.029378</td><br>      <td>-0.029378</td><br>      <td>-0.029378</td><br>      <td>-0.029378</td><br>      <td>-0.029378</td><br>    </tr><br>    <tr><br>      <th>21851</th><br>      <td>-0.013177</td><br>      <td>-0.013177</td><br>      <td>-0.013177</td><br>      <td>-0.013177</td><br>      <td>-0.013177</td><br>      <td>-0.013177</td><br>      <td>-0.013177</td><br>      <td>-0.013177</td><br>      <td>-0.013177</td><br>      <td>-0.013177</td><br>      <td>-0.013177</td><br>      <td>-0.013177</td><br>      <td>-0.013177</td><br>    </tr><br>    <tr><br>      <th>21852</th><br>      <td>0.005453</td><br>      <td>0.005453</td><br>      <td>0.005453</td><br>      <td>0.005453</td><br>      <td>0.005453</td><br>      <td>0.005453</td><br>      <td>0.005453</td><br>      <td>0.005453</td><br>      <td>0.005453</td><br>      <td>0.005453</td><br>      <td>0.005453</td><br>      <td>0.005453</td><br>      <td>0.005453</td><br>    </tr><br>    <tr><br>      <th>21853</th><br>      <td>-0.019841</td><br>      <td>-0.019841</td><br>      <td>-0.019841</td><br>      <td>-0.019841</td><br>      <td>-0.019841</td><br>      <td>-0.019841</td><br>      <td>-0.019841</td><br>      <td>-0.019841</td><br>      <td>-0.019841</td><br>      <td>-0.019841</td><br>      <td>-0.019841</td><br>      <td>-0.019841</td><br>      <td>-0.019841</td><br>    </tr><br>    <tr><br>      <th>21854</th><br>      <td>0.006501</td><br>      <td>0.006501</td><br>      <td>0.006501</td><br>      <td>0.006501</td><br>      <td>0.006501</td><br>      <td>0.006501</td><br>      <td>0.006501</td><br>      <td>0.006501</td><br>      <td>0.006501</td><br>      <td>0.006501</td><br>      <td>0.006501</td><br>      <td>0.006501</td><br>      <td>0.006501</td><br>    </tr><br>    <tr><br>      <th>21855</th><br>      <td>0.000546</td><br>      <td>0.000546</td><br>      <td>0.000546</td><br>      <td>0.000546</td><br>      <td>0.000546</td><br>      <td>0.000546</td><br>      <td>0.000546</td><br>      <td>0.000546</td><br>      <td>0.000546</td><br>      <td>0.000546</td><br>      <td>0.000546</td><br>      <td>0.000546</td><br>      <td>0.000546</td><br>    </tr><br>    <tr><br>      <th>21856</th><br>      <td>0.005091</td><br>      <td>0.005091</td><br>      <td>0.005091</td><br>      <td>0.005091</td><br>      <td>0.005091</td><br>      <td>0.005091</td><br>      <td>0.005091</td><br>      <td>0.005091</td><br>      <td>0.005091</td><br>      <td>0.005091</td><br>      <td>0.005091</td><br>      <td>0.005091</td><br>      <td>0.005091</td><br>    </tr><br>    <tr><br>      <th>21857</th><br>      <td>0.001089</td><br>      <td>0.001089</td><br>      <td>0.001089</td><br>      <td>0.001089</td><br>      <td>0.001089</td><br>      <td>0.001089</td><br>      <td>0.001089</td><br>      <td>0.001089</td><br>      <td>0.001089</td><br>      <td>0.001089</td><br>      <td>0.001089</td><br>      <td>0.001089</td><br>      <td>0.001089</td><br>    </tr><br>    <tr><br>      <th>21858</th><br>      <td>0.008112</td><br>      <td>0.008112</td><br>      <td>0.008112</td><br>      <td>0.008112</td><br>      <td>0.008112</td><br>      <td>0.008112</td><br>      <td>0.008112</td><br>      <td>0.008112</td><br>      <td>0.008112</td><br>      <td>0.008112</td><br>      <td>0.008112</td><br>      <td>0.008112</td><br>      <td>0.008112</td><br>    </tr><br>    <tr><br>      <th>21859</th><br>      <td>-0.034683</td><br>      <td>-0.034683</td><br>      <td>-0.034683</td><br>      <td>-0.034683</td><br>      <td>-0.034683</td><br>      <td>-0.034683</td><br>      <td>-0.034683</td><br>      <td>-0.034683</td><br>      <td>-0.034683</td><br>      <td>-0.034683</td><br>      <td>-0.034683</td><br>      <td>-0.034683</td><br>      <td>-0.034683</td><br>    </tr><br>    <tr><br>      <th>21860</th><br>      <td>-0.015120</td><br>      <td>-0.015120</td><br>      <td>-0.015120</td><br>      <td>-0.015120</td><br>      <td>-0.015120</td><br>      <td>-0.015120</td><br>      <td>-0.015120</td><br>      <td>-0.015120</td><br>      <td>-0.015120</td><br>      <td>-0.015120</td><br>      <td>-0.015120</td><br>      <td>-0.015120</td><br>      <td>-0.015120</td><br>    </tr><br>    <tr><br>      <th>21861</th><br>      <td>0.010026</td><br>      <td>0.010026</td><br>      <td>0.010026</td><br>      <td>0.010026</td><br>      <td>0.010026</td><br>      <td>0.010026</td><br>      <td>0.010026</td><br>      <td>0.010026</td><br>      <td>0.010026</td><br>      <td>0.010026</td><br>      <td>0.010026</td><br>      <td>0.010026</td><br>      <td>0.010026</td><br>    </tr><br>    <tr><br>      <th>21862</th><br>      <td>-0.006984</td><br>      <td>-0.006984</td><br>      <td>-0.006984</td><br>      <td>-0.006984</td><br>      <td>-0.006984</td><br>      <td>-0.006984</td><br>      <td>-0.006984</td><br>      <td>-0.006984</td><br>      <td>-0.006984</td><br>      <td>-0.006984</td><br>      <td>-0.006984</td><br>      <td>-0.006984</td><br>      <td>-0.006984</td><br>    </tr><br>    <tr><br>      <th>21863</th><br>      <td>-0.026515</td><br>      <td>-0.026515</td><br>      <td>-0.026515</td><br>      <td>-0.026515</td><br>      <td>-0.026515</td><br>      <td>-0.026515</td><br>      <td>-0.026515</td><br>      <td>-0.026515</td><br>      <td>-0.026515</td><br>      <td>-0.026515</td><br>      <td>-0.026515</td><br>      <td>-0.026515</td><br>      <td>-0.026515</td><br>    </tr><br>    <tr><br>      <th>21864</th><br>      <td>0.056931</td><br>      <td>0.056931</td><br>      <td>0.056931</td><br>      <td>0.056931</td><br>      <td>0.056931</td><br>      <td>0.056931</td><br>      <td>0.056931</td><br>      <td>0.056931</td><br>      <td>0.056931</td><br>      <td>0.056931</td><br>      <td>0.056931</td><br>      <td>0.056931</td><br>      <td>0.056931</td><br>    </tr><br>    <tr><br>      <th>21865</th><br>      <td>-0.043535</td><br>      <td>-0.043535</td><br>      <td>-0.043535</td><br>      <td>-0.043535</td><br>      <td>-0.043535</td><br>      <td>-0.043535</td><br>      <td>-0.043535</td><br>      <td>-0.043535</td><br>      <td>-0.043535</td><br>      <td>-0.043535</td><br>      <td>-0.043535</td><br>      <td>-0.043535</td><br>      <td>-0.043535</td><br>    </tr><br>    <tr><br>      <th>21866</th><br>      <td>-0.004693</td><br>      <td>-0.004693</td><br>      <td>-0.004693</td><br>      <td>-0.004693</td><br>      <td>-0.004693</td><br>      <td>-0.004693</td><br>      <td>-0.004693</td><br>      <td>-0.004693</td><br>      <td>-0.004693</td><br>      <td>-0.004693</td><br>      <td>-0.004693</td><br>      <td>-0.004693</td><br>      <td>-0.004693</td><br>    </tr><br>    <tr><br>      <th>21867</th><br>      <td>-0.010431</td><br>      <td>-0.010431</td><br>      <td>-0.010431</td><br>      <td>-0.010431</td><br>      <td>-0.010431</td><br>      <td>-0.010431</td><br>      <td>-0.010431</td><br>      <td>-0.010431</td><br>      <td>-0.010431</td><br>      <td>-0.010431</td><br>      <td>-0.010431</td><br>      <td>-0.010431</td><br>      <td>-0.010431</td><br>    </tr><br>    <tr><br>      <th>21868</th><br>      <td>0.005197</td><br>      <td>0.005197</td><br>      <td>0.005197</td><br>      <td>0.005197</td><br>      <td>0.005197</td><br>      <td>0.005197</td><br>      <td>0.005197</td><br>      <td>0.005197</td><br>      <td>0.005197</td><br>      <td>0.005197</td><br>      <td>0.005197</td><br>      <td>0.005197</td><br>      <td>0.005197</td><br>    </tr><br>    <tr><br>      <th>21869</th><br>      <td>-0.005580</td><br>      <td>-0.005580</td><br>      <td>-0.005580</td><br>      <td>-0.005580</td><br>      <td>-0.005580</td><br>      <td>-0.005580</td><br>      <td>-0.005580</td><br>      <td>-0.005580</td><br>      <td>-0.005580</td><br>      <td>-0.005580</td><br>      <td>-0.005580</td><br>      <td>-0.005580</td><br>      <td>-0.005580</td><br>    </tr><br>    <tr><br>      <th>21870</th><br>      <td>0.007118</td><br>      <td>0.007118</td><br>      <td>0.007118</td><br>      <td>0.007118</td><br>      <td>0.007118</td><br>      <td>0.007118</td><br>      <td>0.007118</td><br>      <td>0.007118</td><br>      <td>0.007118</td><br>      <td>0.007118</td><br>      <td>0.007118</td><br>      <td>0.007118</td><br>      <td>0.007118</td><br>    </tr><br>    <tr><br>      <th>21871</th><br>      <td>0.002185</td><br>      <td>0.002185</td><br>      <td>0.002185</td><br>      <td>0.002185</td><br>      <td>0.002185</td><br>      <td>0.002185</td><br>      <td>0.002185</td><br>      <td>0.002185</td><br>      <td>0.002185</td><br>      <td>0.002185</td><br>      <td>0.002185</td><br>      <td>0.002185</td><br>      <td>0.002185</td><br>    </tr><br>    <tr><br>      <th>21872</th><br>      <td>-0.031314</td><br>      <td>-0.031314</td><br>      <td>-0.031314</td><br>      <td>-0.031314</td><br>      <td>-0.031314</td><br>      <td>-0.031314</td><br>      <td>-0.031314</td><br>      <td>-0.031314</td><br>      <td>-0.031314</td><br>      <td>-0.031314</td><br>      <td>-0.031314</td><br>      <td>-0.031314</td><br>      <td>-0.031314</td><br>    </tr><br>    <tr><br>      <th>21873</th><br>      <td>-0.010752</td><br>      <td>-0.010752</td><br>      <td>-0.010752</td><br>      <td>-0.010752</td><br>      <td>-0.010752</td><br>      <td>-0.010752</td><br>      <td>-0.010752</td><br>      <td>-0.010752</td><br>      <td>-0.010752</td><br>      <td>-0.010752</td><br>      <td>-0.010752</td><br>      <td>-0.010752</td><br>      <td>-0.010752</td><br>    </tr><br>    <tr><br>      <th>21874</th><br>      <td>0.001658</td><br>      <td>0.001658</td><br>      <td>0.001658</td><br>      <td>0.001658</td><br>      <td>0.001658</td><br>      <td>0.001658</td><br>      <td>0.001658</td><br>      <td>0.001658</td><br>      <td>0.001658</td><br>      <td>0.001658</td><br>      <td>0.001658</td><br>      <td>0.001658</td><br>      <td>0.001658</td><br>    </tr><br>    <tr><br>      <th>21875</th><br>      <td>-0.002177</td><br>      <td>-0.002177</td><br>      <td>-0.002177</td><br>      <td>-0.002177</td><br>      <td>-0.002177</td><br>      <td>-0.002177</td><br>      <td>-0.002177</td><br>      <td>-0.002177</td><br>      <td>-0.002177</td><br>      <td>-0.002177</td><br>      <td>-0.002177</td><br>      <td>-0.002177</td><br>      <td>-0.002177</td><br>    </tr><br>  </tbody><br></table><br><p>21876 rows × 13 columns</p><br></div><p>将上述13个行业的开盘价收益率其导出为excel文件：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">op_df.to_excel(<span class="string">'/home/fanxi/桌面/syl.xlsx'</span>)<span class="comment">#保存文件</span></span><br></pre></td></tr></table></figure><h3 id="五、对上一步导出的数据进行描述性统计"><a href="#五、对上一步导出的数据进行描述性统计" class="headerlink" title="五、对上一步导出的数据进行描述性统计"></a>五、对上一步导出的数据进行描述性统计</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">op_df.describe()</span><br></pre></td></tr></table></figure><div><br><style scoped><br>    .dataframe tbody tr th:only-of-type {<br>        vertical-align: middle;<br>    }<br><br>    .dataframe tbody tr th {<br>        vertical-align: top;<br>    }<br><br>    .dataframe thead th {<br>        text-align: right;<br>    }<br></style><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>code1</th><br>      <th>code2</th><br>      <th>code3</th><br>      <th>code4</th><br>      <th>code5</th><br>      <th>code6</th><br>      <th>code7</th><br>      <th>code8</th><br>      <th>code9</th><br>      <th>code10</th><br>      <th>code11</th><br>      <th>code12</th><br>      <th>code13</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>count</th><br>      <td>21876.000000</td><br>      <td>21876.000000</td><br>      <td>21876.000000</td><br>      <td>21876.000000</td><br>      <td>21876.000000</td><br>      <td>21876.000000</td><br>      <td>21876.000000</td><br>      <td>21876.000000</td><br>      <td>21876.000000</td><br>      <td>21876.000000</td><br>      <td>21876.000000</td><br>      <td>21876.000000</td><br>      <td>21876.000000</td><br>    </tr><br>    <tr><br>      <th>mean</th><br>      <td>0.000051</td><br>      <td>0.000051</td><br>      <td>0.000051</td><br>      <td>0.000051</td><br>      <td>0.000051</td><br>      <td>0.000051</td><br>      <td>0.000051</td><br>      <td>0.000051</td><br>      <td>0.000051</td><br>      <td>0.000051</td><br>      <td>0.000051</td><br>      <td>0.000051</td><br>      <td>0.000051</td><br>    </tr><br>    <tr><br>      <th>std</th><br>      <td>0.018209</td><br>      <td>0.018209</td><br>      <td>0.018209</td><br>      <td>0.018209</td><br>      <td>0.018209</td><br>      <td>0.018209</td><br>      <td>0.018209</td><br>      <td>0.018209</td><br>      <td>0.018209</td><br>      <td>0.018209</td><br>      <td>0.018209</td><br>      <td>0.018209</td><br>      <td>0.018209</td><br>    </tr><br>    <tr><br>      <th>min</th><br>      <td>-0.143910</td><br>      <td>-0.143910</td><br>      <td>-0.143910</td><br>      <td>-0.143910</td><br>      <td>-0.143910</td><br>      <td>-0.143910</td><br>      <td>-0.143910</td><br>      <td>-0.143910</td><br>      <td>-0.143910</td><br>      <td>-0.143910</td><br>      <td>-0.143910</td><br>      <td>-0.143910</td><br>      <td>-0.143910</td><br>    </tr><br>    <tr><br>      <th>25%</th><br>      <td>-0.008568</td><br>      <td>-0.008568</td><br>      <td>-0.008568</td><br>      <td>-0.008568</td><br>      <td>-0.008568</td><br>      <td>-0.008568</td><br>      <td>-0.008568</td><br>      <td>-0.008568</td><br>      <td>-0.008568</td><br>      <td>-0.008568</td><br>      <td>-0.008568</td><br>      <td>-0.008568</td><br>      <td>-0.008568</td><br>    </tr><br>    <tr><br>      <th>50%</th><br>      <td>0.000187</td><br>      <td>0.000187</td><br>      <td>0.000187</td><br>      <td>0.000187</td><br>      <td>0.000187</td><br>      <td>0.000187</td><br>      <td>0.000187</td><br>      <td>0.000187</td><br>      <td>0.000187</td><br>      <td>0.000187</td><br>      <td>0.000187</td><br>      <td>0.000187</td><br>      <td>0.000187</td><br>    </tr><br>    <tr><br>      <th>75%</th><br>      <td>0.009195</td><br>      <td>0.009195</td><br>      <td>0.009195</td><br>      <td>0.009195</td><br>      <td>0.009195</td><br>      <td>0.009195</td><br>      <td>0.009195</td><br>      <td>0.009195</td><br>      <td>0.009195</td><br>      <td>0.009195</td><br>      <td>0.009195</td><br>      <td>0.009195</td><br>      <td>0.009195</td><br>    </tr><br>    <tr><br>      <th>max</th><br>      <td>0.133592</td><br>      <td>0.133592</td><br>      <td>0.133592</td><br>      <td>0.133592</td><br>      <td>0.133592</td><br>      <td>0.133592</td><br>      <td>0.133592</td><br>      <td>0.133592</td><br>      <td>0.133592</td><br>      <td>0.133592</td><br>      <td>0.133592</td><br>      <td>0.133592</td><br>      <td>0.133592</td><br>    </tr><br>  </tbody><br></table><br></div><h3 id="六、绘制直方图检验正态性"><a href="#六、绘制直方图检验正态性" class="headerlink" title="六、绘制直方图检验正态性"></a>六、绘制直方图检验正态性</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">13</span>):</span><br><span class="line">    </span><br><span class="line">    fig=plt.subplot(<span class="number">2</span>,<span class="number">7</span>,i+<span class="number">1</span>)</span><br><span class="line">    fig.hist(op_df.iloc[i])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_34_0.png" alt="png"></p><p>把所有的图画在一张画布上，显示不太清楚</p><p>可以取出来其中一个，比如第一个行业（即code1）的直方图</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">x0=np.array(op_df.iloc[:,<span class="number">0</span>])</span><br><span class="line">plt.hist(x0)</span><br></pre></td></tr></table></figure><pre><code>(array([6.000e+00, 5.000e+00, 1.300e+01, 1.190e+02, 8.700e+02, 1.008e+03,        1.500e+02, 1.300e+01, 2.000e+00, 1.000e+00]), array([-0.12425762, -0.09959784, -0.07493806, -0.05027828, -0.0256185 ,        -0.00095871,  0.02370107,  0.04836085,  0.07302063,  0.09768041,         0.1223402 ]), &lt;a list of 10 Patch objects&gt;)</code></pre><p><img src="output_37_1.png" alt="png"></p><p>结果显示，近似服从正态分布，其余12个同理可得</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;一、导入处理完毕的数据&quot;&gt;&lt;a href=&quot;#一、导入处理完毕的数据&quot; class=&quot;headerlink&quot; title=&quot;一、导入处理完毕的数据&quot;&gt;&lt;/a&gt;一、导入处理完毕的数据&lt;/h3&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;tab
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>计算方法知识结构</title>
    <link href="http://yoursite.com/2019/01/08/calmethods/"/>
    <id>http://yoursite.com/2019/01/08/calmethods/</id>
    <published>2019-01-08T04:07:46.000Z</published>
    <updated>2019-01-08T04:35:02.336Z</updated>
    
    <content type="html"><![CDATA[<p><img src="1.jpg" alt=""> <br><br><img src="2.jpg" alt=""> <br><br><img src="3.jpg" alt=""> <br><br><img src="4.jpg" alt=""> <br><br><img src="5.jpg" alt=""> <br><br><img src="6.jpg" alt=""> <br><br><img src="7.jpg" alt=""> <br><br><img src="8.jpg" alt=""> <br><br><img src="9.jpg" alt=""> <br><br><img src="10.jpg" alt=""> <br><br><img src="11.jpg" alt=""> <br><br><img src="12.jpg" alt=""> <br></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;1.jpg&quot; alt=&quot;&quot;&gt; &lt;br&gt;&lt;br&gt;&lt;img src=&quot;2.jpg&quot; alt=&quot;&quot;&gt; &lt;br&gt;&lt;br&gt;&lt;img src=&quot;3.jpg&quot; alt=&quot;&quot;&gt; &lt;br&gt;&lt;br&gt;&lt;img src=&quot;4.jpg&quot; alt=&quot;&quot;&gt; &lt;br&gt;&lt;br&gt;&lt;img 
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>git配置sshkey</title>
    <link href="http://yoursite.com/2018/12/29/git%E9%85%8D%E7%BD%AEsshkey/"/>
    <id>http://yoursite.com/2018/12/29/git配置sshkey/</id>
    <published>2018-12-29T14:17:14.000Z</published>
    <updated>2018-12-30T04:36:48.391Z</updated>
    
    <content type="html"><![CDATA[<pre><font color="#8AE234"><b>fanxi@fanxi-Lenovo-B51-35</b></font>:<font color="#729FCF"><b>~/桌面/myblog</b></font>$ mkdir ~/.sshmkdir: 无法创建目录&quot;/home/fanxi/.ssh&quot;: 文件已存在<font color="#8AE234"><b>fanxi@fanxi-Lenovo-B51-35</b></font>:<font color="#729FCF"><b>~/桌面/myblog</b></font>$ cd ~/.ssh<font color="#8AE234"><b>fanxi@fanxi-Lenovo-B51-35</b></font>:<font color="#729FCF"><b>~/.ssh</b></font>$ ssh-keygen -t rsaGenerating public/private rsa key pair.Enter file in which to save the key (/home/fanxi/.ssh/id_rsa): Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/fanxi/.ssh/id_rsa.Your public key has been saved in /home/fanxi/.ssh/id_rsa.pub.The key fingerprint is:SHA256:2FErsmivtwDdCJLhn+lJcH9VusJVLFOzuhBTdVeCTRU fanxi@fanxi-Lenovo-B51-35The key&apos;s randomart image is:+---[RSA 2048]----+|.        .=* =oE=||.o      .++o= o. ||oo..  .oo++.     || .+o++.==oo      ||  .==.+=So       ||  oo... o .      ||   o. .  .       ||     o.          ||    ....         |+----[SHA256]-----+<font color="#8AE234"><b>fanxi@fanxi-Lenovo-B51-35</b></font>:<font color="#729FCF"><b>~/.ssh</b></font>$ cat ~/.ssh/id_rsa.pubssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDVKdKF7K2FIfPnsiiu8z7lr66HDcc+GyirKvO524Q/EH7zPJvfutgEdg31nLbzfopHlwXMxPe3W/wsfZqQdd4s2+WidYayMF6vCv6uDHqtw+6VkaWW/5gJ0MAlCvnOujY1TnxGuapzcGuVdchNE+funEhLJmZCWHi7qoOV+maKtIyllWywpnWwnR24k62Djbl7mZPEe58cgcFtEokf+B+5oUxlyCL8kgJWWcCFU4RxIHKloDfq3EN+vwDiYDgfaTCa+9cGb00I2EuCwSAuW5ItwEldeVutZ0qweXq+VxBREmoFPAIYFXuIO8uGFbTaiv7Qi8dZgd+OEAln0gh/yfmZ fanxi@fanxi-Lenovo-B51-35<font color="#8AE234"><b>fanxi@fanxi-Lenovo-B51-35</b></font>:<font color="#729FCF"><b>~/.ssh</b></font>$ git config -global user.name&quot;fuhanshi&quot;error: did you mean `--global` (with two dashes ?)<font color="#8AE234"><b>fanxi@fanxi-Lenovo-B51-35</b></font>:<font color="#729FCF"><b>~/.ssh</b></font>$ <font color="#8AE234"><b>fanxi@fanxi-Lenovo-B51-35</b></font>:<font color="#729FCF"><b>~/.ssh</b></font>$ git config  --global user.name&quot;fuhanshi&quot;<font color="#8AE234"><b>fanxi@fanxi-Lenovo-B51-35</b></font>:<font color="#729FCF"><b>~/.ssh</b></font>$ git config  --global user.email&quot;1433758491@qq.com&quot;<font color="#8AE234"><b>fanxi@fanxi-Lenovo-B51-35</b></font>:<font color="#729FCF"><b>~/.ssh</b></font>$ ssh -Tgit@github.comWarning: Identity file t@github.com not accessible: No such file or directory.usage: ssh [-46AaCfGgKkMNnqsTtVvXxYy] [-b bind_address] [-c cipher_spec]           [-D [bind_address:]port] [-E log_file] [-e escape_char]           [-F configfile] [-I pkcs11] [-i identity_file]           [-J [user@]host[:port]] [-L address] [-l login_name] [-m mac_spec]           [-O ctl_cmd] [-o option] [-p port] [-Q query_option] [-R address]           [-S ctl_path] [-W host:port] [-w local_tun[:remote_tun]]           [user@]hostname [command]<font color="#8AE234"><b>fanxi@fanxi-Lenovo-B51-35</b></font>:<font color="#729FCF"><b>~/.ssh</b></font>$ ssh -T git@github.comHi fuhanshi! You&apos;ve successfully authenticated, but GitHub does not provide shell access.<font color="#8AE234"><b>fanxi@fanxi-Lenovo-B51-35</b></font>:<font color="#729FCF"><b>~/.ssh</b></font>$ </pre>]]></content>
    
    <summary type="html">
    
      
      
        &lt;pre&gt;&lt;font color=&quot;#8AE234&quot;&gt;&lt;b&gt;fanxi@fanxi-Lenovo-B51-35&lt;/b&gt;&lt;/font&gt;:&lt;font color=&quot;#729FCF&quot;&gt;&lt;b&gt;~/桌面/myblog&lt;/b&gt;&lt;/font&gt;$ mkdir ~/.ssh
mkdir: 无法创建
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>解线性方程组</title>
    <link href="http://yoursite.com/2018/12/29/gdds1/"/>
    <id>http://yoursite.com/2018/12/29/gdds1/</id>
    <published>2018-12-29T13:45:57.340Z</published>
    <updated>2018-10-01T07:20:38.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="解线性方程组"><a href="#解线性方程组" class="headerlink" title="解线性方程组"></a>解线性方程组</h3><p><strong>一、解线性方程组(3-7)</strong><br><br><code>矩阵消元法</code>　<code>阶梯型</code>　<code>主元</code></p><blockquote><p>1.对n个线性方程组成的线性方程组(即<code>n元线性方程组</code>)的增广矩阵做初等行变换得到一个阶梯型矩阵并记为J（J有$n+1$列，因为包含了<code>常数项</code>那一列），记J的非零行的个数为r. <br><br>证明：</p><ul><li>当出现<code>等号左边全是０，而右边非０</code>时（<em>这显然不可能的事儿</em>），该线性方程组<code>无解</code>；<br></li><li>当$r＝n$时，该线性方程组有<code>唯一解</code>；<br></li><li>当$r&lt;n$时,该线性方程组有<code>无穷解</code>；<br></li></ul></blockquote><p>证：<br></p><ul><li>第一条显然成立；<br></li><li><strong>在证明后面两个结论之前，我们先来证明$r&lt;=n$,即阶梯型矩阵非零行的数目$r$不可能大于（超过）未知量$n$的数目:</strong><br><br><img src="1.jpg" alt="矩阵J"><br><br>首先可以明确的是，J的第r个主元不能位于第$n+1$列，至多也只能在第$n$列，因此$t&lt;=n$，稍微想一下就知道，主元是指每一行第一个非零元素，上图中的元素$b$为第$r$行的主元，且位于第$t$列，主元不可能跑到常数项的那个第$n+1$列去，所以有$t&lt;=n$.<br><br>再来想想，每一行的主元所在列数不一定正好是对应的行数，比如在第二行中，$x_２$的系数在经过初等行变换之后为０，而$x_３$的系数非零.类似情况有很多，因此主元所在列数$t$往往是靠右，即主元所在列数$t$往往大于主元所在行数，而图中主元$b$所在的行数被我们设定为$r$，那就是说主元$b$所在列数$t$往往大于主元所在行数$r$，注意一点，我们这里的<code>往往大于</code>是指一般情况，可以取等号，即　$t&gt;=r$，从而有$r&lt;=n$，这就证明了<code>阶梯型矩阵非零行的数目r不可能大于（超过）未知量n的数目</code>.<br><br>等一下，再补充一些（更形象的解释一下）：在证明$t&gt;=r$时，在矩阵J中，b是最后一个主元，或者换句话说，b所在行的下边的行（我们也不知道具体有多少行，也有可能是０行，如果$r=n$），全部是零元素，那么上面的主元所在列由于是要成阶梯型的，可以脑补画面（阶梯型），这个阶梯，每一凳的砖块数不一样，最少是一块，那么也有可能下一凳（下一个主元）用了两块甚至更多块砖，就像Python的缩进一样，当每凳（每个主元）都只用１块砖时，就是最节省的情况（从左到右，一凳一凳的一个阶梯），此时恰好有$r=t$ ，而在有浪费的情况（每凳所用砖头数大于１）下，会<code>向右推进</code>，使得$t$变大，从而就有了$t&gt;=r$.　<br>总结下就是：<br><br><strong>$J$中主元所在列数$t$不可能跑到常数项所在列（第$n+1$列），所以有$t&lt;=n$；<br></strong><br><strong>$J$中主元所在行数$r$在不浪费的情况下也只能等于t,否则小于$t$，所以有$r&lt;=t$；<br></strong><br><strong>综上，有</strong><br>$$r&lt;=n$$</li><li>现在来证明当$r=n$时，该$n$元线性方程组有唯一解.<br><br>将阶梯型矩阵$J$继续进行变换，化为简化的阶梯型，记做$J_1$，如下图<br><center><img src="2.jpg" alt="矩阵J_1"></center><br><br>此时是$r=n$的，那么很明显，<br>$$(C_1,C_2，…，C_n)$$<br>就是该线性方程组的唯一解啦.<br></li><li>现在来证明当$r&lt;n$时，该$n$元线性方程组有无穷多个解.<br><br>经过初等行变换，第一行的主元总是可以在第一列位置处，而其他行的主元所在列位置则不一定正好与其对应的行数,如下图<br><center><img src="03.jpg" alt="矩阵J_1"></center><br>这里要注意啦，第$２$行的主元不一定在第２列，我们不妨标记其为$J_2$列，$J_2$不一定是$２$.<br><br>将所有的主变量（以主元为系数的变量）系数化为$１$，并移到等号左边，将自由未知量（所有$n$个变量除去主变量）移到等号右边，如下图<br><img src="4.jpg" alt="矩阵J_1"><br><br>由于$r&lt;n$，并且左边只有r个主变量，那么右边肯定有$n-r$个自由未知量.<br><br>自由未知量的取值不同，对应的该线性方程组的一组解也不同，从而证明了<code>当r&lt;n时，n元线性方程组有无穷多个解.</code><br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">以上，便是全部的证明过程,Over~~~~~~~~~~~~~~~</span><br></pre></td></tr></table></figure></li></ul><p>现在，我们来讨论一下<strong>齐次线性方程组</strong>（常数项全为零的线性方程组）<br><br>显然，$(0，0，…　，0)$是原方程组的一组解，称为<strong>零解</strong>；<br><br>其余的解（如果存在）则叫做<strong>非零解</strong>.<br><br><em>$n$元齐次线性方程组有非零解的充分必要条件是：系数矩阵经过初等行变换化成的阶梯型矩阵的非零行数目$r&lt;n$.<br></em><br>但如果仅仅是来判断一个$n$元齐次线性方程组是否有非零解，还得每次进行初等行变换，好不麻烦，于是，针对于<strong>齐次</strong>的特殊性，我们有更简单的判别方法，那就是：<br></p><ul><li><strong><center>$n$元<code>齐次</code>线性方程组<code>有非零解</code>的<code>充分条件</code>是方程组中<code>方程的个数</code>$s$<code>小于</code>未知量的个数$n$.</center></strong></li><li>证明很简单：前面已经证过，当$r=n$时，该$n$元齐次线性方程组有唯一解；我们又知道，在$n$元<strong>齐次</strong>线性方程组中，由于常数项全为０，所以$(0，0，…　，0)$是原方程组的一组解（叫做零解），那么这个零解就是该$n$元齐次线性方程组出现的唯一解的情况.去除这种唯一解的情况，那就只剩下了无穷多个解的情况（不可能出现无解的情况，因为不管怎么样，都至少有一组零解了，怎能再无解？）了.按照之前的套路，对该方程组做初等行变换，记住，一共有$s$个方程，那么经过初等行变换之后，所得到的方程的个数$r$肯定小于或等于$s$，即<strong>$r&lt;=s$</strong>，又由于已知（条件）$s&lt;n$，因此我们得到$r&lt;n$，而这个结论正好是前面证过的关于$n$元线性方程组有无穷多个解的条件，这里的无穷多个解肯定全是非零解，因为对于齐次线性方程组来说，零解是必然存在的，而且这种情况已经被我们划分到方程组有唯一解的类别之中，那么另外一种情况，即有无穷多个解中，这无穷多个解肯定全是非零解了，要不然就矛盾了.这样子我们就完成了上述结论的简单证明.<br></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">再次提醒，我们刚才的证明的结论是针对于n元齐次线性方程组，齐次，齐次，齐次!而且只是**充分条件**</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;解线性方程组&quot;&gt;&lt;a href=&quot;#解线性方程组&quot; class=&quot;headerlink&quot; title=&quot;解线性方程组&quot;&gt;&lt;/a&gt;解线性方程组&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;一、解线性方程组(3-7)&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;矩阵消元法&lt;/cod
      
    
    </summary>
    
      <category term="MathA" scheme="http://yoursite.com/categories/MathA/"/>
    
    
  </entry>
  
  <entry>
    <title>Python数据预处理</title>
    <link href="http://yoursite.com/2018/12/29/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"/>
    <id>http://yoursite.com/2018/12/29/数据预处理/</id>
    <published>2018-12-29T13:45:57.336Z</published>
    <updated>2018-09-30T10:29:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>标准化：数据的标准化（normalization）是将数据按比例缩放，使之落入一个小的特定区间。在某些比较和评价的指标处理中经常会用到，去除数据的单位限制，将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权。</p><p>标准化基于正态分布的假设，将数据变换为均值为0、标准差为1的标准正态分布。但即使数据不服从正态分布，也可以用此法。特别适用于数据的最大值和最小值未知，或存在孤立点。</p><p>主要方法：<br>z-score标准化，即零-均值标准化（常用方法）</p><p>$$y=\frac{x-μ}σ$$</p><p><del>~</del>~~ 下面看看在Python中的实现</p><p>方法１.<strong>scale</strong>可以直接对数组进行标准化，请看下例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line">X_train=np.array([[<span class="number">1</span>,<span class="number">50</span>,<span class="number">500</span>],[<span class="number">2</span>,<span class="number">40</span>,<span class="number">400</span>],[<span class="number">5</span>,<span class="number">55</span>,<span class="number">666</span>]])</span><br><span class="line">X_scaled=preprocessing.scale(X_train,axis=<span class="number">0</span>)<span class="comment">#axis默认值就是０，所以也可以不写</span></span><br><span class="line"><span class="keyword">print</span> X_scaled       <span class="comment">#标准化后的数据</span></span><br></pre></td></tr></table></figure><pre><code>[[-0.98058068  0.26726124 -0.20054214] [-0.39223227 -1.33630621 -1.11209733] [ 1.37281295  1.06904497  1.31263947]]</code></pre><p>咱们可以检验一下这个X_scaled的均值和方差</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> X_scaled.mean(axis=<span class="number">0</span>)<span class="comment">#均值</span></span><br><span class="line"><span class="keyword">print</span> X_scaled.std(axis=<span class="number">0</span>)<span class="comment">#方差</span></span><br></pre></td></tr></table></figure><pre><code>[  7.40148683e-17  -2.96059473e-16   0.00000000e+00][ 1.  1.  1.]</code></pre><p>注意这里的axis=0代表按行处理，也就是把行压缩，也就是对每一列进行标准化，常用！</p><p>方法２．<strong>from skelearn.preprocessing import StandardScaler</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scaler = preprocessing.StandardScaler()</span><br><span class="line">scaler</span><br></pre></td></tr></table></figure><pre><code>StandardScaler(copy=True, with_mean=True, with_std=True)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scaler.fit(X_train)</span><br></pre></td></tr></table></figure><pre><code>StandardScaler(copy=True, with_mean=True, with_std=True)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scaler.transform(X_train)</span><br></pre></td></tr></table></figure><pre><code>array([[-0.98058068,  0.26726124, -0.20054214],       [-0.39223227, -1.33630621, -1.11209733],       [ 1.37281295,  1.06904497,  1.31263947]])</code></pre><p>以上是把fit和transform两步分开进行的，我们也可以直接一步完成，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scaler.fit_transform(X_train)</span><br></pre></td></tr></table></figure><pre><code>array([[-0.98058068,  0.26726124, -0.20054214],       [-0.39223227, -1.33630621, -1.11209733],       [ 1.37281295,  1.06904497,  1.31263947]])</code></pre><p>但是要注意，在实际的建模过程中，我们通常将数据集划分为训练数据集和测试数据集，这时候我们应该分两步进行，先fit训练数据集，并将其定义为一个变量，比如ss,然后用ss来transform训练数据集从而进行模型的拟合，之后在检验模型的拟合度时，首先也要对测试数据集进行transform，这是就要用之前fit好的ss来transform测试数据集了，当然，这里只针对于变量数据，不包括target</p><p>同样可以用均值和方差来进行验证：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> scaler.fit_transform(X_train).mean(axis=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">print</span> scaler.fit_transform(X_train).std(axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><pre><code>[  7.40148683e-17  -2.96059473e-16   0.00000000e+00][ 1.  1.  1.]</code></pre><p>我们一般采用方法２，因为它可以保存训练集中的参数（均值、方差）直接使用其对象转换测试集数据.</p><p>其实，对数据进行标准化的数学方法不止上面这一个，还有以下几个：</p><ul><li>离差标准化</li></ul><p>则是对原始数据的一个线性变换，公式如下：</p><p>$$y=\frac{x-x_{min}}{x_{max}-x_{min}}$$</p><p>这种方法有个缺陷就是当有新数据加入时，可能导致$x_{max}$和$x_{min}$的变化，需要重新定义。</p><p>下面来编程模拟实现一个实例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x=np.array([[<span class="number">1.5</span>,<span class="number">8.8</span>,<span class="number">2.3</span>],[<span class="number">5.8</span>,<span class="number">5.0</span>,<span class="number">6.2</span>],[<span class="number">7.2</span>,<span class="number">8.3</span>,<span class="number">9.6</span>],[<span class="number">4.4</span>,<span class="number">5.5</span>,<span class="number">6.6</span>]])</span><br><span class="line">x</span><br></pre></td></tr></table></figure><pre><code>array([[ 1.5,  8.8,  2.3],       [ 5.8,  5. ,  6.2],       [ 7.2,  8.3,  9.6],       [ 4.4,  5.5,  6.6]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.shape</span><br></pre></td></tr></table></figure><pre><code>(4, 3)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_NEW=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,x.shape[<span class="number">1</span>]):</span><br><span class="line">    x_new=(x[:,i]-min(x[:,i]))/(max(x[:,i])-min(x[:,i]))</span><br><span class="line">    <span class="keyword">print</span> x_new</span><br><span class="line">    X_NEW.append(x_new)</span><br></pre></td></tr></table></figure><pre><code>[ 0.          0.75438596  1.          0.50877193][ 1.          0.          0.86842105  0.13157895][ 0.          0.53424658  1.          0.5890411 ]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.array(X_NEW).transpose()<span class="comment">#最终数据</span></span><br></pre></td></tr></table></figure><pre><code>array([[ 0.        ,  1.        ,  0.        ],       [ 0.75438596,  0.        ,  0.53424658],       [ 1.        ,  0.86842105,  1.        ],       [ 0.50877193,  0.13157895,  0.5890411 ]])</code></pre><p>当然，我们也可以直接调用sklearn中的<strong>MinMaxScaler()</strong>来实现上述功能：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing   </span><br><span class="line">min_max_scaler = preprocessing.MinMaxScaler()  </span><br><span class="line">X_minMax = min_max_scaler.fit_transform(x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_minMax<span class="comment">#最终数据</span></span><br></pre></td></tr></table></figure><pre><code>array([[ 0.        ,  1.        ,  0.        ],       [ 0.75438596,  0.        ,  0.53424658],       [ 1.        ,  0.86842105,  1.        ],       [ 0.50877193,  0.13157895,  0.5890411 ]])</code></pre><p>结果是一模一样的！</p><p>为了方便起见，我们今后就直接调用MinMaxScaler() 就好了.</p><p>离差标准化可以扩展一下，比如我们想要把数据映射到－１和１之间，那么就采用以下数学公式：</p><p>$$x_{new}=\frac{x-x_{mean}}{x_{max}-x_{min}}$$</p><p>编程模拟一下，直接对之前的代码做一些改动就可以了，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x=np.array([[<span class="number">1.0</span>,<span class="number">2.2</span>,<span class="number">3.3</span>],[<span class="number">5.2</span>,<span class="number">3.3</span>,<span class="number">2.2</span>],[<span class="number">1.3</span>,<span class="number">2.5</span>,<span class="number">6.8</span>]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_NEW=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">1</span>]):</span><br><span class="line">    x_new=(x[:,i]-np.mean(x[:,i]))/(max(x[:,i])-min(x[:,i]))</span><br><span class="line">    <span class="keyword">print</span> x_new</span><br><span class="line">    X_NEW.append(x_new)</span><br></pre></td></tr></table></figure><pre><code>[-0.56578947  0.18859649  0.43421053 -0.05701754][ 0.5        -0.5         0.36842105 -0.36842105][-0.53082192  0.00342466  0.46917808  0.05821918]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.array(X_NEW).transpose()<span class="comment">#最终数据</span></span><br></pre></td></tr></table></figure><pre><code>array([[-0.56578947,  0.5       , -0.53082192],       [ 0.18859649, -0.5       ,  0.00342466],       [ 0.43421053,  0.36842105,  0.46917808],       [-0.05701754, -0.36842105,  0.05821918]])</code></pre><p>＊＊＊</p><p>以上都是些常用的数据标准化方法，还有一些不太常用的方法，比如：</p><ul><li>对数Logistic模式：</li></ul><p>$$X_{new}=\frac{1}{1+e^{-X_{old}}}$$</p><p>得出的数都在０和１之间</p><p>最后来说一下<strong>数据正则化</strong></p><p>正则化主要是用于解决过拟合，正则性衡量了函数光滑的程度，正则性越高，函数越光滑。（光滑衡量了函数的可导性，如果一个函数是光滑函数，则该函数无穷可导，即任意n阶可导）.<br><br>采用正则化方法会自动削弱不重要的特征变量，自动从许多的特征变量中”提取“重要的特征变量，减小特征变量的数量级。这个方法非常有效，当我们有很多特征变量时，其中每一个变量都能对预测产生一点影响。我们可以有很多特征变量，其中每一个变量都是有用的，因此我们不希望把它们删掉，这就导致了正则化概念的发生。</p><p>看一下在sklearn中的调用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> normalize</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x=np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]])</span><br><span class="line">x</span><br></pre></td></tr></table></figure><pre><code>array([[1, 2, 3],       [4, 5, 6],       [7, 8, 9]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">normalizer = preprocessing.Normalizer().fit(x)  <span class="comment"># fit does nothing</span></span><br><span class="line">normalizer</span><br></pre></td></tr></table></figure><pre><code>Normalizer(copy=True, norm=&apos;l2&apos;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">normalizer.transform(x)<span class="comment">#最终结果</span></span><br></pre></td></tr></table></figure><pre><code>array([[ 0.26726124,  0.53452248,  0.80178373],       [ 0.45584231,  0.56980288,  0.68376346],       [ 0.50257071,  0.57436653,  0.64616234]])</code></pre><p>今天就写到这儿吧，有时间继续，如果能帮到你，还请关注下微信公众号“我将在南极找寻你”，更多干货尽在其中！</p><p>参考：<br> <a href="https://blog.csdn.net/gshgsh1228/article/details/52199870/" target="_blank" rel="noopener">https://blog.csdn.net/gshgsh1228/article/details/52199870/</a>　<br><br><a href="https://www.jianshu.com/p/0d8bb02f98fb" target="_blank" rel="noopener">https://www.jianshu.com/p/0d8bb02f98fb</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;标准化：数据的标准化（normalization）是将数据按比例缩放，使之落入一个小的特定区间。在某些比较和评价的指标处理中经常会用到，去除数据的单位限制，将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权。&lt;/p&gt;
&lt;p&gt;标准化基于正态分布的假设，将数据
      
    
    </summary>
    
      <category term="Data Analysis&amp;Mining with Python" scheme="http://yoursite.com/categories/Data-Analysis-Mining-with-Python/"/>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Python降维处理</title>
    <link href="http://yoursite.com/2018/12/29/%E9%99%8D%E7%BB%B4/"/>
    <id>http://yoursite.com/2018/12/29/降维/</id>
    <published>2018-12-29T13:45:57.336Z</published>
    <updated>2018-09-30T10:00:09.000Z</updated>
    
    <content type="html"><![CDATA[<p>当一个样本数据集的特征数目较多时，通常会造成运行速度缓慢，尤其是在做回归分析的时候，还有可能产生多重共线性，虽然我们可以用岭回归的方法来减小多重共线性，但是仍然存在，那我们何不找个更好的解决办法呢？</p><p>于是乎，降维技术应运而生</p><p>通过降维，我们可以将高维特征缩减至低维</p><p>这样做的好处，一方面在于可以节约计算机运行的时间成本，另一方面，通过降维，可以方便的对数据进行可视化，在前一期的聚类分析中，我们已经了解到，一般地，我们仅能对二维数据进行可视化.</p><p>关于降维的数学原理，这里不做讨论，先站在上帝视角学会证明使用这一技术，再去深入研究其构造原理</p><p>本次采用sklearn自带的数据集load_digits，这是一个关于手写数字识别的数据集，总共1797条数据，64个特征，对应的目标是0到9这10个数字</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加载数据集</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_digits</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">digits=load_digits()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">digits.data.shape</span><br></pre></td></tr></table></figure><pre><code>(1797, 64)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">digits.target.shape</span><br></pre></td></tr></table></figure><pre><code>(1797,)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X=digits.data</span><br><span class="line">y=digits.target</span><br></pre></td></tr></table></figure><p>在降维之前，我们可以先试一下用KNN来训练这个分类器，我们称之为knnclassifier</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">knnclassifier= KNeighborsClassifier()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">knnclassifier</span><br></pre></td></tr></table></figure><pre><code>KNeighborsClassifier(algorithm=&apos;auto&apos;, leaf_size=30, metric=&apos;minkowski&apos;,           metric_params=None, n_jobs=1, n_neighbors=5, p=2,           weights=&apos;uniform&apos;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#分割据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line">X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=<span class="number">33</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">knnclassifier.fit(X_train,y_train)<span class="comment">#模型拟合</span></span><br></pre></td></tr></table></figure><pre><code>CPU times: user 12 ms, sys: 0 ns, total: 12 msWall time: 10.3 msKNeighborsClassifier(algorithm=&apos;auto&apos;, leaf_size=30, metric=&apos;minkowski&apos;,           metric_params=None, n_jobs=1, n_neighbors=5, p=2,           weights=&apos;uniform&apos;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_pre=knnclassifier.predict(X_test)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> knnclassifier.score(X_test,y_test)</span><br></pre></td></tr></table></figure><pre><code>0.984444444444</code></pre><p>可以看到，此时准确度是0.98,运行时间是11.2ms</p><p>接下来我们将尝试降维操作，把64个特征降到两维</p><p>sklearn中的decomposition模块已经为我们封装好了PCA这个降维方法，我们直接调用即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">pca=PCA(n_components=<span class="number">2</span>)<span class="comment">#n_components是指要降成几维，这里降为２维</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#拟合模型</span></span><br><span class="line">pca.fit(X_train)</span><br><span class="line">X_train_reduced=pca.transform(X_train)</span><br><span class="line">X_test_reduced=pca.transform(X_test)</span><br></pre></td></tr></table></figure><p>现在利用降维后的数据再重新训练一个KNN模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">knn_reduced=KNeighborsClassifier()</span><br><span class="line">knn_reduced.fit(X_train_reduced,y_train)</span><br></pre></td></tr></table></figure><pre><code>CPU times: user 4 ms, sys: 0 ns, total: 4 msWall time: 7.79 ms</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_pre=knn_reduced.predict(X_test_reduced)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">knn_reduced.score(X_test_reduced,y_test)</span><br></pre></td></tr></table></figure><pre><code>0.5822222222222222</code></pre><p>可以看出，虽然运行时间缩减了，但是精确度却大幅度下降</p><p>我们所降维度太低了，导致信息量大幅度减少，所以如何才能明确降到多少维合适呢？</p><p>这里需要了解一个概念叫做”解释方差比率”（explained_variance_ratio），代表的是每一个维度（特征）能解释总体的百分比，我们要做的就是找到这个比率排名靠前多少的特征，把这些特征作为主成分，其余的解释方差比率较小的特征就剔除掉.</p><p>我们可以先查看一下刚才做的两个特征维度的解释方差比率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pca.explained_variance_ratio_</span><br></pre></td></tr></table></figure><pre><code>array([ 0.15035598,  0.13838039])</code></pre><p>总体加起来才0.3不到，丢失了总体70%多的信息，很显然这样子是不行的</p><p>我们可以尝试在n_components中传入X_train的特征数，往下看你就明白了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pca=PCA(n_components=X_train.shape[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pca<span class="comment">#此时的值肯定是６４</span></span><br></pre></td></tr></table></figure><pre><code>PCA(copy=True, iterated_power=&apos;auto&apos;, n_components=64, random_state=None,  svd_solver=&apos;auto&apos;, tol=0.0, whiten=False)</code></pre><p>接下来再fit一下新的pca模型并查看解释方差比率</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pca.fit(X_train)</span><br></pre></td></tr></table></figure><pre><code>PCA(copy=True, iterated_power=&apos;auto&apos;, n_components=64, random_state=None,  svd_solver=&apos;auto&apos;, tol=0.0, whiten=False)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> pca.explained_variance_ratio_</span><br></pre></td></tr></table></figure><pre><code>[  1.50355979e-01   1.38380388e-01   1.19425901e-01   8.24718418e-02   5.91243713e-02   4.96975094e-02   4.23449789e-02   3.56664467e-02   3.23294141e-02   3.03973702e-02   2.34758387e-02   2.23682826e-02   1.82648603e-02   1.77678251e-02   1.45825518e-02   1.37044626e-02   1.30959900e-02   1.25622026e-02   1.02151829e-02   9.12424514e-03   8.91759754e-03   7.95889667e-03   7.55729053e-03   7.36310540e-03   6.86397508e-03   5.97732760e-03   5.70365442e-03   5.11943388e-03   4.81884444e-03   4.07489205e-03   3.74656189e-03   3.57400245e-03   3.31144332e-03   3.24522870e-03   3.04902907e-03   2.87135997e-03   2.57153779e-03   2.21866132e-03   2.15818927e-03   2.04639853e-03   1.85231174e-03   1.53306454e-03   1.48233877e-03   1.36725744e-03   1.14492332e-03   1.02513374e-03   9.51122042e-04   7.75946877e-04   5.59969368e-04   3.59004930e-04   2.22920251e-04   7.96771455e-05   4.21882118e-05   3.98867466e-05   3.23086994e-05   1.60861117e-05   7.16567372e-06   3.66786169e-06   8.50034737e-07   7.03829770e-07   4.01254157e-07   6.85795787e-34   6.85795787e-34   6.34067110e-34]</code></pre><p>以上便是按照从大到小顺序排列的每一个特征维度一次可以解释的方差比率</p><p>我们要做的是丢掉那些解释方差比率较小的，也就是后面的那些，保留前面的，那证明判断保留多少个合适呢？下面提供一种方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.plot([i <span class="keyword">for</span> i <span class="keyword">in</span> range(X_train.shape[<span class="number">1</span>])],</span><br><span class="line">         [np.sum(pca.explained_variance_ratio_[:i+<span class="number">1</span>]) <span class="keyword">for</span> i <span class="keyword">in</span> range(X_train.shape[<span class="number">1</span>])])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_42_0.png" alt="png"></p><p>横轴代表的是特征数目，纵轴代表的是所有被选中的主成分特征变量所能解释的方差比率之和</p><p>通过上图，比如我们要求所选取的主成分能解释总体80%的方差，那么对应横轴大约是20，也就是说我们的n_components应该传入20</p><p>这样，我们就可以通过观察上图来确定n_components了</p><p>其实，在sklearn中，这个功能已经封装好了，我们不必画图观察，而是传入我们所想要的主成分变量能解释的方差比率之和这个参数就可以了</p><p>调用方法很简单，直接在初始化模型的时候传入这个参数就行了</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pca=PCA(<span class="number">0.95</span>)<span class="comment">#要保留原始样本95%的解释方差</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pca.fit(X_train)<span class="comment">#重新训练模型</span></span><br></pre></td></tr></table></figure><pre><code>PCA(copy=True, iterated_power=&apos;auto&apos;, n_components=0.95, random_state=None,  svd_solver=&apos;auto&apos;, tol=0.0, whiten=False)</code></pre><p>我们可以看一下自动确定的最佳的保留的特征数，也就是主成分的个数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> pca.n_components_</span><br></pre></td></tr></table></figure><pre><code>28</code></pre><p>28,说明保留了28个主成分，这28个特征变量加起来就能解释总体95%的方差</p><p>那既然已经构建好了降维模型，那就拿来操练一下吧</p><p>先降维处理数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pca.fit(X_train)</span><br><span class="line">X_train_reduction=pca.transform(X_train)</span><br><span class="line">X_test_reduction=pca.transform(X_test)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_train_reduction.shape</span><br></pre></td></tr></table></figure><pre><code>(1347, 28)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_test_reduction.shape</span><br></pre></td></tr></table></figure><pre><code>(450, 28)</code></pre><p>再次重新训练KNN模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%%time</span><br><span class="line">knn_reduced=KNeighborsClassifier()</span><br><span class="line">knn_reduced.fit(X_train_reduction,y_train)</span><br></pre></td></tr></table></figure><pre><code>CPU times: user 4 ms, sys: 0 ns, total: 4 msWall time: 5.25 ms</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">knn_reduced.score(X_test_reduction,y_test)</span><br></pre></td></tr></table></figure><pre><code>0.97999999999999998</code></pre><p>从中可以看出，精确度将近0.98，而运行时间也比刚开始的节省了不少｜</p><p>说完了PCA的使用，最后来看看如何对降维结果进行可视化，当然，这里我们只能对二维或者三维数据进行</p><p>我们把特征降到３维（不考虑解释方差比率，这里只是为了讲解可视化的方法）</p><p>以下用的是整个样本，没有划分数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">pca=PCA(n_components=<span class="number">3</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pca.fit(X)</span><br></pre></td></tr></table></figure><pre><code>PCA(copy=True, iterated_power=&apos;auto&apos;, n_components=3, random_state=None,  svd_solver=&apos;auto&apos;, tol=0.0, whiten=False)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_vis=pca.transform(X)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_vis.shape</span><br></pre></td></tr></table></figure><pre><code>(1797, 3)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入三维数据可视化工具</span></span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#数据可视化</span></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = Axes3D(fig)</span><br><span class="line">plt.scatter(X_vis[:, <span class="number">0</span>], X_vis[:, <span class="number">1</span>], X_vis[:, <span class="number">2</span>],marker=<span class="string">'*'</span>,c=y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>/home/fantasy/anaconda2/lib/python2.7/site-packages/matplotlib/collections.py:865: RuntimeWarning: invalid value encountered in sqrt  scale = np.sqrt(self._sizes) * dpi / 72.0 * self._factor</code></pre><p><img src="output_70_1.png" alt="png"></p><p>我们再继续降到２维</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br><span class="line">pca.fit(X)</span><br><span class="line"><span class="keyword">print</span> pca.explained_variance_ratio_</span><br><span class="line"><span class="keyword">print</span> pca.explained_variance_</span><br><span class="line">X_new = pca.transform(X)</span><br><span class="line">plt.scatter(X_new[:, <span class="number">0</span>], X_new[:, <span class="number">1</span>],marker=<span class="string">'o'</span>,c=y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>[ 0.14890594  0.13618771][ 178.90731578  163.62664073]</code></pre><p><img src="output_72_1.png" alt="png"></p><p>是不是螺旋爆炸式的混乱？</p><p>由于PCA没有利用类别信息，我们可以看到降维后，样本特征和类别的信息关联几乎完全丢失。</p><p>于是LDA(线性判别式分析)应运而生！</p><p>与PCA一样，LDA是一种线性降维算法。不同于PCA只会选择数据变化最大的方向，由于LDA是有监督的（分类标签），所以LDA会主要以类别为思考因素，使得投影后的样本尽可能可分。它通过在k维空间选择一个投影超平面，使得不同类别在该超平面上的投影之间的距离尽可能近，同时不同类别的投影之间的距离尽可能远。从而试图明确地模拟数据类之间的差异。</p><p>一般来说，如果我们的数据是有类别标签的，那么优先选择LDA去尝试降维；当然也可以使用PCA做很小幅度的降维去消去噪声，然后再使用LDA降维。如果没有类别标签，那么肯定PCA是最先考虑的一个选择了。</p><p>而我们使用的数据集是有分类标签的（０，１，２，．．．，９），所以接下来我们将尝试LDA降维，最后再来可视化一下降维结果</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.discriminant_analysis <span class="keyword">import</span> LinearDiscriminantAnalysis</span><br><span class="line">lda = LinearDiscriminantAnalysis(n_components=<span class="number">3</span>)</span><br><span class="line">lda.fit(X,y)</span><br><span class="line">X_new = lda.transform(X)</span><br><span class="line">plt.scatter(X_new[:, <span class="number">0</span>], X_new[:, <span class="number">1</span>],marker=<span class="string">'o'</span>,c=y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>/home/fantasy/anaconda2/lib/python2.7/site-packages/sklearn/discriminant_analysis.py:387: UserWarning: Variables are collinear.  warnings.warn(&quot;Variables are collinear.&quot;)</code></pre><p><img src="output_79_1.png" alt="png"></p><p>LDA降维后可以把数据归为簇，利用了样本类别标签信息</p><p><strong>总结来说，如果样本没有标签，则使用PCA,若有标签，则最好使用LDA</strong></p><hr><p>主成分分析(PCA)和LDA都是直接选择对评价结果贡献度较高的几个维度，或者直接去掉对评价结果贡献度较低的几个维度；</p><p>而下面要讲的FA(<strong>因子分析</strong>)，则是以已知的所有维度为基础，创造数量更少的全新的一组维度来进行评价。先对原始的一组维度进行相关性分析，合并相关性高的，保留相关性低的。或者说，找出一组能够『代表』原维度组的新维度，同时能保留新维度组没有涵盖的特色部分。</p><p>通俗地说，就是造变量，用造的变量去替换原有的变量，并且造的变量的个数小于原有变量的个数</p><p>因子分析（Factor Analysis）是指研究从变量群中提取共性因子的统计技术，这里的共性因子指的是不同变量之间内在的隐藏因子。例如，一个学生的英语、数据、语文成绩都很好，那么潜在的共性因子可能是智力水平高。因此，因子分析的过程其实是寻找共性因子和个性因子并得到最优解释的过程。（摘自网络）</p><p>来看一下在sklearn中的调用方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> FactorAnalysis</span><br><span class="line">fa = FactorAnalysis(n_components=<span class="number">2</span>)<span class="comment">#降到二维</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fa.fit(X)</span><br></pre></td></tr></table></figure><pre><code>FactorAnalysis(copy=True, iterated_power=3, max_iter=1000, n_components=2,        noise_variance_init=None, random_state=0, svd_method=&apos;randomized&apos;,        tol=0.01)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_two_dim=fa.transform(X)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_two_dim.shape<span class="comment">#已经降到二维了</span></span><br></pre></td></tr></table></figure><pre><code>(1797, 2)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_two_dim<span class="comment">#并且是新的两个因子（这两个因子是全部64个变量的线性组合）</span></span><br></pre></td></tr></table></figure><pre><code>array([[-0.06629194,  0.30635624],       [-0.99445736,  0.14948677],       [-1.07480679, -0.40291119],       ...,        [-0.7385388 ,  0.0977223 ],       [-0.40362928, -0.25358677],       [ 0.67042921, -0.89378447]])</code></pre><p>降维结束，现在你可以重新训练之前的KNN模型，试一下效果了</p><p>最后的最后，我们来可视化一下降维后的数据</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">f = plt.figure(figsize=(<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line">ax = f.add_subplot(<span class="number">111</span>)</span><br><span class="line"><span class="comment">#这里的c=y代表颜色按照y的不同来区分，由于我们的y是０－９,故10中颜色</span></span><br><span class="line">ax.scatter(data_two_dim[:,<span class="number">0</span>],data_two_dim[:,<span class="number">1</span>],marker=<span class="string">'o'</span>,c=y)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_95_0.png" alt="png"></p><p>Over!</p><p>参考：<br><br><a href="https://blog.csdn.net/u013719780/article/details/51767314" target="_blank" rel="noopener">https://blog.csdn.net/u013719780/article/details/51767314</a><br><br><a href="https://www.cnblogs.com/pinard/p/6249328.html" target="_blank" rel="noopener">https://www.cnblogs.com/pinard/p/6249328.html</a><br><br><a href="https://blog.csdn.net/sm9sun/article/details/78791985" target="_blank" rel="noopener">https://blog.csdn.net/sm9sun/article/details/78791985</a><br><br>bobo老师机器学习视频教程</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;当一个样本数据集的特征数目较多时，通常会造成运行速度缓慢，尤其是在做回归分析的时候，还有可能产生多重共线性，虽然我们可以用岭回归的方法来减小多重共线性，但是仍然存在，那我们何不找个更好的解决办法呢？&lt;/p&gt;
&lt;p&gt;于是乎，降维技术应运而生&lt;/p&gt;
&lt;p&gt;通过降维，我们可以将
      
    
    </summary>
    
      <category term="Data Analysis&amp;Mining with Python" scheme="http://yoursite.com/categories/Data-Analysis-Mining-with-Python/"/>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>差分方程的解析解</title>
    <link href="http://yoursite.com/2018/12/29/diff/"/>
    <id>http://yoursite.com/2018/12/29/diff/</id>
    <published>2018-12-29T13:45:57.336Z</published>
    <updated>2018-09-30T08:12:08.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="差分方程的解析解"><a href="#差分方程的解析解" class="headerlink" title="差分方程的解析解"></a>差分方程的解析解</h3><p><strong>例题</strong>．求$y_t=a_0+a_1y_{t-1}+\epsilon_t$的解析解<br></p><hr><p>当初始条件已知，即$y_0$已知时，迭代算法如下：</p><ul><li>向前迭代算法<br><br>$$y_1=a_0+a_1y_0+\epsilon_1$$<br>$$y_2=a_0+a_1y_1+\epsilon_2$$<br>$$y_3=a_0+a_1y_2+\epsilon_3$$<br>$$…$$<br>$$y_t=a_0+a_1y_{t-1}+\epsilon_t$$<br><br>以上列举了该差分方程的所有项，我们要做的就是通过这些递推关系来逐步迭代，当迭代完所有项之后，最终可以得到目标解析解.<br><br>$$y_1=a_0+a_1y_0+\epsilon_1$$<br>$$y_2=a_0+a_1y_1+\epsilon_2=a_0+a_1(a_0+a_1y_0+\epsilon_1)+\epsilon_2=$$<br>$$a_0+a_0a_1+{a_1}^2y_0+a_1\epsilon_1+\epsilon_2=[a_0(1+a_1)]+[a_1^2y_0]+[a_1\epsilon_1+\epsilon_2]$$<br>$$y_3=a_0+a_1y_2+\epsilon_3=a_0+a_1(a_0+a_0a_1+{a_1}^2y_0+a_1\epsilon_1+\epsilon_2)+\epsilon_3=$$<br>$$[a_0+a_0a_1+a_0{a_1}^2]+[a_1^3y_0]+[a_1^2\epsilon_1+a_1\epsilon_2+\epsilon_3]=[a_0(1+a_1+a_1^2)]+[a_1^3y_0]+[a_1^2\epsilon_1+a_1\epsilon_2+\epsilon_3]$$<br>$$…$$<br>由数学归纳法，递推下去，则有：<br><br>$$y_t=[a_0(a_1+a_1^2+…+a_1^{t-1})]+[a_1^ty_0]+[a_1^{t-1}\epsilon_1+a_1^{t-2}\epsilon_2+…+a_1^0\epsilon_t]=$$<br>$$a_0\sum_{i=0}^{t-1}a_1^i+a_1^ty_0+\sum_{i=0}^{t-1}a_1^i\epsilon_{t-i}$$<br>上式便是目标解析解，因为$y_0$已知，即不含有任何未知项.<br><br>之所以称之为“向前迭代法”，是因为迭代的索引是从$y_1$开始，一步一步往前面跑去，逐步迭代得到到目标式$y_n$的.<br></li></ul><p>你肯定会想到，既然有向前迭代，那么一定有向后迭代了，没错，向后迭代就是从最后一项$y_t$逐渐向$y_0$进行迭代.</p><ul><li>向后迭代<br><br>同样是求解$y_t=a_0+a_1y_{t-1}+\epsilon_t$的解析解:<br>$$y_t=a_0+a_1y_{t-1}+\epsilon_t$$<br>$$y_{t-1}=a_0+a_1y_{t-2}+\epsilon_{t-1}$$<br>$$y_{t-2}=a_0+a_1y_{t-3}+\epsilon_{t-2}$$<br>$$…$$<br>$$y_2=a_0+a_1y_1+\epsilon_2$$<br>$$y_1=a_0+a_1y_0+\epsilon_1$$<br>$$y_0=a_0+a_1y_{-1}+\epsilon_0$$<br>开始迭代：(前面写了两次迭代过程,总共迭代$t-1$次)<br><br>$$y_t=a_0+a_1y_{t-1}+\epsilon_t=$$<br>$$a_0+a_1(a_0+a_1y_{t-2}+\epsilon_{t-1})+\epsilon_t=$$<br>$$a_0+a_1a_0+a_1^2y_{t-2}+a_1\epsilon_{t-1}+\epsilon_t=$$<br>$$[a_0(1+a_1)]+[a_1^2y_{t-2}]+[a_1\epsilon_{t-1}+\epsilon_t]$$<br>$$a_0+a_1a_0+a_1^2(a_0+a_1y_{t-3}+\epsilon_{t-2})+a_1\epsilon_{t-1}+\epsilon_t=$$<br>$$[a_0+a_1a_0+a_1^2a_0]+[a_1^3y_{t-3}]+[a_1^2\epsilon_{t-2}+a_1\epsilon_{t-1}+\epsilon_t]=$$<br>$$[a_0(1+a_1+a_1^2)]+[a_1^3y_{t-3}]+[a_1^2\epsilon_{t-2}+a_1\epsilon_{t-1}+\epsilon_t]=$$<br>$$…$$<br>$$=[a_0(1+a_1+a_1^2+…+a_1^{t-1})]+[a_1^ty_0]+[a_1^{t-1}\epsilon_{1}+a_1^{t-2}\epsilon_{2}+…+a_1^0\epsilon_t]＝$$<br>$$a_0\sum_{i=0}^{t-1}a_1^i+a_1^ty_0+\sum_{i=0}^{t-1}a_1^i\epsilon_{t-i}$$</li></ul><hr><p>而当初始条件未知，即$y_0$未知时，我们需要对上面得到的差分方程的解继续化：<br><br>$$y_t=a_0\sum_{i=0}^{t-1}a_1^i+a_1^ty_0+\sum_{i=0}^{t-1}a_1^i\epsilon_{t-i}=$$<br>$$a_0\sum_{i=0}^{t-1}a_1^i+a_1^t(a_0+a_1y_{-1}+\epsilon_0)+\sum_{i=0}^{t-1}a_1^i\epsilon_{t-i}=$$<br>$$a_0\sum_{i=0}^{t-1}a_1^i+(a_0a_1^t+a_1^{t+1}y_{-1}+a_1^t\epsilon_0)+\sum_{i=0}^{t-1}a_1^i\epsilon_{t-i}=$$<br>$$a_0\sum_{i=0}^{t}a_1^i+a_1^{t+1}y_{-1}+\sum_{i=0}^{t}a_1^i\epsilon_{t-i}=$$<br>$$…$$<br>$$=a_0\sum_{i=0}^{(t-1)+m}a_1^i+a_1^{t+m}y_{-m}+\sum_{i=0}^{(t-1)+m}a_1^i\epsilon_{t-i}$$</p><p>由于初始条件未知，我们现在来讨论解是否存在，即解的敛散性:<br></p><ul><li>当$|a_1|&lt;1$时，第一项出现等比数列求和，由无穷递缩等比数列性质，当$m\rightarrow\infty$时，第一项收敛到$\frac{a_0}{1-a_1}$;第二项中的$a_1^{t+m}$会收敛到０，因此第二项将收敛到０;第三项是一个有界量，类似于白噪声序列.<br><br>所以此时的解为<br>$$y_t=\frac{a_0}{1-a_1}+\sum_{i=0}^{t+m-1}a_1^i\epsilon_{t-i}$$</li><li>当$|a_1|&gt;1$时，发散；<br></li><li>当$|a_1|＝1$时，直接代回到最开始的式子，即得<br>$$y_t=a_0t+y_0+\sum_{i=0}^{t-1}\epsilon_{t-i}=a_0t+y_0+\sum_{i=1}^{t}\epsilon_{i}$$</li></ul><hr><p>以上便是针对例题展开的求解过程</p><hr><p>而对于一般的线性差分方程<br>$$y_t=a_0+a_1y_{t-1}+a_2y_{t-2}…+a_ny_{t-n}+x(t)．．．（１）$$<br>其齐次线性差分方程为<br>$$y_t=a_1y_{t-1}+a_2y_{t-2}…+a_ny_{t-n}．．．（２）$$<br>我们首先要明确，<br></p><ul><li>对于齐次线性差分方程（２式），对任意常数$A$，如果$y_t^h$是其一个解，那么$Ay_t^h$也是其一个解.<br></li><li>而对于非齐次线性差分方程（１式），我们需要求其一个特解及其对应的齐次线性差分方程的一个通解.<br></li></ul><hr><p>我们接下来将介绍一种新的解法（不同于之前的迭代法），称之为<strong>备选（特征根法）解法</strong>，来求解非齐次线性差分方程，即（１）式.<br><br>步骤如下：<br><br><strong>(1)建立齐次线性差分方程（２）式，求齐次方程的解$y_t^{h_1},y_t^{h_2},…,y_t^{h_n}$；<br></strong><br>(2)求出非齐次线性差分方程(1)式的一个特解$y_t^p$；　<br><br>(3)非齐次线性差分方程（１）式的通解即为其自身的一个特解加上其对应的齐次线性差分方程的一个通解，即<br>$$y_t=y_t^p+A_1y_t^{h_1}+A_2y_t^{h_2}+…+A_ny_t^{h_n}$$<br><br>其中$A_1,A_2,…A_n$是任意常数；<br><br>(4)若已知初始条件$y_0,y_1,…$，则可以求出$A_1,A_2,…A_n$.(选)<br></p><hr><p>其中第一步是最麻烦的，所以下面我们将针对第一步，即齐次线性差分方程的求解问题展开讨论<br><br><strong>例题</strong>：解齐次线性差分方程<br>$$y_t=a_1y_{t-1}+a_2y_{t-2}…+a_ny_{t-n}$$</p><hr><ul><li>一阶齐次差分方程$y_t=ay_{t-1}$，<br>通解为$Aa^t$ ,可以用迭代法证明，用特征根法更为简单：<br><br>猜想其解的形式为$y_t=A\alpha^t$，代入上式，得$A\alpha^t=aA\alpha^{t-1}$，即$\alpha=a$，故通解为$y_t=aA\alpha^{t-1}=Aa^t$.</li><li>二阶齐次差分方程$y_t=a_1y_{t-1}+a_2y_{t-2}$ ，<br><br>利用特征根法，猜想其解的形式为$y_t=A\alpha^t$，代入上式，得：<br>$$A\alpha^t=a_1A\alpha^{t-1}+a_2A\alpha^{t-2}$$<br>化简，得：<br>$$\alpha^2=a_1\alpha+a_2$$<br>移项，得：<br>$$\alpha^2-a_1\alpha-a_2=0$$<br>从而解得特征根为<br>$$\alpha_{1}=\frac{a_1{+}\sqrt{a_1^2+4a_2}}{2}$$<br>$$\alpha_{2}=\frac{a_1{-}\sqrt{a_1^2+4a_2}}{2}$$<br>其中$d=a_1^2+4a_2$　<br><br>$d$有三种情况：$d&gt;0,d＝０,d&lt;0$，下面就分这三种情况来讨论(<strong>下面的内容很重要!</strong>)<br></li></ul><blockquote><p>(1)当$d=a_1^2+4a_2&gt;0$时，$\alpha_1,\alpha_2$为互不相同的实根，此时的通解为<strong>$y_t=A_1\alpha_1^t+A_2\alpha_2^t$</strong>，其中$A_1,A_2$为任意常数.<br><br>当$|\alpha_1|$或者$｜\alpha_2｜$大于1时，发散；<br><br>当$|\alpha_1|$或者$｜\alpha_2｜$小于1时，收敛.<br></p></blockquote><blockquote><p>(2)当$d=a_1^2+4a_2=0$时，即$a_2=-\frac{a_1^2}{4}$时，$\alpha_1=\alpha_2$为重根，代回上面的特征根方程有$\alpha_1=\alpha_2=\frac{a_1}2$，此时$A\alpha_1^t=A{(\frac{a_1}2})^t$是方程的一个解（俩都一样，算一个），并且可以验证，$At{(\frac{a_1}2})^t$也是方程的一个解，所以此时方程的通解为$y_t=A_1{(\frac{a_1}2})^t+A_2t{(\frac{a_1}2})^t$，其中$A_1,A_2$为任意常数.<br><br>当$|\frac{a_1}2|&lt;1$时，收敛；<br><br>当$|\frac{a_1}2|&gt;1$时，发散.<br></p></blockquote><blockquote><p>(3)当$d=a_1^2+4a_2&lt;0$时，即$a_2&lt;-{\frac{a_1^2}4}&lt;=0$，$\alpha_1,\alpha_2$为共轭复根，且<br><br> $$\alpha_1=\frac{a_1+i\sqrt{-d}}2$$<br> $$\alpha_1=\frac{a_1-i\sqrt{-d}}2$$<br> 由于$a+bi=r(cos\theta+isin\theta)$，所以得到$r=\sqrt{a^2+b^2}$，并且我们要知道$cos\theta=\frac{a}{r}$ <br><br> 回到题目，计算得$r=\sqrt{({\frac{a_1}{2})}^2+{-\frac{-d}{4}}}=\sqrt{-a_2}$，从而得$cos\theta=\frac{\frac{a_1}{2}}{\sqrt{-a_2}}=\frac{a_1}{2\sqrt{-a_2}}$. <br><br> 所以得到<br> $$\alpha_1=a+bi=r(cos\theta+isin\theta)=re^{i\theta}$$<br> $$\alpha_2=a-bi=r(cos\theta-isin\theta)=re^{-i\theta}$$<br> 所以<br> $${\alpha_1}^t={(a+bi)}^t=r^t(cos\theta+isin\theta)^t=r^te^{it\theta}$$<br> $${\alpha_2}^t={(a-bi)}^t=r^t(cos\theta-isin\theta)^t=r^te^{-it\theta}$$<br> 所以通解为<br> $$y_t=A_1r^te^{it\theta}+A_2r^te^{-it\theta}=r^t(A_1e^{it\theta}+A_2e^{-it\theta})$$<br> 其中$A_1,A_2$为任意常数.<br><br> 当$r=\sqrt{-a_2}&gt;1$时，即$a_2&lt;-1$时，发散；<br><br>  当$r=\sqrt{-a_2}&lt;1$时，即$-1&lt;a_2&lt;=0$时，收敛；<br><br>   当$r=\sqrt{-a_2}＝1$时，即$a_2=-1$时，波动增幅不变；<br></p></blockquote><p>以上便是二阶齐次差分方程所有可能情况下的解的求法以及解的敛散性的判别.</p><p>下面呢，我们将推广到一般的$n$阶齐次差分方程：<br><br>$$y_t=a_1y_{t-1}+a_2y_{t-2}+…+a_ny_{t-n}，a_n!=0$$<br>假设解的形式为$A\alpha^t$，则特征方程为<br>$$\alpha^n-a_1\alpha^{n-1}-a_2\alpha^{n-2}-…-a_n=0$$<br>从中可以解出非零的<br>$$\alpha_1,\alpha_2,…,\alpha_n$$<br>同样分３种情况，即$\alpha_1,\alpha_2,…,\alpha_n$是互不相同的实根，$\alpha_1,\alpha_2,…,\alpha_n$中有相同的实根和$\alpha_1,\alpha_2,…,\alpha_n$有复根.<br></p><blockquote><p>(1)$\alpha_1,\alpha_2,…,\alpha_n$是$n$个互不相同的实根时，通解为$y_t=A_1\alpha_1^t+A_2\alpha_2^t+…+A_n\alpha_n^t$；<br>(2)$\alpha_1,\alpha_2,…,\alpha_n$中有相同的实根时，不妨设$\alpha_1=\alpha_2=…=\alpha_d$，$\alpha_{d+1},\alpha_{d+2},…,\alpha_n$互不相同，则通解为<br>$$y_t=(A_1+A_2t+A_3t^2+…+A_dt^{d-1})\alpha_1^t+A_{d+1}\alpha_{d+1}^t+A_{d+2}\alpha_{d+2}^t+…+A_{n}\alpha_{n}^t$$<br>(3)$\alpha_1,\alpha_2,…,\alpha_n$有复根时，不妨设$\alpha_1,\alpha_2$为复根，$\alpha_3,\alpha_2,…,\alpha_n$互不相同.<br><br>设$\alpha_1=a+bi,\alpha_2=a-bi$，由于$r=\sqrt{a^2+b^2}，cos\theta=\frac{a}r$，所以有<br>$$\alpha_1=re^{i\theta}$$<br>$$\alpha_2=re^{-i\theta}$$<br>所以通解为<br>$$y_t=r^t(A_1e^{it\theta}+A_2e^{-it\theta})+A_3\alpha_3^t+A_4\alpha_4^t+…+A_n\alpha_n^t$$<br>其中$A_1,A_2,…A_n$为任意常数.</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;差分方程的解析解&quot;&gt;&lt;a href=&quot;#差分方程的解析解&quot; class=&quot;headerlink&quot; title=&quot;差分方程的解析解&quot;&gt;&lt;/a&gt;差分方程的解析解&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;例题&lt;/strong&gt;．求$y_t=a_0+a_1y_{t-1}+\epsi
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Python回归拟合</title>
    <link href="http://yoursite.com/2018/12/29/reg/"/>
    <id>http://yoursite.com/2018/12/29/reg/</id>
    <published>2018-12-29T13:45:57.336Z</published>
    <updated>2018-09-30T10:24:21.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>线性回归</li></ul><p>线性回归的经典解法是最小二乘法，关于最小二乘法的原理网络上都有介绍，咱们这里只看在sklearn中的实现</p><p>我们将采用sklearn自带的美国波斯顿房价数据集进行演示</p><p>首先导入数据并查看数据的基本信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset=load_boston()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">type(dataset)<span class="comment">#数据类型是sklearn的数据集类型</span></span><br></pre></td></tr></table></figure><pre><code>sklearn.datasets.base.Bunch</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset.data.shape<span class="comment">#自变量的维度</span></span><br></pre></td></tr></table></figure><pre><code>(506, 13)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset.target.shape<span class="comment">#因变量的维度</span></span><br></pre></td></tr></table></figure><pre><code>(506,)</code></pre><p>现在来分割数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment"># 随机采样25%的数据构建测试样本，其余作为训练样本。</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target,random_state=<span class="number">33</span>, test_size=<span class="number">0.25</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分析回归目标值的差异。</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"The max target value is"</span>, np.max(dataset.target)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"The min target value is"</span>, np.min(dataset.target)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"The average target value is"</span>, np.mean(dataset.target)</span><br></pre></td></tr></table></figure><pre><code>The max target value is 50.0The min target value is 5.0The average target value is 22.5328063241</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.hist(dataset.target)</span><br><span class="line">plt.show()</span><br><span class="line">plt.hist(dataset.data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_13_0.png" alt="png"></p><p><img src="output_13_1.png" alt="png"></p><p>发现差异较大，所以先进行标准化处理，关于标准化的方法，已经在上一篇文章中讲过，忘记的朋友可以去翻翻看</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#标准化数据</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">ss_x=StandardScaler()</span><br><span class="line">ss_y=StandardScaler()</span><br><span class="line">X_train=ss_x.fit_transform(X_train)</span><br><span class="line">X_test=ss_x.transform(X_test)</span><br><span class="line">y_train=ss_y.fit_transform(y_train)</span><br><span class="line">y_test=ss_y.transform(y_test)</span><br></pre></td></tr></table></figure><pre><code>/home/fantasy/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)/home/fantasy/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)/home/fantasy/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)</code></pre><p>标准化之后，就要开始拟合模型了</p><p>基于最小二乘法的LinearRegression：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">lr = LinearRegression()</span><br><span class="line">lr.fit(X_train, y_train)<span class="comment">#拟合模型</span></span><br><span class="line">lr_y_predict = lr.predict(X_test)<span class="comment">#做预测</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#评估模型</span></span><br><span class="line"><span class="comment"># 使用LinearRegression模型自带的评估模块，并输出评估结果。</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'The value of default measurement of LinearRegression is'</span>, lr.score(X_test, y_test)</span><br></pre></td></tr></table></figure><pre><code>The value of default measurement of LinearRegression is 0.6763403831</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从sklearn.metrics依次导入r2_score、mean_squared_error以及mean_absoluate_error用于回归性能的评估。</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score, mean_squared_error, mean_absolute_error</span><br><span class="line"><span class="comment"># 使用r2_score模块，并输出评估结果。</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'The value of R-squared of LinearRegression is'</span>, r2_score(y_test, lr_y_predict)</span><br></pre></td></tr></table></figure><pre><code>The value of R-squared of LinearRegression is 0.6763403831</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#可视化因变量</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.scatter(np.arange(len(y_test)),y_test,color = <span class="string">'red'</span>,label=<span class="string">'y_test'</span>)</span><br><span class="line">plt.scatter(np.arange(len(lr_y_predict)),lr_y_predict,color = <span class="string">'blue'</span>,label=<span class="string">'y_pred'</span>)</span><br><span class="line">plt.legend(loc=<span class="number">2</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_21_0.png" alt="png"></p><p>拟合效果还不错</p><p>在模型评估时，两种方式是一样的，以后直接用第一种，即模型自带的score就可以了</p><p>但是，一个拟合出来的模型并不是直接可以拿来用的。还需要对其统计性质进行检验</p><p>主要有以下四个检验：<br>（数值型）自变量要与因变量有线性关系；<br>残差基本呈正态分布；<br>残差方差基本不变（同方差性）；<br>残差（样本）间相关独立。</p><p>第一个可以直接绘制每隔变量与因变量之间的散点图（子图）,还是以波斯顿房价为例进行演示，如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">xlabel=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">13</span>):</span><br><span class="line">    x_i=np.array(dataset.data[:,i])</span><br><span class="line">    xlabel.append(x_i)</span><br><span class="line">    plt.style.use(<span class="string">'seaborn'</span>)</span><br><span class="line">    figurei=plt.subplot(<span class="number">2</span>,<span class="number">7</span>,i+<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#figurei.patch.set_facecolor('blue')</span></span><br><span class="line">    figurei.scatter(x_i,dataset.target)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_27_0.png" alt="png"></p><p>检验残差是否基本上呈正态分布也建议直接Spss</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#不确定,建议SPSS</span></span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line">stats.probplot(dataset.target,dist=<span class="string">"norm"</span>, plot=plt)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_29_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#不确定，建议SPSS</span></span><br><span class="line">d= dataset.target</span><br><span class="line">sorted_ = np.sort(d)</span><br><span class="line">yvals = np.arange(len(sorted_))/float(len(sorted_))</span><br><span class="line">plt.plot(sorted_, yvals)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_30_0.png" alt="png"></p><p>共线性检验可直接上Spss,看VIF,简单粗暴</p><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#这个是绘制VIF的程序，没看懂，以后再研究</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">vif2=np.zeros((<span class="number">15</span>,<span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">15</span>):</span><br><span class="line">    tmp=[k <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">13</span>) <span class="keyword">if</span> k!=i]</span><br><span class="line">    <span class="comment">#clf.fit(X2[:,tmp],X2[:,i])</span></span><br><span class="line">    vifi=<span class="number">1</span>/(<span class="number">1</span>-lr.score(X_test, y_test))</span><br><span class="line">    vif2[i]=vifi</span><br><span class="line"></span><br><span class="line">vif3=np.zeros((<span class="number">15</span>,<span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">15</span>):</span><br><span class="line">    tmp=[k <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">15</span>) <span class="keyword">if</span> k!=i]</span><br><span class="line">    <span class="comment">#clf.fit(X3[:,tmp],X3[:,i])</span></span><br><span class="line">    vifi=<span class="number">1</span>/(<span class="number">1</span>-lr.score(X_test, y_test))</span><br><span class="line">    vif3[i]=vifi  </span><br><span class="line">    </span><br><span class="line">plt.figure()</span><br><span class="line">ax = plt.gca()</span><br><span class="line">ax.plot(vif2)</span><br><span class="line">ax.plot(vif3)</span><br><span class="line">plt.xlabel(<span class="string">'feature'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'VIF'</span>)</span><br><span class="line">plt.title(<span class="string">'VIF coefficients of the features'</span>)</span><br><span class="line">plt.axis(<span class="string">'tight'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_33_0.png" alt="png"></p><p>说完了基于最小二乘法的线性回归，咱们接下来看一个<strong>随机梯度下降原理</strong>拟合的线性回归模型</p><p>所谓梯度下降法，就是利用沿着梯度下降最快的方向求偏导数，得到损失函数的全局最小值时的参数θ，涉及到偏导数、学习速度、更新、收敛等问题。</p><p>不过这里我们并不讨论这些，具体的可以看这篇文章<a href="https://github.com/endymecy/spark-ml-source-analysis/blob/master/%E6%9C%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/gradient-descent.md" target="_blank" rel="noopener">https://github.com/endymecy/spark-ml-source-analysis/blob/master/%E6%9C%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/gradient-descent.md</a>　而是在sklearn中实现它，如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line">model = SGDRegressor()</span><br><span class="line">model.fit(X_train,y_train)<span class="comment">#拟合模型</span></span><br></pre></td></tr></table></figure><pre><code>SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,       fit_intercept=True, l1_ratio=0.15, learning_rate=&apos;invscaling&apos;,       loss=&apos;squared_loss&apos;, n_iter=5, penalty=&apos;l2&apos;, power_t=0.25,       random_state=None, shuffle=True, verbose=0, warm_start=False)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sgdr_y_predict=model.predict(X_test)<span class="comment">#做预测</span></span><br></pre></td></tr></table></figure><p>可视化结果y的真实值和预测值之间的差距：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(np.arange(len(y_test)),y_test,color = <span class="string">'red'</span>,label=<span class="string">'y_test'</span>)</span><br><span class="line">plt.scatter(np.arange(len(sgdr_y_predict)),sgdr_y_predict,color = <span class="string">'blue'</span>,label=<span class="string">'y_pre'</span>)</span><br><span class="line">plt.legend(loc=<span class="number">2</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_40_0.png" alt="png"></p><p>看一下R方：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#模型model自带的</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"R_square:"</span>,model.score(X_test,y_test)</span><br></pre></td></tr></table></figure><pre><code>R_square: 0.66058562575</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从sklearn.metrics依次导入r2_score、mean_squared_error以及mean_absoluate_error用于回归性能的评估。</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score, mean_squared_error, mean_absolute_error</span><br><span class="line"><span class="comment"># 使用r2_score模块，并输出评估结果。</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'The value of R-squared of LinearRegression is'</span>, r2_score(y_test, sgdr_y_predict)</span><br></pre></td></tr></table></figure><pre><code>The value of R-squared of LinearRegression is 0.66058562575</code></pre><p>还有一种方法，就是用<strong>岭回归</strong></p><p>岭回归是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法，对病态数据的拟合要强于最小二乘法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge,RidgeCV   <span class="comment"># Ridge岭回归,RidgeCV带有广义交叉验证的岭回归</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ========岭回归========</span></span><br><span class="line">model = Ridge(alpha=<span class="number">0.5</span>)</span><br><span class="line">model = RidgeCV(alphas=[<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">10.0</span>])  <span class="comment"># 通过RidgeCV可以设置多个参数值，算法使用交叉验证获取最佳参数值</span></span><br><span class="line">model.fit(X_train, y_train)   <span class="comment"># 线性回归建模</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'系数矩阵:\n'</span>,model.coef_,model.intercept_</span><br><span class="line"><span class="keyword">print</span> <span class="string">'线性回归模型:\n'</span>,model</span><br><span class="line"><span class="comment"># print('交叉验证最佳alpha值',model.alpha_)  # 只有在使用RidgeCV算法时才有效</span></span><br><span class="line"><span class="comment"># 使用模型预测</span></span><br><span class="line">predicted = model.predict(X_test)</span><br></pre></td></tr></table></figure><pre><code>系数矩阵:[-0.10354081  0.11293307 -0.01049108  0.09295071 -0.15094031  0.32557661 -0.02033021 -0.2991313   0.20061662 -0.15572242 -0.19759762  0.05583187 -0.39404276] 5.52785513551e-15线性回归模型:RidgeCV(alphas=[0.1, 1.0, 10.0], cv=None, fit_intercept=True, gcv_mode=None,    normalize=False, scoring=None, store_cv_values=False)</code></pre><p>结果可视化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(np.arange(len(y_test)),y_test,color = <span class="string">'red'</span>,label=<span class="string">'y_test'</span>)</span><br><span class="line">plt.scatter(np.arange(len(sgdr_y_predict)),sgdr_y_predict,color = <span class="string">'blue'</span>,label=<span class="string">'y_pre'</span>)</span><br><span class="line">plt.legend(loc=<span class="number">2</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_50_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#模型model自带的</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"R_square:"</span>,model.score(X_test,y_test)</span><br></pre></td></tr></table></figure><pre><code>R_square: 0.67691092236</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从sklearn.metrics依次导入r2_score、mean_squared_error以及mean_absoluate_error用于回归性能的评估。</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score, mean_squared_error, mean_absolute_error</span><br><span class="line"><span class="comment"># 使用r2_score模块，并输出评估结果。</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'The value of R-squared of LinearRegression is'</span>, r2_score(y_test, predicted)</span><br></pre></td></tr></table></figure><pre><code>The value of R-squared of LinearRegression is 0.67691092236</code></pre><p>综合上面三种方法的比较，发现岭回归的效果最好</p><p>线性模型掌握这三个完全够用了，下面来看一下非线性模型的回归拟合，主要是关于多项式拟合的，其余的对数，指数拟合这里不再讨论</p><ul><li>多项式拟合</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入线性模型和多项式特征构造模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span>  PolynomialFeatures</span><br><span class="line">poly_reg_x =PolynomialFeatures(degree=<span class="number">2</span>)</span><br><span class="line"><span class="comment">#poly_reg_y =PolynomialFeatures(degree=2)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> X_train.shape<span class="comment">#之前的ｓｉｚｅ</span></span><br><span class="line"><span class="keyword">print</span> X_test.shape</span><br></pre></td></tr></table></figure><pre><code>(379, 13)(127, 13)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X_train=poly_reg_x.fit_transform(X_train)<span class="comment">#每个训练集都被transform</span></span><br><span class="line">X_test=poly_reg_x.transform(X_test)</span><br><span class="line"><span class="comment">#y_train=poly_reg_y.fit_transform(y_train)</span></span><br><span class="line"><span class="comment">#y_test=poly_reg_y.transform(y_test)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> X_train.shape<span class="comment">#之后的ｓｉｚｅ</span></span><br><span class="line"><span class="keyword">print</span> X_test.shape</span><br></pre></td></tr></table></figure><pre><code>(379, 105)(127, 105)</code></pre><p>在构造完多项式特征之后，就可以用之前的线性回归lr来操作了</p><p>注意：在先对数据标准化之后再构造多项式特征与先构造多项式特征再标准化的结果差距很大，就本例而言，前者似乎更有效</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">lr=LinearRegression()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">modeler=lr.fit(X_train,y_train)<span class="comment">#模型拟合</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poly_y_predict=modeler.predict(X_test)<span class="comment">#做预测</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#模型model自带的</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"R_square:"</span>,modeler.score(X_test,y_test)</span><br></pre></td></tr></table></figure><pre><code>R_square: 0.842818486817</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从sklearn.metrics依次导入r2_score、mean_squared_error以及mean_absoluate_error用于回归性能的评估。</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score, mean_squared_error, mean_absolute_error</span><br><span class="line"><span class="comment"># 使用mean_squared_error模块，并输出评估结果。</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'The value of mean_squared_error of LinearRegression is'</span>, mean_squared_error(y_test, predicted)</span><br></pre></td></tr></table></figure><pre><code>The value of mean_squared_error of LinearRegression is 0.290920352888</code></pre><p>均方误差如此小，模型堪称完美</p><p>模型效果可视化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(np.arange(len(y_test)),y_test,color = <span class="string">'red'</span>,label=<span class="string">'y_test'</span>)</span><br><span class="line">plt.scatter(np.arange(len(sgdr_y_predict)),sgdr_y_predict,color = <span class="string">'blue'</span>,label=<span class="string">'y_pre'</span>)</span><br><span class="line">plt.legend(loc=<span class="number">2</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_69_0.png" alt="png"></p><p>以上是在sklearn中的多项式拟合方法，我们可以查看下模型的系数，比较多,这算是一个缺点了（模型难写，容易过拟合）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> modeler.coef_<span class="comment">#由低阶到高阶</span></span><br></pre></td></tr></table></figure><pre><code>[  9.83736707e-13  -2.66737655e-03   3.16462828e-01   1.25375928e+00   3.17151400e+12  -1.61912385e-01   3.80770585e-01  -2.70605062e-01  -2.30644623e-01   6.36550903e-01  -1.23194122e+00   1.82800293e-01   1.47033691e-01  -3.51562500e-01   9.76562500e-03   1.60988998e+00   2.96385193e+00   5.17631531e-01  -2.71759033e-02   5.75256348e-02  -1.39862061e-01  -3.11294556e-01   1.75088501e+00  -4.04202271e+00   9.93446350e-01  -9.21630859e-03   1.07109070e-01  -4.19921875e-02  -4.53796387e-02  -1.73645020e-02  -2.53723145e-01   2.30712891e-02  -3.18298340e-02   1.45568848e-02  -5.79681396e-02   1.94564819e-01  -5.81054688e-02   2.40783691e-02  -1.19384766e-01   1.56875610e-01  -2.15034485e-02   2.81250000e-01   1.61010742e-01   3.10821533e-02   3.52600098e-01   6.44836426e-02  -4.64248657e-02   1.86462402e-02   7.94677734e-02  -3.62548828e-02  -9.95393091e+11  -1.26373291e-01  -9.04617310e-02   6.21032715e-03  -2.34451294e-02  -4.63104248e-02   8.61663818e-02  -5.27343750e-02   3.11126709e-02  -4.30259705e-02  -1.50436401e-01   7.12280273e-02  -1.96792603e-01   1.67648315e-01  -1.43829346e-01   2.98084259e-01  -2.63671875e-01   3.46069336e-02   9.91134644e-02   4.61425781e-02  -1.51935577e-01  -1.54113770e-03  -6.87255859e-02  -2.09899902e-01  -5.36499023e-02  -7.35473633e-03  -7.93457031e-02   1.32598877e-02  -2.28881836e-03   4.61242676e-01  -2.49618530e-01  -2.85339355e-02  -1.33331299e-01  -1.42181396e-01   1.50909424e-01  -7.42797852e-02  -1.14502907e-01  -5.12084961e-02   4.06494141e-02   9.94567871e-02  -8.89060974e-01   8.14544678e-01  -1.85592651e-01  -5.57861328e-02  -2.31964111e-01  -5.03234863e-02   1.87805176e-01   2.02636719e-02  -1.73187256e-02   4.75559235e-02   2.38952637e-02   3.66210938e-03  -3.41796875e-03  -2.86254883e-02   6.39343262e-02]</code></pre><p>以上也是基于最小二乘原理的，因为我们只是用sklearn的多项式构造模块将原来的线性数据通过列方向的扩充，变成了多项式的形式，但还是用的LinearRegression来拟合模型的，那么，我们可以试一下别的原理，比如下面的<strong>岭回归</strong>拟合多项式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line">ridge=Ridge(alpha=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> X_train.shape<span class="comment">#之前的ｓｉｚｅ</span></span><br><span class="line"><span class="keyword">print</span> X_test.shape</span><br></pre></td></tr></table></figure><pre><code>(379, 13)(127, 13)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入构造多项式特征模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span>  PolynomialFeatures</span><br><span class="line">poly_reg =PolynomialFeatures(degree=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h3 id="在下一步之前对原始数据进行了标准化！！！"><a href="#在下一步之前对原始数据进行了标准化！！！" class="headerlink" title="在下一步之前对原始数据进行了标准化！！！"></a>在下一步之前对原始数据进行了标准化！！！</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在这一步之前对原始数据进行了标准化！！！</span></span><br><span class="line">X_train=poly_reg.fit_transform(X_train)<span class="comment">#每个训练集都被transform</span></span><br><span class="line">X_test=poly_reg.transform(X_test)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> X_train.shape<span class="comment">#之后的ｓｉｚｅ</span></span><br><span class="line"><span class="keyword">print</span> X_test.shape</span><br></pre></td></tr></table></figure><pre><code>(379, 105)(127, 105)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poly_ridge=ridge.fit(X_train,y_train)<span class="comment">#模型拟合</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poly_ridge_y_predict=ridge.predict(X_test)<span class="comment">#做预测</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#模型ridge自带的</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"R_square:"</span>,ridge.score(X_test,y_test)</span><br></pre></td></tr></table></figure><pre><code>R_square: 0.846155705955</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#从sklearn.metrics依次导入r2_score、mean_squared_error以及mean_absoluate_error用于回归性能的评估。</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score, mean_squared_error, mean_absolute_error</span><br><span class="line"><span class="comment"># 使用mean_squared_error模块，并输出评估结果。</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'The value of mean_squared_error of RidgeRegression is'</span>, mean_squared_error(y_test, poly_ridge_y_predict)</span><br></pre></td></tr></table></figure><pre><code>The value of mean_squared_error of RidgeRegression is 0.138526615137</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看模型的系数</span></span><br><span class="line"><span class="keyword">print</span> ridge.coef_,ridge.intercept_</span><br></pre></td></tr></table></figure><pre><code>[ 0.         -0.01515184 -0.10580862  0.27932288  0.01645974 -0.14657861  0.36744518 -0.22397917 -0.21912044  0.05965385 -0.04161497 -0.08866449  0.11792374 -0.3637897   0.01224963  0.04046505  0.16591023  0.47025105 -0.0426397   0.06610476 -0.07187838 -0.14978614 -0.23375497 -0.01411628  0.05016413 -0.00793163  0.09939217 -0.0134973  -0.02031623  0.00222154 -0.13674295  0.02549065 -0.02315901  0.00183563 -0.00664953  0.17951566 -0.02818604 -0.03342595 -0.10510401  0.10889808 -0.00633295  0.33583991  0.14526388  0.04291548  0.32826641  0.07628581  0.00221103 -0.0020726  0.03954039 -0.02489515  0.05244391 -0.11941144 -0.08827233  0.01151196 -0.028727   -0.0410782   0.06641088 -0.0236821  -0.00505518 -0.04825191 -0.12339398  0.0680945  -0.1614648   0.13523431 -0.08524669  0.11271328 -0.182551    0.03326487  0.10387014  0.04437453 -0.14262386  0.00168108 -0.06360327 -0.20487222 -0.06044155 -0.01195337 -0.08105273  0.01500186  0.01720694  0.32904656 -0.16341483 -0.03929378 -0.13649985 -0.14039058  0.14996113 -0.11682082 -0.09929801 -0.06146238  0.0137472   0.07554982 -0.50475006  0.39750343 -0.098317   -0.06266169 -0.16932652 -0.04422031  0.18347525  0.04147819 -0.10451011  0.0364601   0.0112839   0.02664297 -0.00190007 -0.02998467  0.07018101] -0.190718574726</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#可视化效果</span></span><br><span class="line">plt.scatter(np.arange(len(y_test)),y_test,color = <span class="string">'red'</span>,label=<span class="string">'y_test'</span>)</span><br><span class="line">plt.scatter(np.arange(len(poly_ridge_y_predict)),poly_ridge_y_predict,color = <span class="string">'blue'</span>,label=<span class="string">'y_pre'</span>)</span><br><span class="line">plt.legend(loc=<span class="number">2</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_84_0.png" alt="png"></p><p>总结一下关于sklearn中的PolynomialFeatures的用法，就是<strong>最好在构造多项式特征之前对原始的数据（x和y）进行标准化处理</strong>，然后就可以使用基于最小二乘法的LinearRegression或者基于别的原理的RidgeRegression了.</p><hr><p>其实，在numpy中也有多项式拟合的模块，只是只能拟合一元的多项式，即一个自变量和一个因变量，下面就一起来看一下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">z1 = np.polyfit(X_train[:,<span class="number">1</span>], y_train, <span class="number">1</span>)  <span class="comment">#一次多项式拟合，相当于线性拟合,返回的是[k,b]，即模型的系数</span></span><br><span class="line">p1 = np.poly1d(z1)<span class="comment">#给出模型表达式，真ｔｍ人性化</span></span><br><span class="line"><span class="keyword">print</span> z1  <span class="comment">#[ 1.          1.49333333]</span></span><br><span class="line"><span class="keyword">print</span> p1  <span class="comment"># 1 x + 1.493</span></span><br></pre></td></tr></table></figure><pre><code>[  0.13493869  21.35130147]0.1349 x + 21.35</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z = np.polyval(z1, X_train[:,<span class="number">1</span>])<span class="comment">#用刚刚拟合处理的模型z1来代入X_train[:,1]求得预模型的测值并保存在z中</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z</span><br></pre></td></tr></table></figure><pre><code>array([ 24.05007536,  21.35130147,  21.35130147,  28.77292967,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  34.17047745,  24.05007536,  23.03803515,        21.35130147,  21.35130147,  21.35130147,  24.05007536,        25.93921708,  32.14639703,  21.35130147,  25.80427839,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  32.14639703,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  24.31995275,  21.35130147,        21.35130147,  26.07415578,  21.35130147,  21.35130147,        32.14639703,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  24.31995275,  21.35130147,        24.72476883,  24.05007536,  21.35130147,  21.35130147,        21.35130147,  24.05007536,  25.12958491,  21.35130147,        21.35130147,  24.72476883,  21.35130147,  32.82109051,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  32.14639703,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  26.07415578,  28.43558293,        21.35130147,  31.47170356,  34.17047745,  24.18501405,        24.05007536,  21.35130147,  21.35130147,  24.05007536,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        32.14639703,  25.3994623 ,  21.35130147,  21.35130147,        21.35130147,  24.05007536,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        24.05007536,  24.72476883,  24.05007536,  21.35130147,        24.72476883,  23.03803515,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        32.14639703,  21.35130147,  25.93921708,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  23.03803515,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  26.74884925,  21.35130147,  28.43558293,        21.35130147,  21.35130147,  29.44762314,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  25.80427839,        24.05007536,  25.3994623 ,  21.35130147,  21.35130147,        21.35130147,  23.03803515,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  25.12958491,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        32.14639703,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  25.12958491,  24.72476883,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  28.77292967,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  26.74884925,  24.05007536,  26.74884925,        21.35130147,  21.35130147,  21.35130147,  24.05007536,        23.03803515,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  27.42354272,  21.35130147,  21.35130147,        24.31995275,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  24.05007536,        27.42354272,  21.35130147,  24.31995275,  21.35130147,        27.42354272,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  28.43558293,  21.35130147,  32.14639703,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  24.05007536,  21.35130147,        31.47170356,  25.80427839,  24.72476883,  26.74884925,        21.35130147,  21.35130147,  24.05007536,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  24.31995275,  21.35130147,  21.35130147,        24.18501405,  33.49578398,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  30.79701009,        21.35130147,  29.44762314,  21.35130147,  32.48374377,        21.35130147,  21.35130147,  21.35130147,  32.14639703,        32.14639703,  21.35130147,  24.31995275,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  25.3994623 ,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        33.49578398,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  23.03803515,  31.47170356,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        27.42354272,  24.05007536,  24.72476883,  34.17047745,        33.49578398,  24.18501405,  21.35130147,  21.35130147,        21.35130147,  24.05007536,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  24.31995275,        21.35130147,  21.35130147,  23.03803515,  21.35130147,        23.03803515,  24.72476883,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  28.77292967,        21.35130147,  21.35130147,  24.31995275,  21.35130147,        21.35130147,  24.72476883,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        32.14639703,  25.93921708,  21.35130147,  21.35130147,        21.35130147,  24.72476883,  21.35130147,  32.14639703,        34.84517093,  32.48374377,  21.35130147,  32.14639703,        21.35130147,  21.35130147,  21.35130147])</code></pre><p>或者我们直接把自变量的值代入拟合好的方程里面,得到的结果和上面的一样.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_pre=p1(X_train[:,<span class="number">1</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_pre</span><br></pre></td></tr></table></figure><pre><code>array([ 24.05007536,  21.35130147,  21.35130147,  28.77292967,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  34.17047745,  24.05007536,  23.03803515,        21.35130147,  21.35130147,  21.35130147,  24.05007536,        25.93921708,  32.14639703,  21.35130147,  25.80427839,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  32.14639703,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  24.31995275,  21.35130147,        21.35130147,  26.07415578,  21.35130147,  21.35130147,        32.14639703,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  24.31995275,  21.35130147,        24.72476883,  24.05007536,  21.35130147,  21.35130147,        21.35130147,  24.05007536,  25.12958491,  21.35130147,        21.35130147,  24.72476883,  21.35130147,  32.82109051,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  32.14639703,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  26.07415578,  28.43558293,        21.35130147,  31.47170356,  34.17047745,  24.18501405,        24.05007536,  21.35130147,  21.35130147,  24.05007536,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        32.14639703,  25.3994623 ,  21.35130147,  21.35130147,        21.35130147,  24.05007536,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        24.05007536,  24.72476883,  24.05007536,  21.35130147,        24.72476883,  23.03803515,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        32.14639703,  21.35130147,  25.93921708,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  23.03803515,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  26.74884925,  21.35130147,  28.43558293,        21.35130147,  21.35130147,  29.44762314,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  25.80427839,        24.05007536,  25.3994623 ,  21.35130147,  21.35130147,        21.35130147,  23.03803515,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  25.12958491,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        32.14639703,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  25.12958491,  24.72476883,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  28.77292967,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  26.74884925,  24.05007536,  26.74884925,        21.35130147,  21.35130147,  21.35130147,  24.05007536,        23.03803515,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  27.42354272,  21.35130147,  21.35130147,        24.31995275,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  24.05007536,        27.42354272,  21.35130147,  24.31995275,  21.35130147,        27.42354272,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  28.43558293,  21.35130147,  32.14639703,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  24.05007536,  21.35130147,        31.47170356,  25.80427839,  24.72476883,  26.74884925,        21.35130147,  21.35130147,  24.05007536,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  24.31995275,  21.35130147,  21.35130147,        24.18501405,  33.49578398,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  30.79701009,        21.35130147,  29.44762314,  21.35130147,  32.48374377,        21.35130147,  21.35130147,  21.35130147,  32.14639703,        32.14639703,  21.35130147,  24.31995275,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  25.3994623 ,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        33.49578398,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  23.03803515,  31.47170356,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        27.42354272,  24.05007536,  24.72476883,  34.17047745,        33.49578398,  24.18501405,  21.35130147,  21.35130147,        21.35130147,  24.05007536,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  24.31995275,        21.35130147,  21.35130147,  23.03803515,  21.35130147,        23.03803515,  24.72476883,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  28.77292967,        21.35130147,  21.35130147,  24.31995275,  21.35130147,        21.35130147,  24.72476883,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        32.14639703,  25.93921708,  21.35130147,  21.35130147,        21.35130147,  24.72476883,  21.35130147,  32.14639703,        34.84517093,  32.48374377,  21.35130147,  32.14639703,        21.35130147,  21.35130147,  21.35130147])</code></pre><p>这种就可以直观的可视化真实值与预测曲线之间的关系了，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X_train[:,<span class="number">1</span>], y_train,color=<span class="string">'red'</span>,label=<span class="string">'true'</span>)</span><br><span class="line">plt.plot(X_train[:,<span class="number">1</span>],y_pre,color=<span class="string">'blue'</span>,label=<span class="string">'pre'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'best'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_95_0.png" alt="png"></p><p>这里我在网上找了一个numpy拟合多项式的例子，贴在下面了，供大家参考</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#多项式拟合(从给定的x,y中解析出最接近数据的方程式)</span></span><br><span class="line"><span class="comment">#要拟合的x,y数据</span></span><br><span class="line">x = np.arange(<span class="number">1</span>, <span class="number">17</span>, <span class="number">1</span>)</span><br><span class="line">y = np.array([<span class="number">4.00</span>, <span class="number">6.40</span>, <span class="number">8.00</span>, <span class="number">8.80</span>, <span class="number">9.22</span>, <span class="number">9.50</span>, <span class="number">9.70</span>, <span class="number">9.86</span>, <span class="number">10.00</span>, <span class="number">10.20</span>, <span class="number">10.32</span>, <span class="number">10.42</span>, <span class="number">10.50</span>, <span class="number">10.55</span>, <span class="number">10.58</span>, <span class="number">10.60</span>])</span><br><span class="line">z1 = np.polyfit(x, y, <span class="number">4</span>)<span class="comment">#3为多项式最高次幂，结果为多项式的各个系数</span></span><br><span class="line"><span class="comment">#最高次幂3，得到4个系数,从高次到低次排列</span></span><br><span class="line"><span class="comment">#最高次幂取几要视情况而定</span></span><br><span class="line">p1 = np.poly1d(z1)<span class="comment">#将系数代入方程，得到函式p1</span></span><br><span class="line">print(z1)<span class="comment">#多项式系数</span></span><br><span class="line">print(p1)<span class="comment">#多项式方程</span></span><br><span class="line">print(p1(<span class="number">18</span>))<span class="comment">#调用，输入x值，得到y</span></span><br><span class="line">x1=np.linspace(x.min(),x.max(),<span class="number">100</span>)<span class="comment">#x给定数据太少，方程曲线不光滑，多取x值得到光滑曲线</span></span><br><span class="line">pp1=p1(x1)<span class="comment">#x1代入多项式，得到pp1,代入matplotlib中画多项式曲线</span></span><br><span class="line">plt.rcParams[<span class="string">'font.sans-serif'</span>]=[<span class="string">'SimHei'</span>]<span class="comment">#显示中文</span></span><br><span class="line">plt.scatter(x,y,color=<span class="string">'g'</span>)<span class="comment">#x，y散点图</span></span><br><span class="line">plt.plot(x,y,color=<span class="string">'r'</span>)<span class="comment">#x,y线形图</span></span><br><span class="line">plt.plot(x1,pp1,color=<span class="string">'b'</span>)<span class="comment">#100个x及对应y值绘制的曲线</span></span><br><span class="line"><span class="comment">#可应用于各个行业的数值预估</span></span><br><span class="line">plt.legend(loc=<span class="string">'best'</span>)</span><br><span class="line"><span class="comment">#plt.savefig('polyfit.png',dpi=400,bbox_inches='tight')</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>[ -9.24538084e-04   3.76792011e-02  -5.54639386e-01   3.60545597e+00   1.03629808e+00]            4           3          2-0.0009245 x + 0.03768 x - 0.5546 x + 3.605 x + 1.0368.922135181</code></pre><p><img src="output_97_1.png" alt="png"></p><p>关于回归拟合的问题就说这么多，在用到的时候直接拿以上代码稍微修改一下便可使用了，更多干货请关注微信公众号“我将在南极找寻你”！</p><p>下课！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;线性回归&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;线性回归的经典解法是最小二乘法，关于最小二乘法的原理网络上都有介绍，咱们这里只看在sklearn中的实现&lt;/p&gt;
&lt;p&gt;我们将采用sklearn自带的美国波斯顿房价数据集进行演示&lt;/p&gt;
&lt;p&gt;首先导入数据并查看数据的基本信
      
    
    </summary>
    
      <category term="Data Analysis&amp;Mining with Python" scheme="http://yoursite.com/categories/Data-Analysis-Mining-with-Python/"/>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Python聚类分析</title>
    <link href="http://yoursite.com/2018/12/29/%E8%81%9A%E7%B1%BB/"/>
    <id>http://yoursite.com/2018/12/29/聚类/</id>
    <published>2018-12-29T13:45:57.336Z</published>
    <updated>2018-09-30T10:01:36.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Kmean聚类</li></ul><p>以下使用的是sklearn自带的鸢尾花数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##加载数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line">iris=load_iris()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.array(iris)</span><br></pre></td></tr></table></figure><pre><code>array({&apos;target_names&apos;: array([&apos;setosa&apos;, &apos;versicolor&apos;, &apos;virginica&apos;],       dtype=&apos;|S10&apos;), &apos;data&apos;: array([[ 5.1,  3.5,  1.4,  0.2],       [ 4.9,  3. ,  1.4,  0.2],       [ 4.7,  3.2,  1.3,  0.2],       [ 4.6,  3.1,  1.5,  0.2],       [ 5. ,  3.6,  1.4,  0.2],       [ 5.4,  3.9,  1.7,  0.4],       [ 4.6,  3.4,  1.4,  0.3],       [ 5. ,  3.4,  1.5,  0.2],       [ 4.4,  2.9,  1.4,  0.2],       [ 4.9,  3.1,  1.5,  0.1],       [ 5.4,  3.7,  1.5,  0.2],       [ 4.8,  3.4,  1.6,  0.2],       [ 4.8,  3. ,  1.4,  0.1],       [ 4.3,  3. ,  1.1,  0.1],       [ 5.8,  4. ,  1.2,  0.2],       [ 5.7,  4.4,  1.5,  0.4],       [ 5.4,  3.9,  1.3,  0.4],       [ 5.1,  3.5,  1.4,  0.3],       [ 5.7,  3.8,  1.7,  0.3],       [ 5.1,  3.8,  1.5,  0.3],       [ 5.4,  3.4,  1.7,  0.2],       [ 5.1,  3.7,  1.5,  0.4],       [ 4.6,  3.6,  1. ,  0.2],       [ 5.1,  3.3,  1.7,  0.5],       [ 4.8,  3.4,  1.9,  0.2],       [ 5. ,  3. ,  1.6,  0.2],       [ 5. ,  3.4,  1.6,  0.4],       [ 5.2,  3.5,  1.5,  0.2],       [ 5.2,  3.4,  1.4,  0.2],       [ 4.7,  3.2,  1.6,  0.2],       [ 4.8,  3.1,  1.6,  0.2],       [ 5.4,  3.4,  1.5,  0.4],       [ 5.2,  4.1,  1.5,  0.1],       [ 5.5,  4.2,  1.4,  0.2],       [ 4.9,  3.1,  1.5,  0.1],       [ 5. ,  3.2,  1.2,  0.2],       [ 5.5,  3.5,  1.3,  0.2],       [ 4.9,  3.1,  1.5,  0.1],       [ 4.4,  3. ,  1.3,  0.2],       [ 5.1,  3.4,  1.5,  0.2],       [ 5. ,  3.5,  1.3,  0.3],       [ 4.5,  2.3,  1.3,  0.3],       [ 4.4,  3.2,  1.3,  0.2],       [ 5. ,  3.5,  1.6,  0.6],       [ 5.1,  3.8,  1.9,  0.4],       [ 4.8,  3. ,  1.4,  0.3],       [ 5.1,  3.8,  1.6,  0.2],       [ 4.6,  3.2,  1.4,  0.2],       [ 5.3,  3.7,  1.5,  0.2],       [ 5. ,  3.3,  1.4,  0.2],       [ 7. ,  3.2,  4.7,  1.4],       [ 6.4,  3.2,  4.5,  1.5],       [ 6.9,  3.1,  4.9,  1.5],       [ 5.5,  2.3,  4. ,  1.3],       [ 6.5,  2.8,  4.6,  1.5],       [ 5.7,  2.8,  4.5,  1.3],       [ 6.3,  3.3,  4.7,  1.6],       [ 4.9,  2.4,  3.3,  1. ],       [ 6.6,  2.9,  4.6,  1.3],       [ 5.2,  2.7,  3.9,  1.4],       [ 5. ,  2. ,  3.5,  1. ],       [ 5.9,  3. ,  4.2,  1.5],       [ 6. ,  2.2,  4. ,  1. ],       [ 6.1,  2.9,  4.7,  1.4],       [ 5.6,  2.9,  3.6,  1.3],       [ 6.7,  3.1,  4.4,  1.4],       [ 5.6,  3. ,  4.5,  1.5],       [ 5.8,  2.7,  4.1,  1. ],       [ 6.2,  2.2,  4.5,  1.5],       [ 5.6,  2.5,  3.9,  1.1],       [ 5.9,  3.2,  4.8,  1.8],       [ 6.1,  2.8,  4. ,  1.3],       [ 6.3,  2.5,  4.9,  1.5],       [ 6.1,  2.8,  4.7,  1.2],       [ 6.4,  2.9,  4.3,  1.3],       [ 6.6,  3. ,  4.4,  1.4],       [ 6.8,  2.8,  4.8,  1.4],       [ 6.7,  3. ,  5. ,  1.7],       [ 6. ,  2.9,  4.5,  1.5],       [ 5.7,  2.6,  3.5,  1. ],       [ 5.5,  2.4,  3.8,  1.1],       [ 5.5,  2.4,  3.7,  1. ],       [ 5.8,  2.7,  3.9,  1.2],       [ 6. ,  2.7,  5.1,  1.6],       [ 5.4,  3. ,  4.5,  1.5],       [ 6. ,  3.4,  4.5,  1.6],       [ 6.7,  3.1,  4.7,  1.5],       [ 6.3,  2.3,  4.4,  1.3],       [ 5.6,  3. ,  4.1,  1.3],       [ 5.5,  2.5,  4. ,  1.3],       [ 5.5,  2.6,  4.4,  1.2],       [ 6.1,  3. ,  4.6,  1.4],       [ 5.8,  2.6,  4. ,  1.2],       [ 5. ,  2.3,  3.3,  1. ],       [ 5.6,  2.7,  4.2,  1.3],       [ 5.7,  3. ,  4.2,  1.2],       [ 5.7,  2.9,  4.2,  1.3],       [ 6.2,  2.9,  4.3,  1.3],       [ 5.1,  2.5,  3. ,  1.1],       [ 5.7,  2.8,  4.1,  1.3],       [ 6.3,  3.3,  6. ,  2.5],       [ 5.8,  2.7,  5.1,  1.9],       [ 7.1,  3. ,  5.9,  2.1],       [ 6.3,  2.9,  5.6,  1.8],       [ 6.5,  3. ,  5.8,  2.2],       [ 7.6,  3. ,  6.6,  2.1],       [ 4.9,  2.5,  4.5,  1.7],       [ 7.3,  2.9,  6.3,  1.8],       [ 6.7,  2.5,  5.8,  1.8],       [ 7.2,  3.6,  6.1,  2.5],       [ 6.5,  3.2,  5.1,  2. ],       [ 6.4,  2.7,  5.3,  1.9],       [ 6.8,  3. ,  5.5,  2.1],       [ 5.7,  2.5,  5. ,  2. ],       [ 5.8,  2.8,  5.1,  2.4],       [ 6.4,  3.2,  5.3,  2.3],       [ 6.5,  3. ,  5.5,  1.8],       [ 7.7,  3.8,  6.7,  2.2],       [ 7.7,  2.6,  6.9,  2.3],       [ 6. ,  2.2,  5. ,  1.5],       [ 6.9,  3.2,  5.7,  2.3],       [ 5.6,  2.8,  4.9,  2. ],       [ 7.7,  2.8,  6.7,  2. ],       [ 6.3,  2.7,  4.9,  1.8],       [ 6.7,  3.3,  5.7,  2.1],       [ 7.2,  3.2,  6. ,  1.8],       [ 6.2,  2.8,  4.8,  1.8],       [ 6.1,  3. ,  4.9,  1.8],       [ 6.4,  2.8,  5.6,  2.1],       [ 7.2,  3. ,  5.8,  1.6],       [ 7.4,  2.8,  6.1,  1.9],       [ 7.9,  3.8,  6.4,  2. ],       [ 6.4,  2.8,  5.6,  2.2],       [ 6.3,  2.8,  5.1,  1.5],       [ 6.1,  2.6,  5.6,  1.4],       [ 7.7,  3. ,  6.1,  2.3],       [ 6.3,  3.4,  5.6,  2.4],       [ 6.4,  3.1,  5.5,  1.8],       [ 6. ,  3. ,  4.8,  1.8],       [ 6.9,  3.1,  5.4,  2.1],       [ 6.7,  3.1,  5.6,  2.4],       [ 6.9,  3.1,  5.1,  2.3],       [ 5.8,  2.7,  5.1,  1.9],       [ 6.8,  3.2,  5.9,  2.3],       [ 6.7,  3.3,  5.7,  2.5],       [ 6.7,  3. ,  5.2,  2.3],       [ 6.3,  2.5,  5. ,  1.9],       [ 6.5,  3. ,  5.2,  2. ],       [ 6.2,  3.4,  5.4,  2.3],       [ 5.9,  3. ,  5.1,  1.8]]), &apos;target&apos;: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), &apos;DESCR&apos;: &apos;Iris Plants Database\n====================\n\nNotes\n-----\nData Set Characteristics:\n    :Number of Instances: 150 (50 in each of three classes)\n    :Number of Attributes: 4 numeric, predictive attributes and the class\n    :Attribute Information:\n        - sepal length in cm\n        - sepal width in cm\n        - petal length in cm\n        - petal width in cm\n        - class:\n                - Iris-Setosa\n                - Iris-Versicolour\n                - Iris-Virginica\n    :Summary Statistics:\n\n    ============== ==== ==== ======= ===== ====================\n                    Min  Max   Mean    SD   Class Correlation\n    ============== ==== ==== ======= ===== ====================\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\n    ============== ==== ==== ======= ===== ====================\n\n    :Missing Attribute Values: None\n    :Class Distribution: 33.3% for each of 3 classes.\n    :Creator: R.A. Fisher\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n    :Date: July, 1988\n\nThis is a copy of UCI ML iris datasets.\nhttp://archive.ics.uci.edu/ml/datasets/Iris\n\nThe famous Iris database, first used by Sir R.A Fisher\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher\&apos;s paper is a classic in the field and\nis referenced frequently to this day.  (See Duda &amp; Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\nReferences\n----------\n   - Fisher,R.A. &quot;The use of multiple measurements in taxonomic problems&quot;\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in &quot;Contributions to\n     Mathematical Statistics&quot; (John Wiley, NY, 1950).\n   - Duda,R.O., &amp; Hart,P.E. (1973) Pattern Classification and Scene Analysis.\n     (Q327.D83) John Wiley &amp; Sons.  ISBN 0-471-22361-1.  See page 218.\n   - Dasarathy, B.V. (1980) &quot;Nosing Around the Neighborhood: A New System\n     Structure and Classification Rule for Recognition in Partially Exposed\n     Environments&quot;.  IEEE Transactions on Pattern Analysis and Machine\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n   - Gates, G.W. (1972) &quot;The Reduced Nearest Neighbor Rule&quot;.  IEEE Transactions\n     on Information Theory, May 1972, 431-433.\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al&quot;s AUTOCLASS II\n     conceptual clustering system finds 3 classes in the data.\n   - Many, many more ...\n&apos;, &apos;feature_names&apos;: [&apos;sepal length (cm)&apos;, &apos;sepal width (cm)&apos;, &apos;petal length (cm)&apos;, &apos;petal width (cm)&apos;]}, dtype=object)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X = iris.data[:, <span class="number">2</span>:<span class="number">4</span>] <span class="comment">##表示我们只取特征空间中的后两个维度</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X</span><br></pre></td></tr></table></figure><pre><code>array([[ 1.4,  0.2],       [ 1.4,  0.2],       [ 1.3,  0.2],       [ 1.5,  0.2],       [ 1.4,  0.2],       [ 1.7,  0.4],       [ 1.4,  0.3],       [ 1.5,  0.2],       [ 1.4,  0.2],       [ 1.5,  0.1],       [ 1.5,  0.2],       [ 1.6,  0.2],       [ 1.4,  0.1],       [ 1.1,  0.1],       [ 1.2,  0.2],       [ 1.5,  0.4],       [ 1.3,  0.4],       [ 1.4,  0.3],       [ 1.7,  0.3],       [ 1.5,  0.3],       [ 1.7,  0.2],       [ 1.5,  0.4],       [ 1. ,  0.2],       [ 1.7,  0.5],       [ 1.9,  0.2],       [ 1.6,  0.2],       [ 1.6,  0.4],       [ 1.5,  0.2],       [ 1.4,  0.2],       [ 1.6,  0.2],       [ 1.6,  0.2],       [ 1.5,  0.4],       [ 1.5,  0.1],       [ 1.4,  0.2],       [ 1.5,  0.1],       [ 1.2,  0.2],       [ 1.3,  0.2],       [ 1.5,  0.1],       [ 1.3,  0.2],       [ 1.5,  0.2],       [ 1.3,  0.3],       [ 1.3,  0.3],       [ 1.3,  0.2],       [ 1.6,  0.6],       [ 1.9,  0.4],       [ 1.4,  0.3],       [ 1.6,  0.2],       [ 1.4,  0.2],       [ 1.5,  0.2],       [ 1.4,  0.2],       [ 4.7,  1.4],       [ 4.5,  1.5],       [ 4.9,  1.5],       [ 4. ,  1.3],       [ 4.6,  1.5],       [ 4.5,  1.3],       [ 4.7,  1.6],       [ 3.3,  1. ],       [ 4.6,  1.3],       [ 3.9,  1.4],       [ 3.5,  1. ],       [ 4.2,  1.5],       [ 4. ,  1. ],       [ 4.7,  1.4],       [ 3.6,  1.3],       [ 4.4,  1.4],       [ 4.5,  1.5],       [ 4.1,  1. ],       [ 4.5,  1.5],       [ 3.9,  1.1],       [ 4.8,  1.8],       [ 4. ,  1.3],       [ 4.9,  1.5],       [ 4.7,  1.2],       [ 4.3,  1.3],       [ 4.4,  1.4],       [ 4.8,  1.4],       [ 5. ,  1.7],       [ 4.5,  1.5],       [ 3.5,  1. ],       [ 3.8,  1.1],       [ 3.7,  1. ],       [ 3.9,  1.2],       [ 5.1,  1.6],       [ 4.5,  1.5],       [ 4.5,  1.6],       [ 4.7,  1.5],       [ 4.4,  1.3],       [ 4.1,  1.3],       [ 4. ,  1.3],       [ 4.4,  1.2],       [ 4.6,  1.4],       [ 4. ,  1.2],       [ 3.3,  1. ],       [ 4.2,  1.3],       [ 4.2,  1.2],       [ 4.2,  1.3],       [ 4.3,  1.3],       [ 3. ,  1.1],       [ 4.1,  1.3],       [ 6. ,  2.5],       [ 5.1,  1.9],       [ 5.9,  2.1],       [ 5.6,  1.8],       [ 5.8,  2.2],       [ 6.6,  2.1],       [ 4.5,  1.7],       [ 6.3,  1.8],       [ 5.8,  1.8],       [ 6.1,  2.5],       [ 5.1,  2. ],       [ 5.3,  1.9],       [ 5.5,  2.1],       [ 5. ,  2. ],       [ 5.1,  2.4],       [ 5.3,  2.3],       [ 5.5,  1.8],       [ 6.7,  2.2],       [ 6.9,  2.3],       [ 5. ,  1.5],       [ 5.7,  2.3],       [ 4.9,  2. ],       [ 6.7,  2. ],       [ 4.9,  1.8],       [ 5.7,  2.1],       [ 6. ,  1.8],       [ 4.8,  1.8],       [ 4.9,  1.8],       [ 5.6,  2.1],       [ 5.8,  1.6],       [ 6.1,  1.9],       [ 6.4,  2. ],       [ 5.6,  2.2],       [ 5.1,  1.5],       [ 5.6,  1.4],       [ 6.1,  2.3],       [ 5.6,  2.4],       [ 5.5,  1.8],       [ 4.8,  1.8],       [ 5.4,  2.1],       [ 5.6,  2.4],       [ 5.1,  2.3],       [ 5.1,  1.9],       [ 5.9,  2.3],       [ 5.7,  2.5],       [ 5.2,  2.3],       [ 5. ,  1.9],       [ 5.2,  2. ],       [ 5.4,  2.3],       [ 5.1,  1.8]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#绘制数据分布图</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c = <span class="string">"green"</span>, marker=<span class="string">'*'</span>, label=<span class="string">'point'</span>)  </span><br><span class="line">plt.xlabel(<span class="string">'petal length'</span>)  </span><br><span class="line">plt.ylabel(<span class="string">'petal width'</span>)  </span><br><span class="line">plt.legend(loc=<span class="number">2</span>)  </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_9_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">estimator = KMeans(n_clusters=<span class="number">3</span>)<span class="comment">#构造聚类器</span></span><br><span class="line">estimator.fit(X)<span class="comment">#聚类</span></span><br><span class="line">label_pred = estimator.labels_ <span class="comment">#获取聚类标签</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">label_pred</span><br></pre></td></tr></table></figure><pre><code>array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,       0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,       2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2,       2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,       2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#绘制k-means结果</span></span><br><span class="line">x0 = X[label_pred == <span class="number">0</span>]</span><br><span class="line">x1 = X[label_pred == <span class="number">1</span>]</span><br><span class="line">x2 = X[label_pred == <span class="number">2</span>]</span><br><span class="line">plt.scatter(x0[:, <span class="number">0</span>], x0[:, <span class="number">1</span>], c = <span class="string">"red"</span>, marker=<span class="string">'o'</span>, label=<span class="string">'label0'</span>)  </span><br><span class="line">plt.scatter(x1[:, <span class="number">0</span>], x1[:, <span class="number">1</span>], c = <span class="string">"green"</span>, marker=<span class="string">'*'</span>, label=<span class="string">'label1'</span>)  </span><br><span class="line">plt.scatter(x2[:, <span class="number">0</span>], x2[:, <span class="number">1</span>], c = <span class="string">"blue"</span>, marker=<span class="string">'+'</span>, label=<span class="string">'label2'</span>)  </span><br><span class="line">plt.xlabel(<span class="string">'petal length'</span>)  </span><br><span class="line">plt.ylabel(<span class="string">'petal width'</span>)  </span><br><span class="line">plt.legend(loc=<span class="number">2</span>)  </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_12_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X.shape</span><br></pre></td></tr></table></figure><pre><code>(150, 2)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X=X.tolist()</span><br><span class="line">label_pred=label_pred.tolist()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X</span><br></pre></td></tr></table></figure><pre><code>[[1.4, 0.2], [1.4, 0.2], [1.3, 0.2], [1.5, 0.2], [1.4, 0.2], [1.7, 0.4], [1.4, 0.3], [1.5, 0.2], [1.4, 0.2], [1.5, 0.1], [1.5, 0.2], [1.6, 0.2], [1.4, 0.1], [1.1, 0.1], [1.2, 0.2], [1.5, 0.4], [1.3, 0.4], [1.4, 0.3], [1.7, 0.3], [1.5, 0.3], [1.7, 0.2], [1.5, 0.4], [1.0, 0.2], [1.7, 0.5], [1.9, 0.2], [1.6, 0.2], [1.6, 0.4], [1.5, 0.2], [1.4, 0.2], [1.6, 0.2], [1.6, 0.2], [1.5, 0.4], [1.5, 0.1], [1.4, 0.2], [1.5, 0.1], [1.2, 0.2], [1.3, 0.2], [1.5, 0.1], [1.3, 0.2], [1.5, 0.2], [1.3, 0.3], [1.3, 0.3], [1.3, 0.2], [1.6, 0.6], [1.9, 0.4], [1.4, 0.3], [1.6, 0.2], [1.4, 0.2], [1.5, 0.2], [1.4, 0.2], [4.7, 1.4], [4.5, 1.5], [4.9, 1.5], [4.0, 1.3], [4.6, 1.5], [4.5, 1.3], [4.7, 1.6], [3.3, 1.0], [4.6, 1.3], [3.9, 1.4], [3.5, 1.0], [4.2, 1.5], [4.0, 1.0], [4.7, 1.4], [3.6, 1.3], [4.4, 1.4], [4.5, 1.5], [4.1, 1.0], [4.5, 1.5], [3.9, 1.1], [4.8, 1.8], [4.0, 1.3], [4.9, 1.5], [4.7, 1.2], [4.3, 1.3], [4.4, 1.4], [4.8, 1.4], [5.0, 1.7], [4.5, 1.5], [3.5, 1.0], [3.8, 1.1], [3.7, 1.0], [3.9, 1.2], [5.1, 1.6], [4.5, 1.5], [4.5, 1.6], [4.7, 1.5], [4.4, 1.3], [4.1, 1.3], [4.0, 1.3], [4.4, 1.2], [4.6, 1.4], [4.0, 1.2], [3.3, 1.0], [4.2, 1.3], [4.2, 1.2], [4.2, 1.3], [4.3, 1.3], [3.0, 1.1], [4.1, 1.3], [6.0, 2.5], [5.1, 1.9], [5.9, 2.1], [5.6, 1.8], [5.8, 2.2], [6.6, 2.1], [4.5, 1.7], [6.3, 1.8], [5.8, 1.8], [6.1, 2.5], [5.1, 2.0], [5.3, 1.9], [5.5, 2.1], [5.0, 2.0], [5.1, 2.4], [5.3, 2.3], [5.5, 1.8], [6.7, 2.2], [6.9, 2.3], [5.0, 1.5], [5.7, 2.3], [4.9, 2.0], [6.7, 2.0], [4.9, 1.8], [5.7, 2.1], [6.0, 1.8], [4.8, 1.8], [4.9, 1.8], [5.6, 2.1], [5.8, 1.6], [6.1, 1.9], [6.4, 2.0], [5.6, 2.2], [5.1, 1.5], [5.6, 1.4], [6.1, 2.3], [5.6, 2.4], [5.5, 1.8], [4.8, 1.8], [5.4, 2.1], [5.6, 2.4], [5.1, 2.3], [5.1, 1.9], [5.9, 2.3], [5.7, 2.5], [5.2, 2.3], [5.0, 1.9], [5.2, 2.0], [5.4, 2.3], [5.1, 1.8]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cluster_result=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> zip(X,label_pred):</span><br><span class="line">    i[<span class="number">0</span>].append(i[<span class="number">1</span>])</span><br><span class="line">    cluster_result.append(i[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cluster_result</span><br></pre></td></tr></table></figure><pre><code>[[1.4, 0.2, 0], [1.4, 0.2, 0], [1.3, 0.2, 0], [1.5, 0.2, 0], [1.4, 0.2, 0], [1.7, 0.4, 0], [1.4, 0.3, 0], [1.5, 0.2, 0], [1.4, 0.2, 0], [1.5, 0.1, 0], [1.5, 0.2, 0], [1.6, 0.2, 0], [1.4, 0.1, 0], [1.1, 0.1, 0], [1.2, 0.2, 0], [1.5, 0.4, 0], [1.3, 0.4, 0], [1.4, 0.3, 0], [1.7, 0.3, 0], [1.5, 0.3, 0], [1.7, 0.2, 0], [1.5, 0.4, 0], [1.0, 0.2, 0], [1.7, 0.5, 0], [1.9, 0.2, 0], [1.6, 0.2, 0], [1.6, 0.4, 0], [1.5, 0.2, 0], [1.4, 0.2, 0], [1.6, 0.2, 0], [1.6, 0.2, 0], [1.5, 0.4, 0], [1.5, 0.1, 0], [1.4, 0.2, 0], [1.5, 0.1, 0], [1.2, 0.2, 0], [1.3, 0.2, 0], [1.5, 0.1, 0], [1.3, 0.2, 0], [1.5, 0.2, 0], [1.3, 0.3, 0], [1.3, 0.3, 0], [1.3, 0.2, 0], [1.6, 0.6, 0], [1.9, 0.4, 0], [1.4, 0.3, 0], [1.6, 0.2, 0], [1.4, 0.2, 0], [1.5, 0.2, 0], [1.4, 0.2, 0], [4.7, 1.4, 2], [4.5, 1.5, 2], [4.9, 1.5, 2], [4.0, 1.3, 2], [4.6, 1.5, 2], [4.5, 1.3, 2], [4.7, 1.6, 2], [3.3, 1.0, 2], [4.6, 1.3, 2], [3.9, 1.4, 2], [3.5, 1.0, 2], [4.2, 1.5, 2], [4.0, 1.0, 2], [4.7, 1.4, 2], [3.6, 1.3, 2], [4.4, 1.4, 2], [4.5, 1.5, 2], [4.1, 1.0, 2], [4.5, 1.5, 2], [3.9, 1.1, 2], [4.8, 1.8, 2], [4.0, 1.3, 2], [4.9, 1.5, 2], [4.7, 1.2, 2], [4.3, 1.3, 2], [4.4, 1.4, 2], [4.8, 1.4, 2], [5.0, 1.7, 1], [4.5, 1.5, 2], [3.5, 1.0, 2], [3.8, 1.1, 2], [3.7, 1.0, 2], [3.9, 1.2, 2], [5.1, 1.6, 1], [4.5, 1.5, 2], [4.5, 1.6, 2], [4.7, 1.5, 2], [4.4, 1.3, 2], [4.1, 1.3, 2], [4.0, 1.3, 2], [4.4, 1.2, 2], [4.6, 1.4, 2], [4.0, 1.2, 2], [3.3, 1.0, 2], [4.2, 1.3, 2], [4.2, 1.2, 2], [4.2, 1.3, 2], [4.3, 1.3, 2], [3.0, 1.1, 2], [4.1, 1.3, 2], [6.0, 2.5, 1], [5.1, 1.9, 1], [5.9, 2.1, 1], [5.6, 1.8, 1], [5.8, 2.2, 1], [6.6, 2.1, 1], [4.5, 1.7, 2], [6.3, 1.8, 1], [5.8, 1.8, 1], [6.1, 2.5, 1], [5.1, 2.0, 1], [5.3, 1.9, 1], [5.5, 2.1, 1], [5.0, 2.0, 1], [5.1, 2.4, 1], [5.3, 2.3, 1], [5.5, 1.8, 1], [6.7, 2.2, 1], [6.9, 2.3, 1], [5.0, 1.5, 2], [5.7, 2.3, 1], [4.9, 2.0, 1], [6.7, 2.0, 1], [4.9, 1.8, 1], [5.7, 2.1, 1], [6.0, 1.8, 1], [4.8, 1.8, 2], [4.9, 1.8, 1], [5.6, 2.1, 1], [5.8, 1.6, 1], [6.1, 1.9, 1], [6.4, 2.0, 1], [5.6, 2.2, 1], [5.1, 1.5, 1], [5.6, 1.4, 1], [6.1, 2.3, 1], [5.6, 2.4, 1], [5.5, 1.8, 1], [4.8, 1.8, 2], [5.4, 2.1, 1], [5.6, 2.4, 1], [5.1, 2.3, 1], [5.1, 1.9, 1], [5.9, 2.3, 1], [5.7, 2.5, 1], [5.2, 2.3, 1], [5.0, 1.9, 1], [5.2, 2.0, 1], [5.4, 2.3, 1], [5.1, 1.8, 1]]</code></pre><p>接下来将３类数据点分别导出到csv文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#分类整合</span></span><br><span class="line">label0=[]<span class="comment">#第０类</span></span><br><span class="line">label1=[]<span class="comment">#第１类</span></span><br><span class="line">label2=[]<span class="comment">##第2类</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> cluster_result:</span><br><span class="line">    <span class="keyword">if</span> i[<span class="number">2</span>]==<span class="number">0</span>:</span><br><span class="line">        label0.append(i)</span><br><span class="line">    <span class="keyword">elif</span> i[<span class="number">2</span>]==<span class="number">1</span>:</span><br><span class="line">        label1.append(i)</span><br><span class="line">    <span class="keyword">elif</span> i[<span class="number">2</span>]==<span class="number">2</span>:</span><br><span class="line">        label2.append(i)</span><br></pre></td></tr></table></figure><p>现在得到的是３个list，我们将先把list转换成array，再进行导出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#转化成array</span></span><br><span class="line">label0=np.array(label0)</span><br><span class="line">label1=np.array(label1)</span><br><span class="line">label2=np.array(label2)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#预先创建一个空的数据框</span></span><br><span class="line">label0_csv=pd.DataFrame()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将第０类样本信息进行填充到之前的看破那个数据框</span></span><br><span class="line">label0_csv[<span class="string">'feature1'</span>]=label0[:,<span class="number">0</span>]</span><br><span class="line">label0_csv[<span class="string">'feature2'</span>]=label0[:,<span class="number">1</span>]</span><br><span class="line">label0_csv[<span class="string">'kind'</span>]=label0[:,<span class="number">2</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#整合之后的样子</span></span><br><span class="line">label0_csv</span><br></pre></td></tr></table></figure><div><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>feature1</th><br>      <th>feature2</th><br>      <th>kind</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>1.4</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>1.4</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>1.3</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>3</th><br>      <td>1.5</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>4</th><br>      <td>1.4</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>5</th><br>      <td>1.7</td><br>      <td>0.4</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>6</th><br>      <td>1.4</td><br>      <td>0.3</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>7</th><br>      <td>1.5</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>8</th><br>      <td>1.4</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>9</th><br>      <td>1.5</td><br>      <td>0.1</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>10</th><br>      <td>1.5</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>11</th><br>      <td>1.6</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>12</th><br>      <td>1.4</td><br>      <td>0.1</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>13</th><br>      <td>1.1</td><br>      <td>0.1</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>14</th><br>      <td>1.2</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>15</th><br>      <td>1.5</td><br>      <td>0.4</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>16</th><br>      <td>1.3</td><br>      <td>0.4</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>17</th><br>      <td>1.4</td><br>      <td>0.3</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>18</th><br>      <td>1.7</td><br>      <td>0.3</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>19</th><br>      <td>1.5</td><br>      <td>0.3</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>20</th><br>      <td>1.7</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>21</th><br>      <td>1.5</td><br>      <td>0.4</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>22</th><br>      <td>1.0</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>23</th><br>      <td>1.7</td><br>      <td>0.5</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>24</th><br>      <td>1.9</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>25</th><br>      <td>1.6</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>26</th><br>      <td>1.6</td><br>      <td>0.4</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>27</th><br>      <td>1.5</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>28</th><br>      <td>1.4</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>29</th><br>      <td>1.6</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>30</th><br>      <td>1.6</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>31</th><br>      <td>1.5</td><br>      <td>0.4</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>32</th><br>      <td>1.5</td><br>      <td>0.1</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>33</th><br>      <td>1.4</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>34</th><br>      <td>1.5</td><br>      <td>0.1</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>35</th><br>      <td>1.2</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>36</th><br>      <td>1.3</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>37</th><br>      <td>1.5</td><br>      <td>0.1</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>38</th><br>      <td>1.3</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>39</th><br>      <td>1.5</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>40</th><br>      <td>1.3</td><br>      <td>0.3</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>41</th><br>      <td>1.3</td><br>      <td>0.3</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>42</th><br>      <td>1.3</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>43</th><br>      <td>1.6</td><br>      <td>0.6</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>44</th><br>      <td>1.9</td><br>      <td>0.4</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>45</th><br>      <td>1.4</td><br>      <td>0.3</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>46</th><br>      <td>1.6</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>47</th><br>      <td>1.4</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>48</th><br>      <td>1.5</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>49</th><br>      <td>1.4</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>  </tbody><br></table><br></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#保存第０类样本信息文件</span></span><br><span class="line">label0_csv.to_csv(<span class="string">r'/home/fantasy/Desktop/数学建模Python/聚类/鸢尾花聚类结果csv/label0.csv'</span>,index=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><p>至于其他两类的数据导出方法也一样，这里不再赘述。<br>其实，可以把这个导出功能封装成一个函数，传入保存路径和第几类就可以了，当然也可以直接来个for循环解决.</p><p>到现在，我们都做了些什么呢？来总结一下：</p><p>首先，我们导入了sklearn自带的鸢尾花数据集并选取了其中两个特征(feature)，拟用这两个特征做聚类.</p><p>接着，我们调用了sklearn的聚类方法做了聚类（聚成了３类），并将样本的特征与所属类别（int）整合在一个list里面，并由外围的list包裹住，然后再将这所有的list按照所属聚类数的不同而归类存储.</p><p>最后，将归类的数据先转化成数组形式，然后做成csv文件，导出到指定目录下.</p><p>有一点值得注意的是，在可视化的时候只能用二维数据，即两个特征，受维度限制.</p><hr><p>接下来我们再来看一个例子，同样是使用上面的数据，只不过这次采用dbscan算法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> DBSCAN</span><br><span class="line">db=DBSCAN(eps=<span class="number">1</span>,min_samples=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#这次使用全部特征进行聚类</span></span><br><span class="line">x=iris.data</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.shape</span><br></pre></td></tr></table></figure><pre><code>(150, 4)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.fit(x)<span class="comment">#训练数据集，构建模型</span></span><br></pre></td></tr></table></figure><pre><code>DBSCAN(algorithm=&apos;auto&apos;, eps=1, leaf_size=30, metric=&apos;euclidean&apos;,    min_samples=10, n_jobs=1, p=None)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">labels=db.labels_</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">labels</span><br></pre></td></tr></table></figure><pre><code>array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#噪声比率</span></span><br><span class="line">ratio=len(labels[labels[:]==<span class="number">-1</span>])*<span class="number">1.0</span>/len(labels)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> <span class="string">"噪声比率:"</span>,ratio</span><br></pre></td></tr></table></figure><pre><code>噪声比率: 0.0</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">n_clusters_=len(set(labels)) - (<span class="number">1</span> <span class="keyword">if</span> <span class="number">-1</span> <span class="keyword">in</span> labels <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'聚类总数为：'</span>,n_clusters_</span><br></pre></td></tr></table></figure><pre><code>聚类总数为： 2</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">print</span> <span class="string">'聚类效果评价指标：'</span>,metrics.silhouette_score(X,labels)<span class="comment">#【-1,1】,越接近１越好</span></span><br></pre></td></tr></table></figure><pre><code>聚类效果评价指标： 0.766723428068</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#总结</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_clusters_):</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Cluter'</span>,i+<span class="number">1</span>,<span class="string">':'</span></span><br><span class="line">    count=len(x[labels==i])</span><br><span class="line">    mean=np.mean(x[labels==i][:,<span class="number">1</span>])</span><br><span class="line">    std=np.std(x[labels==i][:,<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'计数：'</span>,count</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'平均值'</span>,mean</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'标准差'</span>,std</span><br></pre></td></tr></table></figure><pre><code>Cluter 1 :计数： 50平均值 3.418标准差 0.377194909828Cluter 2 :计数： 100平均值 2.872标准差 0.331083071147</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#可视化聚类结果，这里只选取前两个进行绘制,不太准确，只是拿来说明一下绘图做法</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_clusters_):</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'簇 '</span>, i, <span class="string">'的所有样本:'</span></span><br><span class="line">    one_cluster = x[labels == i]</span><br><span class="line">    <span class="keyword">print</span> one_cluster</span><br><span class="line">    plt.plot(one_cluster[:,<span class="number">0</span>],one_cluster[:,<span class="number">1</span>],<span class="string">'o'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>簇  0 的所有样本:[[ 5.1  3.5  1.4  0.2] [ 4.9  3.   1.4  0.2] [ 4.7  3.2  1.3  0.2] [ 4.6  3.1  1.5  0.2] [ 5.   3.6  1.4  0.2] [ 5.4  3.9  1.7  0.4] [ 4.6  3.4  1.4  0.3] [ 5.   3.4  1.5  0.2] [ 4.4  2.9  1.4  0.2] [ 4.9  3.1  1.5  0.1] [ 5.4  3.7  1.5  0.2] [ 4.8  3.4  1.6  0.2] [ 4.8  3.   1.4  0.1] [ 4.3  3.   1.1  0.1] [ 5.8  4.   1.2  0.2] [ 5.7  4.4  1.5  0.4] [ 5.4  3.9  1.3  0.4] [ 5.1  3.5  1.4  0.3] [ 5.7  3.8  1.7  0.3] [ 5.1  3.8  1.5  0.3] [ 5.4  3.4  1.7  0.2] [ 5.1  3.7  1.5  0.4] [ 4.6  3.6  1.   0.2] [ 5.1  3.3  1.7  0.5] [ 4.8  3.4  1.9  0.2] [ 5.   3.   1.6  0.2] [ 5.   3.4  1.6  0.4] [ 5.2  3.5  1.5  0.2] [ 5.2  3.4  1.4  0.2] [ 4.7  3.2  1.6  0.2] [ 4.8  3.1  1.6  0.2] [ 5.4  3.4  1.5  0.4] [ 5.2  4.1  1.5  0.1] [ 5.5  4.2  1.4  0.2] [ 4.9  3.1  1.5  0.1] [ 5.   3.2  1.2  0.2] [ 5.5  3.5  1.3  0.2] [ 4.9  3.1  1.5  0.1] [ 4.4  3.   1.3  0.2] [ 5.1  3.4  1.5  0.2] [ 5.   3.5  1.3  0.3] [ 4.5  2.3  1.3  0.3] [ 4.4  3.2  1.3  0.2] [ 5.   3.5  1.6  0.6] [ 5.1  3.8  1.9  0.4] [ 4.8  3.   1.4  0.3] [ 5.1  3.8  1.6  0.2] [ 4.6  3.2  1.4  0.2] [ 5.3  3.7  1.5  0.2] [ 5.   3.3  1.4  0.2]]簇  1 的所有样本:[[ 7.   3.2  4.7  1.4] [ 6.4  3.2  4.5  1.5] [ 6.9  3.1  4.9  1.5] [ 5.5  2.3  4.   1.3] [ 6.5  2.8  4.6  1.5] [ 5.7  2.8  4.5  1.3] [ 6.3  3.3  4.7  1.6] [ 4.9  2.4  3.3  1. ] [ 6.6  2.9  4.6  1.3] [ 5.2  2.7  3.9  1.4] [ 5.   2.   3.5  1. ] [ 5.9  3.   4.2  1.5] [ 6.   2.2  4.   1. ] [ 6.1  2.9  4.7  1.4] [ 5.6  2.9  3.6  1.3] [ 6.7  3.1  4.4  1.4] [ 5.6  3.   4.5  1.5] [ 5.8  2.7  4.1  1. ] [ 6.2  2.2  4.5  1.5] [ 5.6  2.5  3.9  1.1] [ 5.9  3.2  4.8  1.8] [ 6.1  2.8  4.   1.3] [ 6.3  2.5  4.9  1.5] [ 6.1  2.8  4.7  1.2] [ 6.4  2.9  4.3  1.3] [ 6.6  3.   4.4  1.4] [ 6.8  2.8  4.8  1.4] [ 6.7  3.   5.   1.7] [ 6.   2.9  4.5  1.5] [ 5.7  2.6  3.5  1. ] [ 5.5  2.4  3.8  1.1] [ 5.5  2.4  3.7  1. ] [ 5.8  2.7  3.9  1.2] [ 6.   2.7  5.1  1.6] [ 5.4  3.   4.5  1.5] [ 6.   3.4  4.5  1.6] [ 6.7  3.1  4.7  1.5] [ 6.3  2.3  4.4  1.3] [ 5.6  3.   4.1  1.3] [ 5.5  2.5  4.   1.3] [ 5.5  2.6  4.4  1.2] [ 6.1  3.   4.6  1.4] [ 5.8  2.6  4.   1.2] [ 5.   2.3  3.3  1. ] [ 5.6  2.7  4.2  1.3] [ 5.7  3.   4.2  1.2] [ 5.7  2.9  4.2  1.3] [ 6.2  2.9  4.3  1.3] [ 5.1  2.5  3.   1.1] [ 5.7  2.8  4.1  1.3] [ 6.3  3.3  6.   2.5] [ 5.8  2.7  5.1  1.9] [ 7.1  3.   5.9  2.1] [ 6.3  2.9  5.6  1.8] [ 6.5  3.   5.8  2.2] [ 7.6  3.   6.6  2.1] [ 4.9  2.5  4.5  1.7] [ 7.3  2.9  6.3  1.8] [ 6.7  2.5  5.8  1.8] [ 7.2  3.6  6.1  2.5] [ 6.5  3.2  5.1  2. ] [ 6.4  2.7  5.3  1.9] [ 6.8  3.   5.5  2.1] [ 5.7  2.5  5.   2. ] [ 5.8  2.8  5.1  2.4] [ 6.4  3.2  5.3  2.3] [ 6.5  3.   5.5  1.8] [ 7.7  3.8  6.7  2.2] [ 7.7  2.6  6.9  2.3] [ 6.   2.2  5.   1.5] [ 6.9  3.2  5.7  2.3] [ 5.6  2.8  4.9  2. ] [ 7.7  2.8  6.7  2. ] [ 6.3  2.7  4.9  1.8] [ 6.7  3.3  5.7  2.1] [ 7.2  3.2  6.   1.8] [ 6.2  2.8  4.8  1.8] [ 6.1  3.   4.9  1.8] [ 6.4  2.8  5.6  2.1] [ 7.2  3.   5.8  1.6] [ 7.4  2.8  6.1  1.9] [ 7.9  3.8  6.4  2. ] [ 6.4  2.8  5.6  2.2] [ 6.3  2.8  5.1  1.5] [ 6.1  2.6  5.6  1.4] [ 7.7  3.   6.1  2.3] [ 6.3  3.4  5.6  2.4] [ 6.4  3.1  5.5  1.8] [ 6.   3.   4.8  1.8] [ 6.9  3.1  5.4  2.1] [ 6.7  3.1  5.6  2.4] [ 6.9  3.1  5.1  2.3] [ 5.8  2.7  5.1  1.9] [ 6.8  3.2  5.9  2.3] [ 6.7  3.3  5.7  2.5] [ 6.7  3.   5.2  2.3] [ 6.3  2.5  5.   1.9] [ 6.5  3.   5.2  2. ] [ 6.2  3.4  5.4  2.3] [ 5.9  3.   5.1  1.8]]</code></pre><p><img src="output_45_1.png" alt="png"></p><p>参考：<br><a href="https://blog.csdn.net/luanpeng825485697/article/details/79443512" target="_blank" rel="noopener">https://blog.csdn.net/luanpeng825485697/article/details/79443512</a><br><a href="https://blog.csdn.net/linzch3/article/details/76038172" target="_blank" rel="noopener">https://blog.csdn.net/linzch3/article/details/76038172</a><br><a href="https://blog.csdn.net/u010159842/article/details/78624135" target="_blank" rel="noopener">https://blog.csdn.net/u010159842/article/details/78624135</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;Kmean聚类&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以下使用的是sklearn自带的鸢尾花数据集&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;l
      
    
    </summary>
    
      <category term="Data Analysis&amp;Mining with Python" scheme="http://yoursite.com/categories/Data-Analysis-Mining-with-Python/"/>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>黎曼函数的连续性</title>
    <link href="http://yoursite.com/2018/12/29/liman/"/>
    <id>http://yoursite.com/2018/12/29/liman/</id>
    <published>2018-12-29T13:45:57.332Z</published>
    <updated>2018-09-30T09:56:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="黎曼函数的连续性"><a href="#黎曼函数的连续性" class="headerlink" title="黎曼函数的连续性"></a>黎曼函数的连续性</h2><p>黎曼函数，即<br><br>$$R(x)=\begin{cases}<br>0&amp;x为无理数\<br>\frac1{q}&amp;x=\frac{p}{q},p,q为正整数<br>\end{cases}$$<br>黎曼函数周期为１，所以只研究区间$[0,1]$的性质即可<br><br>性质：<strong>黎曼函数在区间$[0,1]$内的极限处处为零</strong><br><br>换句话说，<strong>黎曼函数在$[0,1]$上的无理点处处连续，在有理点间断</strong>.<br><br>下面我们来证明之~<br><br>证：对于有理数来说，可以写成$\frac{p}{q}$的既约真分数形式<br><br>$x=0$可以写成$x=\frac01$，即$R(0)=1$.　<br><br>所以，在$[0,1]$上，分母为1的有理点(既约)只有两个：$\frac01$和$\frac11$　<br><br>分母为2的有理点只有一个：$\frac12$　<br><br>分母为3的有理点只有两个：$\frac13$和$\frac23$　<br><br>分母为4的有理点只有两个：$\frac１４$和$\frac34$ <br><br>分母为5的有理点只有四个：$\frac15$,$\frac25$,$\frac35$和$\frac45$ <br><br>…<br><br>总之，对任意自然数$k$，分母不超过$k$的有理点个数是有限的<br><br>设$x_0$是$[0,1]$内任意一点，对任意给定的$\epsilon&gt;0$，设$k=[\frac{1}{\epsilon}]$，因为分母不超过$k$的有理点个数有限，设它们为$r_1,r_2,…,r_n$.<br><br>令$\delta=min{[r_i-x_0]}$，其中$1&lt;=i&lt;=n,r_i!=x_0$ <br><br>从而$\delta&gt;0$　<br><br>当$0&lt;|x-x_0|&lt;\delta$时，若$x$是无理数，则$R(x)=0$，若$x$为有理数，则其分母一定大于$k=[\frac{1}{\epsilon}]$(因为…)，于是$R(x)&lt;=\frac{1}{[\frac{1}{\epsilon}]+1}&lt;\epsilon$,因此成立$|R(x)-0|&lt;\epsilon$.<br><br>此即说明$R(x)$在$x_0$的极限为0（$x_0=0$时是指右极限，$x_0=1$时是指左极限）.根据$R(x)$的周期性，对一切$x_0属于(-\infty,+\infty)$成立$lim_{x{\rightarrow}x_0}R(x)=0$,证毕.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;黎曼函数的连续性&quot;&gt;&lt;a href=&quot;#黎曼函数的连续性&quot; class=&quot;headerlink&quot; title=&quot;黎曼函数的连续性&quot;&gt;&lt;/a&gt;黎曼函数的连续性&lt;/h2&gt;&lt;p&gt;黎曼函数，即&lt;br&gt;&lt;br&gt;$$R(x)=\begin{cases}&lt;br&gt;0&amp;amp;x为
      
    
    </summary>
    
      <category term="Mathematic Analysis" scheme="http://yoursite.com/categories/Mathematic-Analysis/"/>
    
    
  </entry>
  
  <entry>
    <title>时序复习</title>
    <link href="http://yoursite.com/2018/12/29/timeseriesanalysis/"/>
    <id>http://yoursite.com/2018/12/29/timeseriesanalysis/</id>
    <published>2018-12-29T13:45:57.332Z</published>
    <updated>2018-12-09T07:17:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>若序列平稳，则往下进行，否则进行差分处理使序列变平稳 <br></p><h2 id="时间序列的预处理"><a href="#时间序列的预处理" class="headerlink" title="时间序列的预处理"></a>时间序列的预处理</h2><h4 id="特征统计量"><a href="#特征统计量" class="headerlink" title="特征统计量"></a>特征统计量</h4><p><a href="http://classroom.dufe.edu.cn/spsk/c102/wlkj/CourseContents/Chapter10/10_03_01.htm" target="_blank" rel="noopener">http://classroom.dufe.edu.cn/spsk/c102/wlkj/CourseContents/Chapter10/10_03_01.htm</a></p><h4 id="宽平稳时间序列-只要求二阶平稳即可"><a href="#宽平稳时间序列-只要求二阶平稳即可" class="headerlink" title="宽平稳时间序列(只要求二阶平稳即可)"></a>宽平稳时间序列(只要求二阶平稳即可)</h4><p>若${X(t)}$满足如下三个条件，则称${X(t)}$为<code>宽平稳时间序列</code>（弱平稳/二阶平稳）<br></p><blockquote><p>(1)任取$t\in T$,有$E{X_t}^2&lt;\infty$ <br><br>(2)任取$t\in T$,有$EX_t=\mu$,$\mu$为常数<br><br>(3)任取$t,s,k \in T$,且$k+s-t \in T$,有$\gamma(t,s)=\gamma(k,k+s-t)$ <br></p></blockquote><h4 id="平稳时间序列的统计性质"><a href="#平稳时间序列的统计性质" class="headerlink" title="平稳时间序列的统计性质"></a>平稳时间序列的统计性质</h4><blockquote><p>(1)$EX_t=\mu$,任意$t \in T$ <br><br>(2)$\gamma(t,s)=\gamma(k,k+s-t)$ <br><br>$\gamma(s-t)=\gamma(t,s)$,任意$t,s \in T $ <br><br>(3)$DX_t=\gamma(t,t)=\gamma(0)$ <br></p></blockquote><p>延迟$k$阶自协方差函数$\gamma(k)=\gamma(t,t+k)$ <br><br>延迟$k$阶自相关系数$\rho_k=\frac{\gamma(t,t+k)}{\sqrt{DX_tDX_{t+k}.}.}=\frac{\gamma(k)}{\gamma(0)}$，3个性质为“规范性”,“对称性”以及“非负定性”<br></p><blockquote><p>一个平稳时间序列唯一确定其自相关系数，反之不成立 <br></p></blockquote><h4 id="平稳性检验"><a href="#平稳性检验" class="headerlink" title="平稳性检验"></a>平稳性检验</h4><p>单位根检验、时序图检验、自相关图检验 <br></p><h4 id="纯随机性检验"><a href="#纯随机性检验" class="headerlink" title="纯随机性检验"></a>纯随机性检验</h4><blockquote><p>纯随机序列的定义：<br><br>(1)任取$t \in T$,有$EX_t=\mu$ <br><br>(2)任取$t,s \in T$,有<br><br>$\gamma(t,s)=\sigma^2 ,t=s$ <br><br>$\gamma(t,s)=0,t!=s$</p></blockquote><p>白噪声序列(white noise series)是平稳时间序列，但同时也是纯随机序列，所以没有研究价值<br></p><h2 id="平稳时间序列分析-key"><a href="#平稳时间序列分析-key" class="headerlink" title="平稳时间序列分析(key!)"></a>平稳时间序列分析(key!)</h2><p>$p$阶差分与$k$步差分 <br><br>延迟算子表示差分运算 <br><br>看课本P41 <br><br>线性差分方程的解法<a href="https://fuhanshi.github.io/2018/09/30/diff/" target="_blank" rel="noopener">https://fuhanshi.github.io/2018/09/30/diff/</a></p><h4 id="AR模型"><a href="#AR模型" class="headerlink" title="AR模型"></a>AR模型</h4><p>$AR(p):x_t=\psi_0+\psi_1x_{t-1}+\psi_2x_{t-2}+…+\psi_px_{t-p}+\epsilon_t$ <br></p><p><strong>平稳性判别</strong>： <br><br>（1）特征根判别 <br></p><blockquote><p>$AR(p)$模型平稳的充要条件是其$p$个特征根都在单位圆内 <br></p></blockquote><p>（2）平稳域判别 <br></p><blockquote><p>$AR(1)$模型平稳的充要条件是$|\psi_1|&lt;1$ <br><br>$AR(2)$模型平稳的充要条件是${\psi_1,\psi_2| |\psi_2|&lt;1,且\psi_2+\psi_1&lt;1,\psi_2-\psi_1&lt;1}$ <br></p></blockquote><p><strong>统计性质</strong>： <br><br>(1)均值 <br><br>(i)普通$AR(p)模型$：$\mu=\frac{\psi_0}{1-\psi_1-\psi_2-…-\psi_p}$ <br><br>(ii)中心化的$AR(p)模型$：$\mu=0$ <br></p><p>(2)方差： <br><br>$AR(1):Var(X_t)=\frac{\sigma_{\epsilon}^2}{1-\psi_1^2}$ <br><br>$AR(2):\frac{1-\psi_2}{(1+\psi_2)(1-\psi_1-\psi_2)(1+\psi_1-\psi_2)}\sigma_{\epsilon}^2$</p><p>(3)自协方差函数：<br><br>$AR(1):\gamma_k=\psi_1^k \frac{\sigma_{\epsilon}^2}{1-\psi_1^2},任意k&gt;=1$ <br><br>$AR(2):$ <br><br>$\gamma_0=\frac{1-\psi_2}{(1+\psi_2)(1-\psi_1-\psi_2)(1+\psi_1-\psi_2)}\sigma_{\epsilon}^2$ <br><br>$\gamma_1=\frac{\psi_1\gamma_0}{1-\psi_2}$ <br><br>$\gamma_k=\psi_1\gamma_{k-1}+\psi_2\gamma_{k-2},k&gt;=2$ <br></p><p>(4)自相关系数: <br><br>$AR(1):\rho_k=\psi_1^k,k&gt;=0$ <br><br>$AR(2):$ <br><br>$\begin{eqnarray}<br>\rho_k=<br>\begin{cases}<br>1 &amp;k=0\<br>\frac{\psi_1}{1-\psi_2} &amp;k=1\<br>\psi_1\rho_{k-1}+\psi_2\rho_{k-2} &amp;k&gt;=2<br>\end{cases}<br>\end{eqnarray}<br>$ <br></p><p>$AR(p)$模型的自相关系数的两个性质：(1)拖尾性;(2)呈指数衰减 <br></p><p>(2)偏自相关系数: <br><br>$AR(1):$ <br><br>$\begin{eqnarray}<br>\psi_{kk}=<br>\begin{cases}<br>\psi_1 &amp;k=1\<br>0 &amp;k&gt;=1<br>\end{cases}<br>\end{eqnarray}<br>$ <br></p><p>$AR(2):$ <br><br>$\begin{eqnarray}<br>\psi_{kk}=<br>\begin{cases}<br>\frac{\psi_1}{1-\psi_2} &amp;k=1\<br>\psi_2 &amp;k=2\<br>0 &amp;k&gt;=3<br>\end{cases}<br>\end{eqnarray}<br>$ <br></p><h4 id="MA模型"><a href="#MA模型" class="headerlink" title="MA模型"></a>MA模型</h4><p>$MA(q):x_t=\mu+\epsilon_t-\theta_1\epsilon_{t-1}-\theta_2\epsilon_{t-2}-…-\theta_q\epsilon_{t-q}$ ($\mu=0$时为中心化模型)<br><br><strong>统计性质：</strong> <br><br>(1)均值： <br><br>普通的：$EX_t=\mu$ <br><br>中心化$MA(q)$模型的均值为0 <br></p><p>(2)方差： <br><br>$Var(X_t)=(1+\theta_1^2+\theta_2^2+…+\theta_q^2)\sigma_{\epsilon}^2$ <br></p><p>(3)自协方差函数： <br><br>$\begin{eqnarray}<br>\gamma_k=<br>\begin{cases}<br>(1+\theta_1^2+\theta_2^2+…+\theta_q^2)\sigma_{\epsilon}^2 &amp;k=0\<br>\frac{(-\theta_k+\sum_{i=1}^{q-k}\theta_i\theta_{k+i})}{1+\theta_1^2+…+\theta_q^2}\sigma_{\epsilon}^2 &amp;1&lt;=k&lt;=q\<br>0 &amp;k&gt;q<br>\end{cases}<br>\end{eqnarray}<br>$ <br></p><p>(4)自相关系数： <br><br>$MA(1):$ <br><br>$\begin{eqnarray}<br>\rho_k=<br>\begin{cases}<br>1 &amp;k=0\<br>\frac{-\theta_1}{1+\theta_1^2} &amp;k=1\<br>0 &amp;k&gt;=2<br>\end{cases}<br>\end{eqnarray}<br>$ <br></p><p>$MA(2):$ <br><br>$\begin{eqnarray}<br>\rho_k=<br>\begin{cases}<br>1 &amp;k=0\<br>\frac{-\theta_1+\theta_1\theta_2}{1+\theta_1^2+\theta_2^2} &amp;k=1\<br>\frac{-\theta_2}{1+\theta_1^2+\theta_2^2} &amp;k=2\<br>0 &amp;k&gt;=3<br>\end{cases}<br>\end{eqnarray}<br>$</p><p><strong>MA模型的可逆性判别：</strong> <br><br>$MA(1):{\theta_1|-1&lt;\theta_1&lt;1}$ <br><br>$MA(2):{\theta_1,\theta_2| |\theta_2|&lt;1,且\theta_2+\theta_1&lt;1,\theta_2-\theta_1&lt;1}$ <br></p><p><strong>偏自相关系数：</strong> <br><br>$MA(1):$ <br><br>$\psi_{11}=\rho_1=\frac{-\theta_1}{1+\theta_1^2}$ <br><br>$\psi_{22}=\frac{-\theta_1^2}{1+\theta_1^2+\theta_2^4}$ <br><br>$\psi_{33}=\frac{-\theta_1^3}{1+\theta_1^2+\theta_1^4+\theta_1^6}$ <br></p><p>通式为$\psi_{kk}=\frac{-\theta_1^k}{\sum_{j=0}^{k}}\theta_1^{2j},k&gt;=1$ <br></p><h4 id="ARMA模型"><a href="#ARMA模型" class="headerlink" title="ARMA模型"></a>ARMA模型</h4><p>$ARMA(p,q):x_t=\psi_0+\psi_1x_{t-1}+\psi_2x_{t-2}+…+\psi_px_{t-p}+\epsilon_t-\theta_1\epsilon_{t-1}-\theta_2\epsilon_{t-2}-…-\theta_q\epsilon_{t-q}$ <br></p><p>平稳性：$AR(p)$部分可逆即可逆 <br><br>可逆性：$MA(q)$部分可逆即可逆 <br><br><strong>统计性质：P64,65</strong> <br></p><h4 id="模型识别规则"><a href="#模型识别规则" class="headerlink" title="模型识别规则"></a>模型识别规则</h4><p>AR(p)：自相关系数$\rho_k$拖尾，偏自相关系数$\psi_{kk}$是p阶截尾 <br><br>MA(q)：自相关系数$\rho_k$是q阶截尾，偏自相关系数$\psi_{kk}$拖尾 <br><br>ARMA(p,q)：自相关系数$\rho_k$拖尾，偏自相关系数$\psi_{kk}$也拖尾 <br></p><h4 id="平稳序列建模"><a href="#平稳序列建模" class="headerlink" title="平稳序列建模"></a>平稳序列建模</h4><h4 id="序列预测"><a href="#序列预测" class="headerlink" title="序列预测"></a>序列预测</h4><h2 id="非平稳序列的随机性分析"><a href="#非平稳序列的随机性分析" class="headerlink" title="非平稳序列的随机性分析"></a>非平稳序列的随机性分析</h2><h2 id="非平稳序列的确定性分析"><a href="#非平稳序列的确定性分析" class="headerlink" title="非平稳序列的确定性分析"></a>非平稳序列的确定性分析</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;若序列平稳，则往下进行，否则进行差分处理使序列变平稳 &lt;br&gt;&lt;/p&gt;
&lt;h2 id=&quot;时间序列的预处理&quot;&gt;&lt;a href=&quot;#时间序列的预处理&quot; class=&quot;headerlink&quot; title=&quot;时间序列的预处理&quot;&gt;&lt;/a&gt;时间序列的预处理&lt;/h2&gt;&lt;h4 id=&quot;特征
      
    
    </summary>
    
      <category term="R" scheme="http://yoursite.com/categories/R/"/>
    
    
  </entry>
  
  <entry>
    <title>神奇的多元统计</title>
    <link href="http://yoursite.com/2018/12/18/magical%20multivariate%20statistical%20analysis/"/>
    <id>http://yoursite.com/2018/12/18/magical multivariate statistical analysis/</id>
    <published>2018-12-17T16:18:17.000Z</published>
    <updated>2018-12-30T04:25:40.562Z</updated>
    
    <content type="html"><![CDATA[<p><strong>１．<code>马氏距离</code>与<code>欧氏距离</code>各自的<code>优缺点</code>(两者之间的<code>区别</code>与<code>联系</code>)?</strong> <br><br>1.马氏距离： <br></p><blockquote><p>(1)公式：$d_m^2(X,Y)=(X-Y)’{\Sigma}^{-1}(X-Y)$ <br><br>(2)缺点：计算不便，须事先知道$\Sigma$，并计算$\Sigma^{-1}$，计算过程会产生误差，只有数据量较大时误差才较小<br><br>(3)优点：既不受量纲影响，也不受相关性影响<br></p></blockquote><p>2.欧氏距离：　<br></p><blockquote><p>(1)公式：$d_{X,Y}^2=(X-Y)^{T}(X-Y)$ <br><br>(2)缺点：受各指标间量纲的差异以及数量级不同的影响，受不同指标之间相关性的影响<br><br>(3)优点：在计算方法和理解上较为简单<br></p></blockquote><p><strong>2．$k-means$的思想、步骤？</strong><br><br>1.思想：<br></p><blockquote><p>把每个样品聚集到其最近形心(均值)类中.<br></p></blockquote><p>２.步骤：<br></p><blockquote><p>(1)确定要分的类别数目$k$ <br><br>(2)确定$k$个类别的初始聚类中心<br><br>(3)根据确定的$k$个初始聚类中心，依次计算每个样本到$k$个聚类中心的距离，并根据距离最近的原则将所有的样本分到事先确定的$k$个类别中<br><br>(4)根据所分成的$k$个类别，计算出各类别中每个变量的均值，并以均值点作为新的$k$个类别中心。根据新的中心位置，重新计算每个样本到新中心的距离，并重新进行分类<br><br>(5)重复第4步，直到满足终止聚类条件为止<br></p></blockquote><p><strong>3.系统聚类的思想方法和步骤？</strong>　<br></p><p>1.思想： <br></p><blockquote><p>通过度量样本之间的相似性来将样本相似度最高的样本聚为一类，重复进行下去，逐渐减小类别数目，直到聚类数为1<br></p></blockquote><p>2.步骤： <br></p><blockquote><p>首先将$n$个待聚类样本点分为$n$类，之后计算每个样本点（即每一个类）与所有样本点（其他所有类）之间的相似性（一般是距离），将相似性最强（即距离最小）的两个类合并为一个新的类，然后重新度量相似性，这样子依次下去，直到满足某个条件为止（无条件的话就默认是类别数变为$1$），系统聚类结束。<br></p></blockquote><p><strong>4．$Fisher$判别法的思想、方法？</strong><br><br>1.思想：<br></p><blockquote><p>投影，将$k$组$p$维数据投影到某一个方向，使得组与组之间的投影尽可能地分开，然后再选择合适的判别规则，将新的样品进行分类判别。<br></p></blockquote><p>2.方法：<br></p><blockquote><p>从$k$个总体中抽取具有$p$个指标的样本观测数据，借助方差分析的思想，构造一个线性判别函数，系数确定的原则是使得组内平方和尽可能小，组间平方和尽可能大，对于一个新的样本，只需将它的$p$个指标代入判别函数，再根据一定的判别规则即可判别新的样品属于哪个总体<br></p></blockquote><p><strong>5．<code>聚类分析</code>与<code>判别分析</code>的区别与联系？</strong><br><br>  1、基本思想不同</p><blockquote><p><code>聚类分析</code>是根据研究对象特征对研究对象进行分类的一种多元分析技术,在未知各样本类别的情况下，把性质相近的个体归为一类, 使得同一类中的个体都具有高度的同质性, 不同类之间的个体具有高度的异质性；<br><br><code>判别分析</code>是对已知分类的数据建立由数值指标构成的分类规则即判别函数, 然后把这样的规则应用到未知分类的样本去分类<br></p></blockquote><p>  2、研究目的不同</p><blockquote><p>虽然都是研究分类的，但在进行<code>聚类分析</code>前，<code>对总体到底有几种类型不知道</code>（研究分几类较为合适需从计算中加以调整）。<code>判别分析</code>则是<code>在总体类型划分已知</code>的前提下，来判断当前新样本属于哪个总体的。</p></blockquote><p>  3.所具有的方法不同<br></p><blockquote><p><code>聚类分析</code>分两种：$Q$型聚类（对样品的聚类），$R$型聚类（对变量的聚类）。聚类分析需要注意的是，一般小样本数据可以用系统聚类法，大样本数据一般用快速聚类法（$K$均值聚类法），当研究因素既有分类变量又有计量变量，可以用两步聚类。<br><br><code>判别分析</code>有$Fisher$判别，$Bayes$判别和逐步判别。一般用$Fisher$判别即可，要考虑概率及误判损失最小的用$Bayes$判别，但变量较多时，一般先进行逐步判别筛选出有统计意义的变量，再结合实际情况选择用哪种判别方法.</p></blockquote><p>4.对数据要求不同<br></p><blockquote><p><code>聚类分析</code>的方法基本上与分布理论和限制性检验无关，一般不从样本推断总体<br><br><code>判别分析</code>的一个基本假设是每一个类别都应取自一个多元正态总体的样本，而且所有正态总体的协方差矩阵或相关矩阵都假定是相同的，否则可能要采用非线性判别函数<br></p></blockquote><p><strong>6.$PCA$的思想与方法？</strong><br></p><blockquote><p><code>思想：</code>PCA在损失很少信息的前提下，通过线性变换将原始数据变换为一组各维度线性无关的综合变量（称为主成分），并且这些综合变量都是原始变量的线性组合<br><br><code>目的：</code>为了节省计算机在进行计算时所占用的资源，在减少需要分析的指标的同时，尽量减少原指标包含信息的损失，并用随机变量的方差来代表保留信息的比重，以达到对所收集数据进行全面分析的目的。<br><br><code>方法：</code>　　　　　　　　　　　　　<br><br>1.根据研究问题选取原始变量<br><br>2.根据初始变量的特性判断由协方差阵求主成分还是由相关阵求主成分<br><br>3.求协方差阵或相关阵的特征根和特征向量，并对特征向量进行规范正交化<br><br>4.判断是否存在明显的多重共线性，若存在，回到第一步<br><br>5.得到主成分的表达式并确定主成分的个数，选取主成分<br><br>6.结合主成分对研究问题进行分析并深入研究<br></p></blockquote><p><strong>7．$FA$的思想与方法？</strong><br><br>1.思想：<br></p><blockquote><p>根据相关性大小把原始变量分组，使得同组内的变量之间相关性较高，而不同组的变量间的相关性则较低。每组变量代表一个基本结构并用一个不可观测的综合变量来表示，这个基本结构就称为公共因子<br></p></blockquote><p>2.方法：　<br></p><p>1.根据研究问题选取原始变量<br><br>2.标准化原始变量并求其相关阵，分析变量之间的相关性<br><br>3.求解初始公共因子以及因子载荷矩阵<br><br>4.因子旋转<br><br>5.计算因子得分<br><br>6.根据因子得分值进行进一步的分析<br></p><p><strong>8．$PCA$与$FA$的区别与联系？</strong><br></p><p>1．联系</p><blockquote><p>(1)$PCA$是以方差度量保留的主成分的，使方差尽量大，$FA$中因子与原变量间有很高的相关性，且有很强的解释能力<br><br>(2)形式不同.$PCA$是$Y=UX$，而$FA$为$X=AF+e$ <br><br>(3)$FA$除主成分法外还有其他方法，比如极大似然法 <br><br>(4)$FA$最后要进行因子旋转 <br><br>2．区别<br>(1)都是一种多维随机变量降维的方法<br><br>(2)$FA$求解初始因子时有一种方法是$PCA$ <br></p></blockquote><p><strong>9.对应分析的基本思想和步骤</strong> <br><br>1.基本思想：　<br></p><blockquote><p><del>首先分别在每组变量中找出第一对典型相关变量，使其具有最大相关性，然后找出第二对典型变量，使其分别与第一对典型相关变量不相关，第二对本身具有次大的相关性。如此下去，直到进行到R步，两组变量的相关关系被提取完为止，可以得到R组典型相关变量</del>。<br></p></blockquote><p>2.方法步骤：<br></p><blockquote><p>(1)数据标准化：$\frac{P_{ij}}{P_i\sqrt{p_{.j}}}$以及$\frac{P_{ij}}{P_j\sqrt{p_{i.}}}$ <br><br>(2)按照行和列进行主成分分析<br><br>(3)对应分析结果中，协方差矩阵$ZZ’$和$Z’Z$有相同的特征值，特征向量之间有一定的关系，从而两者的特征向量可互化　<br><br>(4)一般选取前两个主成分作图分析<br></p></blockquote><p><strong>10.典型相关分析的思想和方法步骤？</strong></p><p>1.思想：<br></p><blockquote><p>对于两随机向量$X=(x_1,x_2,…,x_p)^T,Y=(y_1,y_2,…,y_q)^T$，取定$X$中的信息，$U_1=a_1x_1+…+a_px_p,V_1=b_1y_1+…+b_qy_q$，在一定条件下，寻找适当的系数$a$与$b$，使得$U_1$与$V_1$之间的相关系数的绝对值达到最大，这里的$U_1$与$V_1$是$X$与$Y$的第一对典型相关变量，可用相同的方法寻找第$i$对，$i=1,2,…,n$ <br></p></blockquote><p>2.方法步骤：　<br></p><blockquote><p>对于$X$与$Y$的一对线性组合$U_1=a’X,V_1=b’Y$，利用拉格朗日乘子法求解使得$U_1$和$V_1$之间相关系数最大时的$a’$与$b’$这两个向量，即<br><br>$$max\rho_{U_1,V_1}=a’\Sigma_{12}b$$<br>$st.$<br>$$a’\Sigma_{11}a=1$$<br>$$b’\Sigma_{22}b=1$$ <br><br>即：　<br><br>$$L(a,b,\lambda,\mu)=a’\Sigma_{12}b-\frac{\lambda}{2}(a’\Sigma_{11}a-1)-\frac{\mu}{2}(b’\Sigma_{22}b-1)$$ <br><br>求解即得$a’$与$b’$的值　<br><br>此为简化版，适于答题，详细推导请戳这里！<br><a href="https://fuhanshi.github.io/2018/11/08/%E5%85%B8%E5%9E%8B%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90/#more" target="_blank" rel="noopener">https://fuhanshi.github.io/2018/11/08/%E5%85%B8%E5%9E%8B%E7%9B%B8%E5%85%B3%E5%88%86%E6%9E%90/#more</a></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;１．&lt;code&gt;马氏距离&lt;/code&gt;与&lt;code&gt;欧氏距离&lt;/code&gt;各自的&lt;code&gt;优缺点&lt;/code&gt;(两者之间的&lt;code&gt;区别&lt;/code&gt;与&lt;code&gt;联系&lt;/code&gt;)?&lt;/strong&gt; &lt;br&gt;&lt;br&gt;1.马氏距离： &lt;br&gt;&lt;/p&gt;
      
    
    </summary>
    
      <category term="R" scheme="http://yoursite.com/categories/R/"/>
    
    
  </entry>
  
  <entry>
    <title>对应分析</title>
    <link href="http://yoursite.com/2018/12/09/end/"/>
    <id>http://yoursite.com/2018/12/09/end/</id>
    <published>2018-12-09T07:01:26.000Z</published>
    <updated>2018-12-09T07:11:29.000Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">#第一题</span><br><span class="line">file1=read.csv(&apos;C://Users//Administrator//Desktop//data//1.csv&apos;,head=T);</span><br><span class="line"></span><br><span class="line">#第二题</span><br><span class="line">data1=file1[,2:3];</span><br><span class="line">chisq.test(data1);</span><br><span class="line">#分析结果：p-value = 7.628e-06,所以拒绝原假设（不相关），说明收入高低对满意程度相关</span><br><span class="line"></span><br><span class="line">#第三题</span><br><span class="line">file2=read.csv(&apos;C://Users//Administrator//Desktop//data//2.csv&apos;);</span><br><span class="line">data2=file2[,2:5];</span><br><span class="line">freq_num= table (data2);#统计频数</span><br><span class="line">prop.table(freq_num);#将频数表变成频率表</span><br><span class="line"></span><br><span class="line">#第四题</span><br><span class="line">#首先统计双变量频数形成列联表</span><br><span class="line">b= table (data2$ Treatment ,data2$Sex);#统计双变量频数形成列联表</span><br><span class="line">c=table (data2$Sex ,data2$Improved);#统计双变量频数形成列联表</span><br><span class="line">#接下来将频数表变成频率表</span><br><span class="line">prop.table(b);</span><br><span class="line">prop.table(c);</span><br><span class="line">#最后利用列联分析说明Sex与Treatment和Improved是否相关</span><br><span class="line">chisq.test(b);#p-value = 0.5356，说明Treatment与Sex不相关</span><br><span class="line">chisq.test(c);#p-value = 0.08889，说明Improved与Sex也不相关</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#第五题</span><br><span class="line">library(MASS);</span><br><span class="line">file3=read.csv(&apos;C://Users//Administrator//Desktop//data//3.csv&apos;);</span><br><span class="line">data3=file3[,2:6];</span><br><span class="line">d=corresp(data3,2)#2指选用两个因子，结果中有行列因子的得分</span><br><span class="line">biplot(d);#绘图</span><br><span class="line">abline(v=0,h=0)#加入参考线</span><br></pre></td></tr></table></figure><p><img src="1.png" alt=""> <br><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#分析：对应分析图被参考线划分为4个区域，其中，</span><br><span class="line">#(1)在不同象限表示距离较远，这里的距离是任意两者之间相关性的度量；</span><br><span class="line">#(2)位置是相对于中心的距离，距该离越小说明相对所属类别相对常见；</span><br><span class="line">#(3)角度越小，代表两者相关性越强。</span><br><span class="line">#因此我们可以得出以下结论：</span><br><span class="line"># 1.学士，高中和大专的学历相对常见，也对，一般上了高中之后要么进入university（学士学位），或者进入college(大专)</span><br><span class="line"># 2.高中和大专的相关性较之于其它比较来说相关性最强</span><br><span class="line"># 3.硕士和高中以下出现频率较低，说明高中以下学历和硕士学历两者一个处于学历链的偏上端，一个处于偏下端，这也和我们的认知相符。</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#第六题</span><br><span class="line">file4=read.csv(&apos;C://Users//Administrator//Desktop//data//4.csv&apos;);</span><br><span class="line">data4=file4[,1:6];</span><br><span class="line">e=corresp(data4,2)#2指选用两个因子，结果中有行列因子的得分</span><br><span class="line">biplot(e);#绘图</span><br><span class="line">abline(v=0,h=0)#加入参考线</span><br></pre></td></tr></table></figure><p><img src="2.png" alt=""> <br><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#分析：</span><br><span class="line">#(1)使用小米和荣耀的人群相对常见，而使用苹果和VIVO的人群相对较少；</span><br><span class="line">#(2)使用小米和荣耀的人群之间相关性较强，说明一般使用小米的人也可能会对荣耀情有独钟，反之亦然；</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class
      
    
    </summary>
    
      <category term="R" scheme="http://yoursite.com/categories/R/"/>
    
    
  </entry>
  
  <entry>
    <title>TimeSeries with R</title>
    <link href="http://yoursite.com/2018/11/30/R/"/>
    <id>http://yoursite.com/2018/11/30/R/</id>
    <published>2018-11-30T11:42:45.000Z</published>
    <updated>2018-11-30T11:47:05.000Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#一、数据读取</span></span><br><span class="line">ts(data,start,end,frequency)</span><br><span class="line"><span class="comment">##文件是矩阵时按列读入，可以通过先转置再线性化为向量，代码如下</span></span><br><span class="line">D=read.table(<span class="string">'zz.txt'</span>)</span><br><span class="line">D=t(D)<span class="comment">#转置</span></span><br><span class="line">D=as.vector(D)<span class="comment">#将矩阵线性化为向量</span></span><br><span class="line"><span class="comment">#数据读取结束</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#二、描述时间序列</span></span><br><span class="line">ts.plot(D)<span class="comment">#ts(D);plot(D)</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#三、自相关和偏相关</span></span><br><span class="line">acf(D,lag,type,plot=True)</span><br><span class="line">pacf()</span><br><span class="line"></span><br><span class="line"><span class="comment">#四、平稳性检验</span></span><br><span class="line"><span class="comment">#（1）时间序列图检验</span></span><br><span class="line"><span class="comment">#（2）自相关图检验</span></span><br><span class="line"><span class="comment">#1.确定性时间序列</span></span><br><span class="line">x=seq(<span class="number">0.1</span>,<span class="number">5</span>,<span class="number">0.02</span>)</span><br><span class="line">y=<span class="number">0.8</span>*x+<span class="number">2</span></span><br><span class="line">D=jitter(y,amount=<span class="number">0.2</span>)<span class="comment">#加入随机扰动</span></span><br><span class="line">D=ts(D)</span><br><span class="line">par(mfrow=c(<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">plot(D)</span><br><span class="line"><span class="comment">#以上是时间序列图检验，下面进行自相关图检验                                                                           </span></span><br><span class="line">acf(D)</span><br><span class="line">pacf(D)</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.周期性时间序列</span></span><br><span class="line">x=seq(<span class="number">0.1</span>,<span class="number">15</span>,<span class="number">0.1</span>)</span><br><span class="line">y=sin(x)</span><br><span class="line">par(mfrow=c(<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line">D=jitter(y,amount=<span class="number">0.2</span>)</span><br><span class="line">ts.plot(D)</span><br><span class="line">acf(D)</span><br><span class="line">pacf(D)</span><br><span class="line"></span><br><span class="line"><span class="comment">#3.伪周期时间序列</span></span><br><span class="line">x=seq(<span class="number">0.1</span>,<span class="number">20</span>,<span class="number">0.1</span>)</span><br><span class="line">y=<span class="number">100</span>*sin(x)/x</span><br><span class="line">D=jitter(y,amount=<span class="number">3</span>)</span><br><span class="line">par(mfrow=c(<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line">plot(D)</span><br><span class="line">acf(D)</span><br><span class="line">pacf(D)</span><br><span class="line"></span><br><span class="line"><span class="comment">#五、纯随机性检验</span></span><br><span class="line"><span class="comment">#H0:是白噪声序列(acf,pacf基本为0)</span></span><br><span class="line">D=rnorm(<span class="number">300</span>,<span class="number">0</span>)</span><br><span class="line">D=ts(D)</span><br><span class="line">par(mfrow=c(<span class="number">3</span>,<span class="number">1</span>))</span><br><span class="line">plot(D)</span><br><span class="line">acf(D)</span><br><span class="line">pacf(D)</span><br><span class="line"><span class="comment">#接下来利用Box.test()进行检验</span></span><br><span class="line"><span class="comment">#result=0;lag=0;lb=0;p=0;</span></span><br><span class="line"><span class="keyword">for</span>(i <span class="keyword">in</span> <span class="number">1</span>:<span class="number">12</span>)</span><br><span class="line">&#123;btest=Box.test(D,type=<span class="string">"Ljung-Box"</span>,lag=i)</span><br><span class="line">lag[i]=i</span><br><span class="line">lb[i]=btest$statistic</span><br><span class="line">p[i]=btest$p.value</span><br><span class="line">result=cbind(lag,lb,p)</span><br><span class="line">&#125;</span><br><span class="line">print(result)</span><br><span class="line"></span><br><span class="line"><span class="comment">#六、ARMA模型拟合与识别 </span></span><br><span class="line"><span class="comment">#arima.sim(model,n),model是模型 ，n是序列的长度</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">#1.AR(1):X(t)=0.8*X(t-1)+E(t)</span></span><br><span class="line">D=arima.sim(list(order=c(<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>),ar=<span class="number">0.8</span>),n=<span class="number">200</span>)<span class="comment">#order的三个参数分别对应AR,I,MA的阶数</span></span><br><span class="line">plot(D)</span><br><span class="line">par(mfrow=c(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">acf(D)</span><br><span class="line">pacf(D)</span><br><span class="line"></span><br><span class="line"><span class="comment">#2.AR(2):X(t)=0.8*X(t-1)+0.1X(t-2)+E(t)</span></span><br><span class="line">D=arima.sim(list(order=c(<span class="number">2</span>,<span class="number">0</span>,<span class="number">0</span>),ar=c(<span class="number">0.8</span>，<span class="number">0.1</span>)),n=<span class="number">200</span>)<span class="comment">#order的三个参数分别对应AR,I,MA的阶数</span></span><br><span class="line">plot(D)</span><br><span class="line">par(mfrow=c(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">acf(D)</span><br><span class="line">pacf(D)</span><br><span class="line"></span><br><span class="line"><span class="comment">#3.MA(2):x(t)=E(t)-0.7*E(t-1)-0.2*E(t-2)</span></span><br><span class="line">D=arima.sim(list(order=c(<span class="number">0</span>,<span class="number">0</span>,<span class="number">2</span>),ma=c(<span class="number">0.7</span>,<span class="number">0.2</span>)),n=<span class="number">200</span>)</span><br><span class="line">plot(D)</span><br><span class="line">par(mfrow=c(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">acf(D)</span><br><span class="line">pacf(D)</span><br><span class="line"></span><br><span class="line"><span class="comment">#ARMA(2,2):X(t)=0.3*X(t-1)+0.4*X(t-2)+E(t)-0.5*E(t-1)-0.4*E(t-2)</span></span><br><span class="line">D=arima.sim(list(order=c(<span class="number">2</span>,<span class="number">0</span>,<span class="number">2</span>),ar=c(<span class="number">0.3</span>,<span class="number">0.4</span>)ma=c(<span class="number">0.5</span>,<span class="number">0.4</span>)),n=<span class="number">200</span>)</span><br><span class="line">plot(D)</span><br><span class="line">par(mfrow=c(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">acf(D)</span><br><span class="line">pacf(D)</span><br><span class="line"></span><br><span class="line"><span class="comment">#七、ARMA模型的估计</span></span><br><span class="line">D=arima.sim(list(order=c(<span class="number">2</span>,<span class="number">0</span>,<span class="number">2</span>),ar=c(-<span class="number">0.5</span>,<span class="number">0.4</span>),ma=c(<span class="number">0.5</span>,<span class="number">0.4</span>)),n=<span class="number">1000</span>)</span><br><span class="line">ARIMA=arima(D,order=c(<span class="number">2</span>,<span class="number">0</span>,<span class="number">2</span>),method=<span class="string">"ML"</span>)</span><br><span class="line">summary(ARIMA)</span><br><span class="line">ARIMA$coef</span><br><span class="line"></span><br><span class="line"><span class="comment">#八、ARMA模型的预测</span></span><br><span class="line"></span><br><span class="line">D=arima.sim(list(order=c(<span class="number">2</span>,<span class="number">0</span>,<span class="number">2</span>),ar=c(<span class="number">0.5</span>,-<span class="number">0.2</span>),ma=c(<span class="number">0.5</span>,<span class="number">0.2</span>)),n=<span class="number">100</span>)</span><br><span class="line">plot(D)</span><br><span class="line">par(mfrow=c(<span class="number">1</span>,<span class="number">2</span>))</span><br><span class="line">acf(D)</span><br><span class="line">pacf(D)</span><br><span class="line">ARIMA=arima(D,order=c(<span class="number">2</span>,<span class="number">0</span>,<span class="number">2</span>),method=<span class="string">"ML"</span>，include.mean=<span class="literal">FALSE</span>)</span><br><span class="line">summary(ARIMA)</span><br><span class="line">resid=ARIMA$residual</span><br><span class="line">Box.test(resid,lag=<span class="number">6</span>)</span><br><span class="line">Box.test(resid,lag=<span class="number">12</span>)</span><br><span class="line">pre=predict(ARIMA,n.ahead=<span class="number">10</span>)</span><br><span class="line">U=pre$pred+<span class="number">1.96</span>*pre$se<span class="comment">#置信上限</span></span><br><span class="line">L=pre$pred-<span class="number">1.96</span>*pre$se<span class="comment">#置信下限</span></span><br><span class="line">ts.plot(pre$pred,col=<span class="number">1</span>:<span class="number">2</span>)</span><br><span class="line">lines(U,col=<span class="string">"blue"</span>,lty=<span class="string">"dashed"</span>)</span><br><span class="line">lines(L.col=<span class="string">"blue"</span>,lty=<span class="string">"dashed"</span>)</span><br></pre></td></tr></table></figure><p><img src="1.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight r&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;li
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>运筹学运输问题</title>
    <link href="http://yoursite.com/2018/11/28/%E8%BF%90%E7%AD%B9%E5%AD%A6%E8%BF%90%E8%BE%93%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2018/11/28/运筹学运输问题/</id>
    <published>2018-11-28T08:10:59.000Z</published>
    <updated>2018-11-28T08:14:00.000Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br></pre></td><td class="code"><pre><span class="line">Exam2</span><br><span class="line"></span><br><span class="line">model:</span><br><span class="line">sets:</span><br><span class="line">Warehouse/1..3/:a;</span><br><span class="line">Customer/1..4/:b;</span><br><span class="line">Routes(Warehouse,Customer):c,x;</span><br><span class="line">endsets</span><br><span class="line">data:</span><br><span class="line">a=7,5,7;</span><br><span class="line">b=2,3,4,6;</span><br><span class="line">c=2,11,3,4,10,3,5,9,7,8,1,2;</span><br><span class="line">enddata</span><br><span class="line">[obj]min=@sum(Routes:c*x);</span><br><span class="line">@for(Warehouse(i):[SUP]</span><br><span class="line">@sum(Customer(j):x(i,j))&lt;=a(i));</span><br><span class="line">@for(Customer(j):[DEM]</span><br><span class="line">@sum(Warehouse(i):x(i,j))=b(j));</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  Global optimal solution found.</span><br><span class="line">  Objective value:                              35.00000</span><br><span class="line">  Infeasibilities:                              0.000000</span><br><span class="line">  Total solver iterations:                             5</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                       Variable           Value        Reduced Cost</span><br><span class="line">                          A( 1)        7.000000            0.000000</span><br><span class="line">                          A( 2)        5.000000            0.000000</span><br><span class="line">                          A( 3)        7.000000            0.000000</span><br><span class="line">                          B( 1)        2.000000            0.000000</span><br><span class="line">                          B( 2)        3.000000            0.000000</span><br><span class="line">                          B( 3)        4.000000            0.000000</span><br><span class="line">                          B( 4)        6.000000            0.000000</span><br><span class="line">                       C( 1, 1)        2.000000            0.000000</span><br><span class="line">                       C( 1, 2)        11.00000            0.000000</span><br><span class="line">                       C( 1, 3)        3.000000            0.000000</span><br><span class="line">                       C( 1, 4)        4.000000            0.000000</span><br><span class="line">                       C( 2, 1)        10.00000            0.000000</span><br><span class="line">                       C( 2, 2)        3.000000            0.000000</span><br><span class="line">                       C( 2, 3)        5.000000            0.000000</span><br><span class="line">                       C( 2, 4)        9.000000            0.000000</span><br><span class="line">                       C( 3, 1)        7.000000            0.000000</span><br><span class="line">                       C( 3, 2)        8.000000            0.000000</span><br><span class="line">                       C( 3, 3)        1.000000            0.000000</span><br><span class="line">                       C( 3, 4)        2.000000            0.000000</span><br><span class="line">                       X( 1, 1)        2.000000            0.000000</span><br><span class="line">                       X( 1, 2)        0.000000            8.000000</span><br><span class="line">                       X( 1, 3)        0.000000            0.000000</span><br><span class="line">                       X( 1, 4)        3.000000            0.000000</span><br><span class="line">                       X( 2, 1)        0.000000            8.000000</span><br><span class="line">                       X( 2, 2)        3.000000            0.000000</span><br><span class="line">                       X( 2, 3)        0.000000            2.000000</span><br><span class="line">                       X( 2, 4)        0.000000            5.000000</span><br><span class="line">                       X( 3, 1)        0.000000            7.000000</span><br><span class="line">                       X( 3, 2)        0.000000            7.000000</span><br><span class="line">                       X( 3, 3)        4.000000            0.000000</span><br><span class="line">                       X( 3, 4)        3.000000            0.000000</span><br><span class="line"></span><br><span class="line">                            Row    Slack or Surplus      Dual Price</span><br><span class="line">                            OBJ        35.00000           -1.000000</span><br><span class="line">                        SUP( 1)        2.000000            0.000000</span><br><span class="line">                        SUP( 2)        2.000000            0.000000</span><br><span class="line">                        SUP( 3)        0.000000            2.000000</span><br><span class="line">                        DEM( 1)        0.000000           -2.000000</span><br><span class="line">                        DEM( 2)        0.000000           -3.000000</span><br><span class="line">                        DEM( 3)        0.000000           -3.000000</span><br><span class="line">                        DEM( 4)        0.000000           -4.000000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Exam3</span><br><span class="line">model:</span><br><span class="line">sets:</span><br><span class="line">Warehouse/1..3/:a;</span><br><span class="line">Customer/1..4/:b,D;</span><br><span class="line">Routes(Warehouse,Customer):c,x;</span><br><span class="line">endsets</span><br><span class="line">data:</span><br><span class="line">a=50,60,50;</span><br><span class="line">b=30,70,0,10;</span><br><span class="line">d=50,70,30,1000000;</span><br><span class="line">c=16,13,22,17,14,13,19,15,19,20,23,1000000;</span><br><span class="line">enddata</span><br><span class="line">[obj]min=@sum(Routes:c*x);</span><br><span class="line">@for(Warehouse(i):[SUP]</span><br><span class="line">@sum(Customer(j):x(i,j))&lt;=a(i));</span><br><span class="line">@for(Customer(j):[DEM]</span><br><span class="line">@sum(Warehouse(i):x(i,j))&gt;=b(j));</span><br><span class="line">@for(Customer(j):[FHS]</span><br><span class="line">@sum(Warehouse(i):x(i,j))&lt;=d(j));</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  Global optimal solution found.</span><br><span class="line">  Objective value:                              1480.000</span><br><span class="line">  Infeasibilities:                              0.000000</span><br><span class="line">  Total solver iterations:                             4</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                       Variable           Value        Reduced Cost</span><br><span class="line">                          A( 1)        50.00000            0.000000</span><br><span class="line">                          A( 2)        60.00000            0.000000</span><br><span class="line">                          A( 3)        50.00000            0.000000</span><br><span class="line">                          B( 1)        30.00000            0.000000</span><br><span class="line">                          B( 2)        70.00000            0.000000</span><br><span class="line">                          B( 3)        0.000000            0.000000</span><br><span class="line">                          B( 4)        10.00000            0.000000</span><br><span class="line">                          D( 1)        50.00000            0.000000</span><br><span class="line">                          D( 2)        70.00000            0.000000</span><br><span class="line">                          D( 3)        30.00000            0.000000</span><br><span class="line">                          D( 4)        1000000.            0.000000</span><br><span class="line">                       C( 1, 1)        16.00000            0.000000</span><br><span class="line">                       C( 1, 2)        13.00000            0.000000</span><br><span class="line">                       C( 1, 3)        22.00000            0.000000</span><br><span class="line">                       C( 1, 4)        17.00000            0.000000</span><br><span class="line">                       C( 2, 1)        14.00000            0.000000</span><br><span class="line">                       C( 2, 2)        13.00000            0.000000</span><br><span class="line">                       C( 2, 3)        19.00000            0.000000</span><br><span class="line">                       C( 2, 4)        15.00000            0.000000</span><br><span class="line">                       C( 3, 1)        19.00000            0.000000</span><br><span class="line">                       C( 3, 2)        20.00000            0.000000</span><br><span class="line">                       C( 3, 3)        23.00000            0.000000</span><br><span class="line">                       C( 3, 4)        1000000.            0.000000</span><br><span class="line">                       X( 1, 1)        0.000000            2.000000</span><br><span class="line">                       X( 1, 2)        50.00000            0.000000</span><br><span class="line">                       X( 1, 3)        0.000000            22.00000</span><br><span class="line">                       X( 1, 4)        0.000000            2.000000</span><br><span class="line">                       X( 2, 1)        30.00000            0.000000</span><br><span class="line">                       X( 2, 2)        20.00000            0.000000</span><br><span class="line">                       X( 2, 3)        0.000000            19.00000</span><br><span class="line">                       X( 2, 4)        10.00000            0.000000</span><br><span class="line">                       X( 3, 1)        0.000000            5.000000</span><br><span class="line">                       X( 3, 2)        0.000000            7.000000</span><br><span class="line">                       X( 3, 3)        0.000000            23.00000</span><br><span class="line">                       X( 3, 4)        0.000000            999985.0</span><br><span class="line"></span><br><span class="line">                            Row    Slack or Surplus      Dual Price</span><br><span class="line">                            OBJ        1480.000           -1.000000</span><br><span class="line">                        SUP( 1)        0.000000            0.000000</span><br><span class="line">                        SUP( 2)        0.000000            0.000000</span><br><span class="line">                        SUP( 3)        50.00000            0.000000</span><br><span class="line">                        DEM( 1)        0.000000           -14.00000</span><br><span class="line">                        DEM( 2)        0.000000           -13.00000</span><br><span class="line">                        DEM( 3)        0.000000            0.000000</span><br><span class="line">                        DEM( 4)        0.000000           -15.00000</span><br><span class="line">                        FHS( 1)        20.00000            0.000000</span><br><span class="line">                        FHS( 2)        0.000000            0.000000</span><br><span class="line">                        FHS( 3)        30.00000            0.000000</span><br><span class="line">                        FHS( 4)        999990.0            0.000000</span><br><span class="line"></span><br><span class="line">Exam4</span><br><span class="line">model:</span><br><span class="line">sets:</span><br><span class="line">Warehouse/1..11/:a;</span><br><span class="line">Customer/1..11/:b;</span><br><span class="line">Routes(Warehouse,Customer):c,x;</span><br><span class="line">endsets</span><br><span class="line">data:</span><br><span class="line">a=27,24,29,20,20,20,20,20,20,20,20;</span><br><span class="line">b=20,20,20,20,20,20,20,23,26,25,26;</span><br><span class="line">c=0,1,3,2,1,4,3,3,11,3,10,1,0,1000000000,3,5,100000000,2,1,9,2,8,3,100000000,0,1,10000000,2,3,7,4,10,5,2,3,1,0,1,3,2,2,8,4,6,1,5,10000000,1,0,1,1,4,5,2,7,4,10000,2,3,1,0,2,1,8,2,4,3,2,3,2,1,2,0,1,100000,2,6,3,1,7,2,4,1,1,0,1,4,2,11,9,4,8,5,8,10000,1,0,2,1,3,2,10,4,2,2,2,4,2,0,8,10,8,5,6,7,4,6,2,1,8,0;</span><br><span class="line">enddata</span><br><span class="line">[obj]min=@sum(Routes:c*x);</span><br><span class="line">@for(Warehouse(i):[SUP]</span><br><span class="line">@sum(Customer(j):x(i,j))&lt;=a(i));</span><br><span class="line">@for(Customer(j):[DEM]</span><br><span class="line">@sum(Warehouse(i):x(i,j))=b(j));</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">  Global optimal solution found.</span><br><span class="line">  Objective value:                              68.00000</span><br><span class="line">  Infeasibilities:                              0.000000</span><br><span class="line">  Total solver iterations:                            22</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                       Variable           Value        Reduced Cost</span><br><span class="line">                          A( 1)        27.00000            0.000000</span><br><span class="line">                          A( 2)        24.00000            0.000000</span><br><span class="line">                          A( 3)        29.00000            0.000000</span><br><span class="line">                          A( 4)        20.00000            0.000000</span><br><span class="line">                          A( 5)        20.00000            0.000000</span><br><span class="line">                          A( 6)        20.00000            0.000000</span><br><span class="line">                          A( 7)        20.00000            0.000000</span><br><span class="line">                          A( 8)        20.00000            0.000000</span><br><span class="line">                          A( 9)        20.00000            0.000000</span><br><span class="line">                         A( 10)        20.00000            0.000000</span><br><span class="line">                         A( 11)        20.00000            0.000000</span><br><span class="line">                          B( 1)        20.00000            0.000000</span><br><span class="line">                          B( 2)        20.00000            0.000000</span><br><span class="line">                          B( 3)        20.00000            0.000000</span><br><span class="line">                          B( 4)        20.00000            0.000000</span><br><span class="line">                          B( 5)        20.00000            0.000000</span><br><span class="line">                          B( 6)        20.00000            0.000000</span><br><span class="line">                          B( 7)        20.00000            0.000000</span><br><span class="line">                          B( 8)        23.00000            0.000000</span><br><span class="line">                          B( 9)        26.00000            0.000000</span><br><span class="line">                         B( 10)        25.00000            0.000000</span><br><span class="line">                         B( 11)        26.00000            0.000000</span><br><span class="line">                       C( 1, 1)        0.000000            0.000000</span><br><span class="line">                       C( 1, 2)        1.000000            0.000000</span><br><span class="line">                       C( 1, 3)        3.000000            0.000000</span><br><span class="line">                       C( 1, 4)        2.000000            0.000000</span><br><span class="line">                       C( 1, 5)        1.000000            0.000000</span><br><span class="line">                       C( 1, 6)        4.000000            0.000000</span><br><span class="line">                       C( 1, 7)        3.000000            0.000000</span><br><span class="line">                       C( 1, 8)        3.000000            0.000000</span><br><span class="line">                       C( 1, 9)        11.00000            0.000000</span><br><span class="line">                      C( 1, 10)        3.000000            0.000000</span><br><span class="line">                      C( 1, 11)        10.00000            0.000000</span><br><span class="line">                       C( 2, 1)        1.000000            0.000000</span><br><span class="line">                       C( 2, 2)        0.000000            0.000000</span><br><span class="line">                       C( 2, 3)       0.1000000E+10        0.000000</span><br><span class="line">                       C( 2, 4)        3.000000            0.000000</span><br><span class="line">                       C( 2, 5)        5.000000            0.000000</span><br><span class="line">                       C( 2, 6)       0.1000000E+09        0.000000</span><br><span class="line">                       C( 2, 7)        2.000000            0.000000</span><br><span class="line">                       C( 2, 8)        1.000000            0.000000</span><br><span class="line">                       C( 2, 9)        9.000000            0.000000</span><br><span class="line">                      C( 2, 10)        2.000000            0.000000</span><br><span class="line">                      C( 2, 11)        8.000000            0.000000</span><br><span class="line">                       C( 3, 1)        3.000000            0.000000</span><br><span class="line">                       C( 3, 2)       0.1000000E+09        0.000000</span><br><span class="line">                       C( 3, 3)        0.000000            0.000000</span><br><span class="line">                       C( 3, 4)        1.000000            0.000000</span><br><span class="line">                       C( 3, 5)       0.1000000E+08        0.000000</span><br><span class="line">                       C( 3, 6)        2.000000            0.000000</span><br><span class="line">                       C( 3, 7)        3.000000            0.000000</span><br><span class="line">                       C( 3, 8)        7.000000            0.000000</span><br><span class="line">                       C( 3, 9)        4.000000            0.000000</span><br><span class="line">                      C( 3, 10)        10.00000            0.000000</span><br><span class="line">                      C( 3, 11)        5.000000            0.000000</span><br><span class="line">                       C( 4, 1)        2.000000            0.000000</span><br><span class="line">                       C( 4, 2)        3.000000            0.000000</span><br><span class="line">                       C( 4, 3)        1.000000            0.000000</span><br><span class="line">                       C( 4, 4)        0.000000            0.000000</span><br><span class="line">                       C( 4, 5)        1.000000            0.000000</span><br><span class="line">                       C( 4, 6)        3.000000            0.000000</span><br><span class="line">                       C( 4, 7)        2.000000            0.000000</span><br><span class="line">                       C( 4, 8)        2.000000            0.000000</span><br><span class="line">                       C( 4, 9)        8.000000            0.000000</span><br><span class="line">                      C( 4, 10)        4.000000            0.000000</span><br><span class="line">                      C( 4, 11)        6.000000            0.000000</span><br><span class="line">                       C( 5, 1)        1.000000            0.000000</span><br><span class="line">                       C( 5, 2)        5.000000            0.000000</span><br><span class="line">                       C( 5, 3)       0.1000000E+08        0.000000</span><br><span class="line">                       C( 5, 4)        1.000000            0.000000</span><br><span class="line">                       C( 5, 5)        0.000000            0.000000</span><br><span class="line">                       C( 5, 6)        1.000000            0.000000</span><br><span class="line">                       C( 5, 7)        1.000000            0.000000</span><br><span class="line">                       C( 5, 8)        4.000000            0.000000</span><br><span class="line">                       C( 5, 9)        5.000000            0.000000</span><br><span class="line">                      C( 5, 10)        2.000000            0.000000</span><br><span class="line">                      C( 5, 11)        7.000000            0.000000</span><br><span class="line">                       C( 6, 1)        4.000000            0.000000</span><br><span class="line">                       C( 6, 2)        10000.00            0.000000</span><br><span class="line">                       C( 6, 3)        2.000000            0.000000</span><br><span class="line">                       C( 6, 4)        3.000000            0.000000</span><br><span class="line">                       C( 6, 5)        1.000000            0.000000</span><br><span class="line">                       C( 6, 6)        0.000000            0.000000</span><br><span class="line">                       C( 6, 7)        2.000000            0.000000</span><br><span class="line">                       C( 6, 8)        1.000000            0.000000</span><br><span class="line">                       C( 6, 9)        8.000000            0.000000</span><br><span class="line">                      C( 6, 10)        2.000000            0.000000</span><br><span class="line">                      C( 6, 11)        4.000000            0.000000</span><br><span class="line">                       C( 7, 1)        3.000000            0.000000</span><br><span class="line">                       C( 7, 2)        2.000000            0.000000</span><br><span class="line">                       C( 7, 3)        3.000000            0.000000</span><br><span class="line">                       C( 7, 4)        2.000000            0.000000</span><br><span class="line">                       C( 7, 5)        1.000000            0.000000</span><br><span class="line">                       C( 7, 6)        2.000000            0.000000</span><br><span class="line">                       C( 7, 7)        0.000000            0.000000</span><br><span class="line">                       C( 7, 8)        1.000000            0.000000</span><br><span class="line">                       C( 7, 9)        100000.0            0.000000</span><br><span class="line">                      C( 7, 10)        2.000000            0.000000</span><br><span class="line">                      C( 7, 11)        6.000000            0.000000</span><br><span class="line">                       C( 8, 1)        3.000000            0.000000</span><br><span class="line">                       C( 8, 2)        1.000000            0.000000</span><br><span class="line">                       C( 8, 3)        7.000000            0.000000</span><br><span class="line">                       C( 8, 4)        2.000000            0.000000</span><br><span class="line">                       C( 8, 5)        4.000000            0.000000</span><br><span class="line">                       C( 8, 6)        1.000000            0.000000</span><br><span class="line">                       C( 8, 7)        1.000000            0.000000</span><br><span class="line">                       C( 8, 8)        0.000000            0.000000</span><br><span class="line">                       C( 8, 9)        1.000000            0.000000</span><br><span class="line">                      C( 8, 10)        4.000000            0.000000</span><br><span class="line">                      C( 8, 11)        2.000000            0.000000</span><br><span class="line">                       C( 9, 1)        11.00000            0.000000</span><br><span class="line">                       C( 9, 2)        9.000000            0.000000</span><br><span class="line">                       C( 9, 3)        4.000000            0.000000</span><br><span class="line">                       C( 9, 4)        8.000000            0.000000</span><br><span class="line">                       C( 9, 5)        5.000000            0.000000</span><br><span class="line">                       C( 9, 6)        8.000000            0.000000</span><br><span class="line">                       C( 9, 7)        10000.00            0.000000</span><br><span class="line">                       C( 9, 8)        1.000000            0.000000</span><br><span class="line">                       C( 9, 9)        0.000000            0.000000</span><br><span class="line">                      C( 9, 10)        2.000000            0.000000</span><br><span class="line">                      C( 9, 11)        1.000000            0.000000</span><br><span class="line">                      C( 10, 1)        3.000000            0.000000</span><br><span class="line">                      C( 10, 2)        2.000000            0.000000</span><br><span class="line">                      C( 10, 3)        10.00000            0.000000</span><br><span class="line">                      C( 10, 4)        4.000000            0.000000</span><br><span class="line">                      C( 10, 5)        2.000000            0.000000</span><br><span class="line">                      C( 10, 6)        2.000000            0.000000</span><br><span class="line">                      C( 10, 7)        2.000000            0.000000</span><br><span class="line">                      C( 10, 8)        4.000000            0.000000</span><br><span class="line">                      C( 10, 9)        2.000000            0.000000</span><br><span class="line">                     C( 10, 10)        0.000000            0.000000</span><br><span class="line">                     C( 10, 11)        8.000000            0.000000</span><br><span class="line">                      C( 11, 1)        10.00000            0.000000</span><br><span class="line">                      C( 11, 2)        8.000000            0.000000</span><br><span class="line">                      C( 11, 3)        5.000000            0.000000</span><br><span class="line">                      C( 11, 4)        6.000000            0.000000</span><br><span class="line">                      C( 11, 5)        7.000000            0.000000</span><br><span class="line">                      C( 11, 6)        4.000000            0.000000</span><br><span class="line">                      C( 11, 7)        6.000000            0.000000</span><br><span class="line">                      C( 11, 8)        2.000000            0.000000</span><br><span class="line">                      C( 11, 9)        1.000000            0.000000</span><br><span class="line">                     C( 11, 10)        8.000000            0.000000</span><br><span class="line">                     C( 11, 11)        0.000000            0.000000</span><br><span class="line">                       X( 1, 1)        20.00000            0.000000</span><br><span class="line">                       X( 1, 2)        7.000000            0.000000</span><br><span class="line">                       X( 1, 3)        0.000000            4.000000</span><br><span class="line">                       X( 1, 4)        0.000000            2.000000</span><br><span class="line">                       X( 1, 5)        0.000000            0.000000</span><br><span class="line">                       X( 1, 6)        0.000000            3.000000</span><br><span class="line">                       X( 1, 7)        0.000000            2.000000</span><br><span class="line">                       X( 1, 8)        0.000000            1.000000</span><br><span class="line">                       X( 1, 9)        0.000000            8.000000</span><br><span class="line">                      X( 1, 10)        0.000000            0.000000</span><br><span class="line">                      X( 1, 11)        0.000000            6.000000</span><br><span class="line">                       X( 2, 1)        0.000000            2.000000</span><br><span class="line">                       X( 2, 2)        13.00000            0.000000</span><br><span class="line">                       X( 2, 3)        0.000000           0.1000000E+10</span><br><span class="line">                       X( 2, 4)        0.000000            4.000000</span><br><span class="line">                       X( 2, 5)        0.000000            5.000000</span><br><span class="line">                       X( 2, 6)        0.000000           0.1000000E+09</span><br><span class="line">                       X( 2, 7)        0.000000            2.000000</span><br><span class="line">                       X( 2, 8)        6.000000            0.000000</span><br><span class="line">                       X( 2, 9)        0.000000            7.000000</span><br><span class="line">                      X( 2, 10)        5.000000            0.000000</span><br><span class="line">                      X( 2, 11)        0.000000            5.000000</span><br><span class="line">                       X( 3, 1)        0.000000            2.000000</span><br><span class="line">                       X( 3, 2)        0.000000           0.1000000E+09</span><br><span class="line">                       X( 3, 3)        20.00000            0.000000</span><br><span class="line">                       X( 3, 4)        0.000000            0.000000</span><br><span class="line">                       X( 3, 5)        0.000000            9999998.</span><br><span class="line">                       X( 3, 6)        0.000000            0.000000</span><br><span class="line">                       X( 3, 7)        0.000000            1.000000</span><br><span class="line">                       X( 3, 8)        0.000000            4.000000</span><br><span class="line">                       X( 3, 9)        9.000000            0.000000</span><br><span class="line">                      X( 3, 10)        0.000000            6.000000</span><br><span class="line">                      X( 3, 11)        0.000000            0.000000</span><br><span class="line">                       X( 4, 1)        0.000000            2.000000</span><br><span class="line">                       X( 4, 2)        0.000000            2.000000</span><br><span class="line">                       X( 4, 3)        0.000000            2.000000</span><br><span class="line">                       X( 4, 4)        20.00000            0.000000</span><br><span class="line">                       X( 4, 5)        0.000000            0.000000</span><br><span class="line">                       X( 4, 6)        0.000000            2.000000</span><br><span class="line">                       X( 4, 7)        0.000000            1.000000</span><br><span class="line">                       X( 4, 8)        0.000000            0.000000</span><br><span class="line">                       X( 4, 9)        0.000000            5.000000</span><br><span class="line">                      X( 4, 10)        0.000000            1.000000</span><br><span class="line">                      X( 4, 11)        0.000000            2.000000</span><br><span class="line">                       X( 5, 1)        0.000000            2.000000</span><br><span class="line">                       X( 5, 2)        0.000000            5.000000</span><br><span class="line">                       X( 5, 3)        0.000000           0.1000000E+08</span><br><span class="line">                       X( 5, 4)        0.000000            2.000000</span><br><span class="line">                       X( 5, 5)        20.00000            0.000000</span><br><span class="line">                       X( 5, 6)        0.000000            1.000000</span><br><span class="line">                       X( 5, 7)        0.000000            1.000000</span><br><span class="line">                       X( 5, 8)        0.000000            3.000000</span><br><span class="line">                       X( 5, 9)        0.000000            3.000000</span><br><span class="line">                      X( 5, 10)        0.000000            0.000000</span><br><span class="line">                      X( 5, 11)        0.000000            4.000000</span><br><span class="line">                       X( 6, 1)        0.000000            5.000000</span><br><span class="line">                       X( 6, 2)        0.000000            10000.00</span><br><span class="line">                       X( 6, 3)        0.000000            4.000000</span><br><span class="line">                       X( 6, 4)        0.000000            4.000000</span><br><span class="line">                       X( 6, 5)        0.000000            1.000000</span><br><span class="line">                       X( 6, 6)        20.00000            0.000000</span><br><span class="line">                       X( 6, 7)        0.000000            2.000000</span><br><span class="line">                       X( 6, 8)        0.000000            0.000000</span><br><span class="line">                       X( 6, 9)        0.000000            6.000000</span><br><span class="line">                      X( 6, 10)        0.000000            0.000000</span><br><span class="line">                      X( 6, 11)        0.000000            1.000000</span><br><span class="line">                       X( 7, 1)        0.000000            4.000000</span><br><span class="line">                       X( 7, 2)        0.000000            2.000000</span><br><span class="line">                       X( 7, 3)        0.000000            5.000000</span><br><span class="line">                       X( 7, 4)        0.000000            3.000000</span><br><span class="line">                       X( 7, 5)        0.000000            1.000000</span><br><span class="line">                       X( 7, 6)        0.000000            2.000000</span><br><span class="line">                       X( 7, 7)        20.00000            0.000000</span><br><span class="line">                       X( 7, 8)        0.000000            0.000000</span><br><span class="line">                       X( 7, 9)        0.000000            99998.00</span><br><span class="line">                      X( 7, 10)        0.000000            0.000000</span><br><span class="line">                      X( 7, 11)        0.000000            3.000000</span><br><span class="line">                       X( 8, 1)        0.000000            5.000000</span><br><span class="line">                       X( 8, 2)        0.000000            2.000000</span><br><span class="line">                       X( 8, 3)        0.000000            10.00000</span><br><span class="line">                       X( 8, 4)        0.000000            4.000000</span><br><span class="line">                       X( 8, 5)        0.000000            5.000000</span><br><span class="line">                       X( 8, 6)        0.000000            2.000000</span><br><span class="line">                       X( 8, 7)        0.000000            2.000000</span><br><span class="line">                       X( 8, 8)        17.00000            0.000000</span><br><span class="line">                       X( 8, 9)        3.000000            0.000000</span><br><span class="line">                      X( 8, 10)        0.000000            3.000000</span><br><span class="line">                      X( 8, 11)        0.000000            0.000000</span><br><span class="line">                       X( 9, 1)        0.000000            14.00000</span><br><span class="line">                       X( 9, 2)        0.000000            11.00000</span><br><span class="line">                       X( 9, 3)        0.000000            8.000000</span><br><span class="line">                       X( 9, 4)        0.000000            11.00000</span><br><span class="line">                       X( 9, 5)        0.000000            7.000000</span><br><span class="line">                       X( 9, 6)        0.000000            10.00000</span><br><span class="line">                       X( 9, 7)        0.000000            10002.00</span><br><span class="line">                       X( 9, 8)        0.000000            2.000000</span><br><span class="line">                       X( 9, 9)        14.00000            0.000000</span><br><span class="line">                      X( 9, 10)        0.000000            2.000000</span><br><span class="line">                      X( 9, 11)        6.000000            0.000000</span><br><span class="line">                      X( 10, 1)        0.000000            6.000000</span><br><span class="line">                      X( 10, 2)        0.000000            4.000000</span><br><span class="line">                      X( 10, 3)        0.000000            14.00000</span><br><span class="line">                      X( 10, 4)        0.000000            7.000000</span><br><span class="line">                      X( 10, 5)        0.000000            4.000000</span><br><span class="line">                      X( 10, 6)        0.000000            4.000000</span><br><span class="line">                      X( 10, 7)        0.000000            4.000000</span><br><span class="line">                      X( 10, 8)        0.000000            5.000000</span><br><span class="line">                      X( 10, 9)        0.000000            2.000000</span><br><span class="line">                     X( 10, 10)        20.00000            0.000000</span><br><span class="line">                     X( 10, 11)        0.000000            7.000000</span><br><span class="line">                      X( 11, 1)        0.000000            14.00000</span><br><span class="line">                      X( 11, 2)        0.000000            11.00000</span><br><span class="line">                      X( 11, 3)        0.000000            10.00000</span><br><span class="line">                      X( 11, 4)        0.000000            10.00000</span><br><span class="line">                      X( 11, 5)        0.000000            10.00000</span><br><span class="line">                      X( 11, 6)        0.000000            7.000000</span><br><span class="line">                      X( 11, 7)        0.000000            9.000000</span><br><span class="line">                      X( 11, 8)        0.000000            4.000000</span><br><span class="line">                      X( 11, 9)        0.000000            2.000000</span><br><span class="line">                     X( 11, 10)        0.000000            9.000000</span><br><span class="line">                     X( 11, 11)        20.00000            0.000000</span><br><span class="line"></span><br><span class="line">                            Row    Slack or Surplus      Dual Price</span><br><span class="line">                            OBJ        68.00000           -1.000000</span><br><span class="line">                        SUP( 1)        0.000000            1.000000</span><br><span class="line">                        SUP( 2)        0.000000            2.000000</span><br><span class="line">                        SUP( 3)        0.000000            0.000000</span><br><span class="line">                        SUP( 4)        0.000000            1.000000</span><br><span class="line">                        SUP( 5)        0.000000            2.000000</span><br><span class="line">                        SUP( 6)        0.000000            2.000000</span><br><span class="line">                        SUP( 7)        0.000000            2.000000</span><br><span class="line">                        SUP( 8)        0.000000            3.000000</span><br><span class="line">                        SUP( 9)        0.000000            4.000000</span><br><span class="line">                       SUP( 10)        0.000000            4.000000</span><br><span class="line">                       SUP( 11)        0.000000            5.000000</span><br><span class="line">                        DEM( 1)        0.000000           -1.000000</span><br><span class="line">                        DEM( 2)        0.000000           -2.000000</span><br><span class="line">                        DEM( 3)        0.000000            0.000000</span><br><span class="line">                        DEM( 4)        0.000000           -1.000000</span><br><span class="line">                        DEM( 5)        0.000000           -2.000000</span><br><span class="line">                        DEM( 6)        0.000000           -2.000000</span><br><span class="line">                        DEM( 7)        0.000000           -2.000000</span><br><span class="line">                        DEM( 8)        0.000000           -3.000000</span><br><span class="line">                        DEM( 9)        0.000000           -4.000000</span><br><span class="line">                       DEM( 10)        0.000000           -4.000000</span><br><span class="line">                       DEM( 11)        0.000000           -5.000000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a.</span><br><span class="line">model:</span><br><span class="line">sets:</span><br><span class="line">Warehouse/1..3/:a;</span><br><span class="line">Customer/1..4/:b;</span><br><span class="line">Routes(Warehouse,Customer):c,x;</span><br><span class="line">endsets</span><br><span class="line">data:</span><br><span class="line">a=15,25,5;</span><br><span class="line">b=5,15,15,10;</span><br><span class="line">c=10,2,20,11,12,7,9,20,2,14,16,18;</span><br><span class="line">enddata</span><br><span class="line">[obj]min=@sum(Routes:c*x);</span><br><span class="line">@for(Warehouse(i):[SUP]</span><br><span class="line">@sum(Customer(j):x(i,j))&lt;=a(i));</span><br><span class="line">@for(Customer(j):[DEM]</span><br><span class="line">@sum(Warehouse(i):x(i,j))=b(j));</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  Global optimal solution found.</span><br><span class="line">  Objective value:                              335.0000</span><br><span class="line">  Infeasibilities:                              0.000000</span><br><span class="line">  Total solver iterations:                             5</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                       Variable           Value        Reduced Cost</span><br><span class="line">                          A( 1)        15.00000            0.000000</span><br><span class="line">                          A( 2)        25.00000            0.000000</span><br><span class="line">                          A( 3)        5.000000            0.000000</span><br><span class="line">                          B( 1)        5.000000            0.000000</span><br><span class="line">                          B( 2)        15.00000            0.000000</span><br><span class="line">                          B( 3)        15.00000            0.000000</span><br><span class="line">                          B( 4)        10.00000            0.000000</span><br><span class="line">                       C( 1, 1)        10.00000            0.000000</span><br><span class="line">                       C( 1, 2)        2.000000            0.000000</span><br><span class="line">                       C( 1, 3)        20.00000            0.000000</span><br><span class="line">                       C( 1, 4)        11.00000            0.000000</span><br><span class="line">                       C( 2, 1)        12.00000            0.000000</span><br><span class="line">                       C( 2, 2)        7.000000            0.000000</span><br><span class="line">                       C( 2, 3)        9.000000            0.000000</span><br><span class="line">                       C( 2, 4)        20.00000            0.000000</span><br><span class="line">                       C( 3, 1)        2.000000            0.000000</span><br><span class="line">                       C( 3, 2)        14.00000            0.000000</span><br><span class="line">                       C( 3, 3)        16.00000            0.000000</span><br><span class="line">                       C( 3, 4)        18.00000            0.000000</span><br><span class="line">                       X( 1, 1)        0.000000            13.00000</span><br><span class="line">                       X( 1, 2)        5.000000            0.000000</span><br><span class="line">                       X( 1, 3)        0.000000            16.00000</span><br><span class="line">                       X( 1, 4)        10.00000            0.000000</span><br><span class="line">                       X( 2, 1)        0.000000            10.00000</span><br><span class="line">                       X( 2, 2)        10.00000            0.000000</span><br><span class="line">                       X( 2, 3)        15.00000            0.000000</span><br><span class="line">                       X( 2, 4)        0.000000            4.000000</span><br><span class="line">                       X( 3, 1)        5.000000            0.000000</span><br><span class="line">                       X( 3, 2)        0.000000            7.000000</span><br><span class="line">                       X( 3, 3)        0.000000            7.000000</span><br><span class="line">                       X( 3, 4)        0.000000            2.000000</span><br><span class="line"></span><br><span class="line">                            Row    Slack or Surplus      Dual Price</span><br><span class="line">                            OBJ        335.0000           -1.000000</span><br><span class="line">                        SUP( 1)        0.000000            5.000000</span><br><span class="line">                        SUP( 2)        0.000000            0.000000</span><br><span class="line">                        SUP( 3)        0.000000            0.000000</span><br><span class="line">                        DEM( 1)        0.000000           -2.000000</span><br><span class="line">                        DEM( 2)        0.000000           -7.000000</span><br><span class="line">                        DEM( 3)        0.000000           -9.000000</span><br><span class="line">                        DEM( 4)        0.000000           -16.00000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">b.</span><br><span class="line">model:</span><br><span class="line">sets:</span><br><span class="line">Warehouse/1..3/:a;</span><br><span class="line">Customer/1..4/:b;</span><br><span class="line">Routes(Warehouse,Customer):c,x;</span><br><span class="line">endsets</span><br><span class="line">data:</span><br><span class="line">a=7,25,26;</span><br><span class="line">b=10,10,20,15;</span><br><span class="line">c=8,4,1,2,6,9,4,7,5,3,4,3;</span><br><span class="line">enddata</span><br><span class="line">[obj]min=@sum(Routes:c*x);</span><br><span class="line">@for(Warehouse(i):[SUP]</span><br><span class="line">@sum(Customer(j):x(i,j))&lt;=a(i));</span><br><span class="line">@for(Customer(j):[DEM]</span><br><span class="line">@sum(Warehouse(i):x(i,j))=b(j));</span><br><span class="line">end</span><br><span class="line">  Global optimal solution found.</span><br><span class="line">  Objective value:                              193.0000</span><br><span class="line">  Infeasibilities:                              0.000000</span><br><span class="line">  Total solver iterations:                             7</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                       Variable           Value        Reduced Cost</span><br><span class="line">                          A( 1)        7.000000            0.000000</span><br><span class="line">                          A( 2)        25.00000            0.000000</span><br><span class="line">                          A( 3)        26.00000            0.000000</span><br><span class="line">                          B( 1)        10.00000            0.000000</span><br><span class="line">                          B( 2)        10.00000            0.000000</span><br><span class="line">                          B( 3)        20.00000            0.000000</span><br><span class="line">                          B( 4)        15.00000            0.000000</span><br><span class="line">                       C( 1, 1)        8.000000            0.000000</span><br><span class="line">                       C( 1, 2)        4.000000            0.000000</span><br><span class="line">                       C( 1, 3)        1.000000            0.000000</span><br><span class="line">                       C( 1, 4)        2.000000            0.000000</span><br><span class="line">                       C( 2, 1)        6.000000            0.000000</span><br><span class="line">                       C( 2, 2)        9.000000            0.000000</span><br><span class="line">                       C( 2, 3)        4.000000            0.000000</span><br><span class="line">                       C( 2, 4)        7.000000            0.000000</span><br><span class="line">                       C( 3, 1)        5.000000            0.000000</span><br><span class="line">                       C( 3, 2)        3.000000            0.000000</span><br><span class="line">                       C( 3, 3)        4.000000            0.000000</span><br><span class="line">                       C( 3, 4)        3.000000            0.000000</span><br><span class="line">                       X( 1, 1)        0.000000            5.000000</span><br><span class="line">                       X( 1, 2)        0.000000            3.000000</span><br><span class="line">                       X( 1, 3)        7.000000            0.000000</span><br><span class="line">                       X( 1, 4)        0.000000            1.000000</span><br><span class="line">                       X( 2, 1)        9.000000            0.000000</span><br><span class="line">                       X( 2, 2)        0.000000            5.000000</span><br><span class="line">                       X( 2, 3)        13.00000            0.000000</span><br><span class="line">                       X( 2, 4)        0.000000            3.000000</span><br><span class="line">                       X( 3, 1)        1.000000            0.000000</span><br><span class="line">                       X( 3, 2)        10.00000            0.000000</span><br><span class="line">                       X( 3, 3)        0.000000            1.000000</span><br><span class="line">                       X( 3, 4)        15.00000            0.000000</span><br><span class="line"></span><br><span class="line">                            Row    Slack or Surplus      Dual Price</span><br><span class="line">                            OBJ        193.0000           -1.000000</span><br><span class="line">                        SUP( 1)        0.000000            3.000000</span><br><span class="line">                        SUP( 2)        3.000000            0.000000</span><br><span class="line">                        SUP( 3)        0.000000            1.000000</span><br><span class="line">                        DEM( 1)        0.000000           -6.000000</span><br><span class="line">                        DEM( 2)        0.000000           -4.000000</span><br><span class="line">                        DEM( 3)        0.000000           -4.000000</span><br><span class="line">                        DEM( 4)        0.000000           -4.000000</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>二叉树的前/后/中序的遍历的非递归实现</title>
    <link href="http://yoursite.com/2018/11/26/%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E5%89%8D-%E5%90%8E-%E4%B8%AD%E5%BA%8F%E7%9A%84%E9%81%8D%E5%8E%86%E7%9A%84%E9%9D%9E%E9%80%92%E5%BD%92%E5%AE%9E%E7%8E%B0/"/>
    <id>http://yoursite.com/2018/11/26/二叉树的前-后-中序的遍历的非递归实现/</id>
    <published>2018-11-26T12:58:16.000Z</published>
    <updated>2018-11-26T13:02:57.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>前序遍历</li></ul><p><img src="qian.png" alt=""> <br></p><ul><li>中序遍历</li></ul><p><img src="zhong.png" alt=""> <br></p><ul><li>后序遍历</li></ul><p><img src="hou.png" alt=""> <br></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;前序遍历&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;qian.png&quot; alt=&quot;&quot;&gt; &lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;中序遍历&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&quot;zhong.png&quot; alt=&quot;&quot;&gt; &lt;br&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
      
    
    </summary>
    
      <category term="数据结构自学史" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%87%AA%E5%AD%A6%E5%8F%B2/"/>
    
    
  </entry>
  
  <entry>
    <title>用Python自制考研英语单词手册</title>
    <link href="http://yoursite.com/2018/11/24/kyyy/"/>
    <id>http://yoursite.com/2018/11/24/kyyy/</id>
    <published>2018-11-24T13:40:04.000Z</published>
    <updated>2018-11-24T13:50:43.000Z</updated>
    
    <content type="html"><![CDATA[<h4 id="本文目标"><a href="#本文目标" class="headerlink" title="本文目标:"></a>本文目标:</h4><p><strong>从考研英语往年真题中提取单词，统计单词出现次数，按照频数排序并写入表格文件</strong><br></p><h4 id="第一步"><a href="#第一步" class="headerlink" title="第一步"></a>第一步</h4><p><strong>导入必要的库</strong><br><br>其中<code>re</code>用于文本处理，<code>jieba</code>用于分词，<code>pandas</code>用于写入表格文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br></pre></td></tr></table></figure><h4 id="第二步"><a href="#第二步" class="headerlink" title="第二步"></a>第二步</h4><p>打开文件<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">file=open(<span class="string">'/home/fantasy/Desktop/kyyy.txt'</span>).read()</span><br></pre></td></tr></table></figure></p><h4 id="第三步"><a href="#第三步" class="headerlink" title="第三步"></a>第三步</h4><p>文本处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> file:</span><br><span class="line">    line=line.strip(<span class="string">'\n'</span>)</span><br><span class="line">    <span class="comment">#去除文本中的中文符号和英文符号</span></span><br><span class="line">    line=re.sub(<span class="string">"[\s+\.\!\/_,$%^*(+\"\']+|[+——__！，。.,\？?、~@#￥%……&amp;*（）]+"</span>, <span class="string">""</span>,line)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 大佬请自动忽略这段辣眼睛的代码qaq~^---^~~~</span></span><br><span class="line">file = re.sub(<span class="string">'________'</span>, <span class="string">''</span>,file)</span><br><span class="line">file=re.sub(<span class="string">'__'</span>,<span class="string">''</span>,file)</span><br><span class="line">file=re.sub(<span class="string">'\n'</span>,<span class="string">''</span>,file)</span><br><span class="line">file=re.sub(<span class="string">''</span>,<span class="string">''</span>,file)</span><br><span class="line">file=re.sub(<span class="string">','</span>,<span class="string">''</span>,file)</span><br><span class="line"><span class="comment">#file=re.sub('.','',file)</span></span><br><span class="line">file=re.sub(<span class="string">r'[^\x00-\x7f]'</span>, <span class="string">''</span>, file)<span class="comment">#去掉中文字符串</span></span><br><span class="line">file=re.sub(<span class="string">'[A]'</span>,<span class="string">''</span>,file)</span><br><span class="line">file=re.sub(<span class="string">'[B]'</span>,<span class="string">''</span>,file)</span><br><span class="line">file=re.sub(<span class="string">'[C]'</span>,<span class="string">''</span>,file)</span><br><span class="line">file=re.sub(<span class="string">'[D]'</span>,<span class="string">''</span>,file)</span><br><span class="line"><span class="comment">#file=re.sub('[]','',file)</span></span><br><span class="line"></span><br><span class="line">file=re.sub(<span class="string">'\t'</span>,<span class="string">''</span>,file)</span><br><span class="line">file=re.sub(<span class="string">'()'</span>,<span class="string">''</span>,file)</span><br><span class="line">file=re.sub(<span class="string">'---!!!'</span>,<span class="string">''</span>,file)</span><br><span class="line">file=re.sub(<span class="string">'\[]'</span>,<span class="string">''</span>,file)</span><br><span class="line">file=re.sub(<span class="string">'、'</span>,<span class="string">''</span>,file)</span><br><span class="line">file=re.sub(<span class="string">"''"</span>,<span class="string">''</span>,file)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">f=jieba.cut(file,<span class="string">' '</span> )</span><br><span class="line">f=<span class="string">','</span>.join(f)</span><br></pre></td></tr></table></figure><h4 id="第四步"><a href="#第四步" class="headerlink" title="第四步"></a>第四步</h4><p>利用字典数据结构统计词频并存入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">counter = &#123;&#125;</span><br><span class="line"><span class="comment"># 如果字典里有该词则加1，否则添加入字典</span></span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> f.split(<span class="string">','</span>):</span><br><span class="line">    <span class="keyword">if</span> s <span class="keyword">not</span> <span class="keyword">in</span> counter:</span><br><span class="line">        counter[s] = <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        counter[s] += <span class="number">1</span></span><br><span class="line"><span class="comment">#词频从高到低排序</span></span><br><span class="line">sorted_counter=sorted(counter.items(),key = <span class="keyword">lambda</span> x:x[<span class="number">1</span>],reverse = <span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#忽略的常用词汇</span></span><br><span class="line">ignore=[<span class="string">'one'</span>,<span class="string">'two'</span>,<span class="string">'three'</span>,<span class="string">'four'</span>,<span class="string">'five'</span>,<span class="string">'six'</span>,<span class="string">'seven'</span>,<span class="string">'eight'</span>,<span class="string">'nine'</span>,<span class="string">'the'</span>,<span class="string">'of'</span>,<span class="string">'can'</span>,<span class="string">'may'</span>,<span class="string">'should'</span>,<span class="string">'in'</span>,<span class="string">'a'</span>,<span class="string">'an'</span>,<span class="string">'to'</span>,<span class="string">'and'</span>,<span class="string">'am'</span>,<span class="string">'is'</span>,<span class="string">'are'</span>,<span class="string">'be'</span>,<span class="string">'on'</span>,<span class="string">'that'</span>,</span><br><span class="line">   <span class="string">'this'</span>,<span class="string">'be'</span>,<span class="string">'he'</span>,<span class="string">'she'</span>,<span class="string">'you'</span>,<span class="string">'me'</span>,<span class="string">'him'</span>,<span class="string">'her'</span>,<span class="string">'mom'</span>,<span class="string">'son'</span>,<span class="string">'sister'</span>,<span class="string">'brother'</span>,<span class="string">'with'</span>,<span class="string">'it'</span>,<span class="string">'go'</span>,<span class="string">'run'</span>,<span class="string">'bed'</span>,</span><br><span class="line">   <span class="string">'home'</span>,<span class="string">'house'</span>,<span class="string">'great'</span>,<span class="string">'yes'</span>,<span class="string">'no'</span>,<span class="string">'nope'</span>,<span class="string">'yep'</span>,<span class="string">'but'</span>,<span class="string">'by'</span>,<span class="string">'do'</span>,<span class="string">'doing'</span>,<span class="string">'done'</span>,<span class="string">'over'</span>,<span class="string">'as'</span>,<span class="string">'who'</span>,<span class="string">'would'</span>,<span class="string">'the'</span>,<span class="string">'not'</span>,<span class="string">'or'</span>,<span class="string">'will'</span>,<span class="string">'have'</span>,<span class="string">'from'</span>,<span class="string">'their'</span>,<span class="string">'than'</span>,</span><br><span class="line">   <span class="string">'its'</span>,<span class="string">'all'</span>,<span class="string">'up'</span>,<span class="string">'down'</span>,<span class="string">'how'</span>,<span class="string">'for'</span>,<span class="string">'the'</span>,<span class="string">'like'</span>]</span><br></pre></td></tr></table></figure><h4 id="第五步"><a href="#第五步" class="headerlink" title="第五步"></a>第五步</h4><p>设定提取规则</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#提取设定规则的单词及其频数</span></span><br><span class="line">word=[]</span><br><span class="line">values=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> sorted_counter:</span><br><span class="line">    <span class="keyword">if</span> i[<span class="number">0</span>] <span class="keyword">not</span> <span class="keyword">in</span> ignore <span class="keyword">and</span> len(i[<span class="number">0</span>])&gt;<span class="number">3</span>:</span><br><span class="line">        c=i[<span class="number">0</span>].lower()</span><br><span class="line">        word.append(c)</span><br><span class="line">        values.append(i[<span class="number">1</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">len(word)==len(values)</span><br></pre></td></tr></table></figure><pre><code>True</code></pre><h4 id="第六步"><a href="#第六步" class="headerlink" title="第六步"></a>第六步</h4><p>将词频大于1的单词写入文件并保存之</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建一个空的数据框</span></span><br><span class="line">result=pd.DataFrame(columns=(<span class="string">'word'</span>,<span class="string">'freq'</span>))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#写入文件</span></span><br><span class="line">result[<span class="string">'word'</span>]=word</span><br><span class="line">result[<span class="string">'freq'</span>]=values</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#只提取出现次数大于１的单词</span></span><br><span class="line">final_words_file=result.loc[result[<span class="string">'freq'</span>]&gt;<span class="number">1</span>,:]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#保存文件</span></span><br><span class="line">final_words_file.to_csv(<span class="string">'/home/fantasy/Desktop/kyyy.csv'</span>)</span><br></pre></td></tr></table></figure><hr><p>～～～～～～～～～～华丽的分割线～～～～～～～～～～～</p><h4 id="第七步"><a href="#第七步" class="headerlink" title="第七步"></a>第七步</h4><p>翻译单词<br><br><strong>以下翻译过程暂时未完成，有时间再研究下</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#翻译</span></span><br><span class="line"><span class="keyword">import</span> http.client</span><br><span class="line"><span class="keyword">import</span> hashlib</span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> parse</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">appid = <span class="string">'20181008000216770'</span></span><br><span class="line">secretKey = <span class="string">'2fWF_wlQ5sifMJVKWqFF'</span></span><br><span class="line">httpClient = <span class="keyword">None</span></span><br><span class="line">myurl = <span class="string">'/api/trans/vip/translate'</span></span><br><span class="line">fromLang=<span class="string">'en'</span></span><br><span class="line">toLang = <span class="string">'zh'</span></span><br><span class="line">salt = random.randint(<span class="number">32768</span>, <span class="number">65536</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(word)):</span><br><span class="line">    q=word[i]</span><br><span class="line">    print(q)</span><br><span class="line">    sign = appid+q+str(salt)+secretKey</span><br><span class="line">    m1 = hashlib.md5()</span><br><span class="line">    m1.update(sign.encode(encoding=<span class="string">'utf-8'</span>))</span><br><span class="line">    sign = m1.hexdigest()</span><br><span class="line">    myurl = myurl+<span class="string">'?appid='</span>+appid+<span class="string">'&amp;q='</span>+parse.quote(q)+<span class="string">'&amp;from='</span>+fromLang+<span class="string">'&amp;to='</span>+toLang+<span class="string">'&amp;salt='</span>+str(salt)+<span class="string">'&amp;sign='</span>+sign </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        httpClient = http.client.HTTPConnection(<span class="string">'api.fanyi.baidu.com'</span>)</span><br><span class="line">        httpClient.request(<span class="string">'GET'</span>, myurl)</span><br><span class="line">        response = httpClient.getresponse()</span><br><span class="line">        stri = response.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">        stri = eval(str)</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> str[<span class="string">'trans_result'</span>]:</span><br><span class="line">            file.write(line[<span class="string">'dst'</span>]+<span class="string">'\n'</span>)</span><br><span class="line">    <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">        print(e)</span><br><span class="line">    <span class="keyword">finally</span>:</span><br><span class="line">        <span class="keyword">if</span> httpClient:</span><br><span class="line">            httpClient.close()</span><br></pre></td></tr></table></figure><pre><code>theyeval() arg 1 must be a string, bytes or code objectmoreeval() arg 1 must be a string, bytes or code objectpointseval() arg 1 must be a string, bytes or code objectthereeval() arg 1 must be a string, bytes or code objectwereeval() arg 1 must be a string, bytes or code object......</code></pre><h4 id="后记："><a href="#后记：" class="headerlink" title="后记："></a>后记：</h4><p>这次用到的文本文件是截止到2010年之前的考研英语真题，其原始格式是word，内容被copy至新建的txt文件中，并且部分单词拼写有误或者“被空格中断”，所以在最后get的表格文件中有大约2%的单词拼写不准确（但还可以分辨出来），如果想要使得最后获取的表格文件更加准确，可以应用更合格(单词错误率低＋更多文本数据，比如加入2011至今的题目（这次因为没有找到合适的近八年真题文件，所以没有统计这一部分））的文本文件来进行上述处理。</p><p>这就好比机器学习中用于模型训练和测试的数据集一样，数据的真实性和准确性越高，则模型的拟合效果越好，其预测或分类能力也就越强。</p><p>当然，你也可以搜集高考单词，四六级单词等等进行同样的处理。发现没有，举一个栗子可以返好多栗子嘞</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;本文目标&quot;&gt;&lt;a href=&quot;#本文目标&quot; class=&quot;headerlink&quot; title=&quot;本文目标:&quot;&gt;&lt;/a&gt;本文目标:&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;从考研英语往年真题中提取单词，统计单词出现次数，按照频数排序并写入表格文件&lt;/strong&gt;&lt;br&gt;&lt;/
      
    
    </summary>
    
      <category term="Python自然语言处理" scheme="http://yoursite.com/categories/Python%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="文本分析＆挖掘,Python" scheme="http://yoursite.com/tags/%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90%EF%BC%86%E6%8C%96%E6%8E%98-Python/"/>
    
  </entry>
  
  <entry>
    <title>多元统计上机第六次实验之列联分析与对应分析</title>
    <link href="http://yoursite.com/2018/11/21/%E5%A4%9A%E5%85%83%E7%BB%9F%E8%AE%A1%E4%B8%8A%E6%9C%BA%E7%AC%AC%E5%85%AD%E6%AC%A1%E5%AE%9E%E9%AA%8C%E5%88%97%E8%81%94%E5%88%86%E6%9E%90%E4%B8%8E%E5%AF%B9%E5%BA%94%E5%88%86%E6%9E%90/"/>
    <id>http://yoursite.com/2018/11/21/多元统计上机第六次实验列联分析与对应分析/</id>
    <published>2018-11-21T07:39:15.000Z</published>
    <updated>2018-11-21T07:45:39.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>第一题</strong><br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">file1=read.csv(<span class="string">'C://Users//Administrator//Desktop//data//1.csv'</span>,head=<span class="literal">T</span>);</span><br></pre></td></tr></table></figure></p><p><strong>第二题</strong><br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data1=file1[,<span class="number">2</span>:<span class="number">3</span>];</span><br><span class="line">chisq.test(data1);</span><br></pre></td></tr></table></figure></p><blockquote><p>分析结果：p-value = 7.628e-06,所以拒绝原假设（不相关），说明收入高低对满意程度相关</p></blockquote><p><strong>第三题</strong><br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">file2=read.csv(<span class="string">'C://Users//Administrator//Desktop//data//2.csv'</span>);</span><br><span class="line">data2=file2[,<span class="number">2</span>:<span class="number">5</span>];</span><br><span class="line">freq_num= table (data2);<span class="comment">#统计频数</span></span><br><span class="line">prop.table(freq_num);<span class="comment">#将频数表变成频率表</span></span><br></pre></td></tr></table></figure></p><p><strong>第四题</strong><br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#首先统计双变量频数形成列联表</span></span><br><span class="line">b= table (data2$ Treatment ,data2$Sex);<span class="comment">#统计双变量频数形成列联表</span></span><br><span class="line">c=table (data2$Sex ,data2$Improved);<span class="comment">#统计双变量频数形成列联表</span></span><br><span class="line"><span class="comment">#接下来将频数表变成频率表</span></span><br><span class="line">prop.table(b);</span><br><span class="line">prop.table(c);</span><br><span class="line"><span class="comment">#最后利用列联分析说明Sex与Treatment和Improved是否相关</span></span><br><span class="line">chisq.test(b);<span class="comment">#p-value = 0.5356，说明Treatment与Sex不相关</span></span><br><span class="line">chisq.test(c);<span class="comment">#p-value = 0.08889，说明Improved与Sex也不相关</span></span><br></pre></td></tr></table></figure></p><p><strong>第五题</strong><br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">library</span>(MASS);</span><br><span class="line">file3=read.csv(<span class="string">'C://Users//Administrator//Desktop//data//3.csv'</span>);</span><br><span class="line">data3=file3[,<span class="number">2</span>:<span class="number">6</span>];</span><br><span class="line">d=corresp(data3,<span class="number">2</span>)<span class="comment">#2指选用两个因子，结果中有行列因子的得分</span></span><br><span class="line">biplot(d);<span class="comment">#绘图</span></span><br><span class="line">abline(v=<span class="number">0</span>,h=<span class="number">0</span>)<span class="comment">#加入参考线</span></span><br></pre></td></tr></table></figure></p><p><img src="1.png" alt=""> <br><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#分析：对应分析图被参考线划分为4个区域，其中，</span><br><span class="line">#(1)在不同象限表示距离较远，这里的距离是任意两者之间相关性的度量；</span><br><span class="line">#(2)位置是相对于中心的距离，距该离越小说明相对所属类别相对常见；</span><br><span class="line">#(3)角度越小，代表两者相关性越强。</span><br><span class="line">#因此我们可以得出以下结论：</span><br><span class="line"># 1.学士，高中和大专的学历相对常见，也对，一般上了高中之后要么进入university（学士学位），或者进入college(大专)</span><br><span class="line"># 2.高中和大专的相关性较之于其它比较来说相关性最强</span><br><span class="line"># 3.硕士和高中以下出现频率较低，说明高中以下学历和硕士学历两者一个处于学历链的偏上端，一个处于偏下端，这也和我们的认知相符。</span><br></pre></td></tr></table></figure></p><p><strong>第六题</strong><br><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">file4=read.csv(<span class="string">'C://Users//Administrator//Desktop//data//4.csv'</span>);</span><br><span class="line">data4=file4[,<span class="number">1</span>:<span class="number">6</span>];</span><br><span class="line">e=corresp(data4,<span class="number">2</span>)<span class="comment">#2指选用两个因子，结果中有行列因子的得分</span></span><br><span class="line">biplot(e);<span class="comment">#绘图</span></span><br><span class="line">abline(v=<span class="number">0</span>,h=<span class="number">0</span>)<span class="comment">#加入参考线</span></span><br></pre></td></tr></table></figure></p><p><img src="2.png" alt=""> <br><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#分析：</span><br><span class="line">#(1)使用小米和荣耀的人群相对常见，而使用苹果和VIVO的人群相对较少；</span><br><span class="line">#(2)使用小米和荣耀的人群之间相关性较强，说明一般使用小米的人也可能会对荣耀情有独钟，反之亦然；</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;第一题&lt;/strong&gt;&lt;br&gt;&lt;figure class=&quot;highlight r&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td clas
      
    
    </summary>
    
      <category term="R" scheme="http://yoursite.com/categories/R/"/>
    
    
  </entry>
  
  <entry>
    <title>对有序顺序表去重并保持有序状态</title>
    <link href="http://yoursite.com/2018/11/15/%E5%AF%B9%E6%9C%89%E5%BA%8F%E9%A1%BA%E5%BA%8F%E8%A1%A8%E5%8E%BB%E9%87%8D%E5%B9%B6%E4%BF%9D%E6%8C%81%E6%9C%89%E5%BA%8F%E7%8A%B6%E6%80%81/"/>
    <id>http://yoursite.com/2018/11/15/对有序顺序表去重并保持有序状态/</id>
    <published>2018-11-15T07:07:35.000Z</published>
    <updated>2018-11-15T07:12:15.000Z</updated>
    
    <content type="html"><![CDATA[<ul><li>对有序数组进行去重</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line">#define maxsize 7</span><br><span class="line">int delsame(int arr[]);</span><br><span class="line">int length=maxsize;</span><br><span class="line">void main()</span><br><span class="line">&#123;</span><br><span class="line">int s[maxsize]=&#123;1,2,2,3,4,4,7&#125;;</span><br><span class="line">delsame(s);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int delsame(int arr[])</span><br><span class="line">&#123;</span><br><span class="line">    int i=0;</span><br><span class="line">    int j;</span><br><span class="line">    for(j=0;j&lt;maxsize;j++)</span><br><span class="line">    &#123;   if(length==0)</span><br><span class="line">            return 0;</span><br><span class="line">        if(arr[i]!=arr[j])</span><br><span class="line">            &#123;</span><br><span class="line">                ++i;</span><br><span class="line">                arr[i]=arr[j];</span><br><span class="line">            &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    length=i+1;</span><br><span class="line"></span><br><span class="line">    //打印最终的结果</span><br><span class="line">    for(int m=0;m&lt;length;m++)</span><br><span class="line">        &#123;</span><br><span class="line">            printf(&quot;%d&quot;,arr[m]);</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;对有序数组进行去重&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=
      
    
    </summary>
    
      <category term="数据结构自学史" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%87%AA%E5%AD%A6%E5%8F%B2/"/>
    
    
  </entry>
  
  <entry>
    <title>二分法</title>
    <link href="http://yoursite.com/2018/11/12/%E4%BA%8C%E5%88%86%E6%B3%95/"/>
    <id>http://yoursite.com/2018/11/12/二分法/</id>
    <published>2018-11-12T03:48:38.000Z</published>
    <updated>2018-11-12T04:50:53.000Z</updated>
    
    <content type="html"><![CDATA[<p><strong>二分法</strong>是数学领域的一种求零点的算法<br><br>对于定义区间为$[a,b]$上的连续函数$f(x)$，若有$f(a)$与$f(b)$异号，根据介值定理，$f(x)=0$的解必在$[a,b]$上，酱紫，就可以通过二分运算逐步缩小$[a,b]$（也就是取其子区间）来逼近函数的零点了<br></p><p><strong>图解</strong>如下：<br><br><img src="1.jpg" alt=""> <br></p><p><strong>二分法的计算步骤</strong>如下：　<br><br>取中点$\frac{a+b}2$ <br><br>若$f(a)与f(x)$异号，则令$b=x$,否则令$a=x$ <br><br>当子区间小于给定的要求长度时，停止二分运算<br></p><p>以上是在数学问题上的求解，在计算机科学方面，又衍生出了二分查找算法，其思想一样，只是处理的是问题是离散形式的<br></p><p><strong>使用条件</strong>：<br></p><ul><li><p>待查找的序列区间单调有序（单调递增或单调递减都可以）</p></li><li><p>待查找序列和题目的要求建立的函数关系单调有序</p></li></ul><p><strong>方法</strong>：假设待查找序列和题目的要求之间的关系是单调递增的，先取区间的中心，判断该处函数值和题目标准值的大小关系，如果函数值偏小，那么应该在中心右侧的区间继续查找；如果函数值偏大 ，那么应该在中心左侧区间继续查找，直到找到对应的值或者区间缩小到左右端点之间不再包含其他数据结束。</p><p><strong>实例代码</strong>：<br><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line">int search(int arr[],int n,int key);</span><br><span class="line">void main()</span><br><span class="line">&#123;</span><br><span class="line">int arr[]=&#123;3,6,7,10,11,16,20,33,56,89&#125;;</span><br><span class="line">search(arr,10,56);</span><br><span class="line">&#125;</span><br><span class="line">int search(int arr[],int n,int key)</span><br><span class="line">&#123;</span><br><span class="line">    int low = 0;</span><br><span class="line">    int high = n-1;</span><br><span class="line">    int mid,count=0;</span><br><span class="line">    while(low&lt;=high)</span><br><span class="line">    &#123;</span><br><span class="line">    count++;</span><br><span class="line">    mid=(low+high)/2;</span><br><span class="line">    if(arr[mid]==key)</span><br><span class="line">        return mid;</span><br><span class="line">    else if (arr[mid]&lt;key)</span><br><span class="line">        low=mid;//low=mid+1;</span><br><span class="line">    else</span><br><span class="line">        high=mid;//high=mid+1;</span><br><span class="line">    &#125;</span><br><span class="line">return -1;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p><strong>查找结果</strong> <br><br><img src="2.png" alt=""> <br></p><p>参考：<a href="https://www.cnblogs.com/detrol/p/7533577.html" target="_blank" rel="noopener">https://www.cnblogs.com/detrol/p/7533577.html</a><br><a href="https://www.cnblogs.com/wanglog/p/6650695.html" target="_blank" rel="noopener">https://www.cnblogs.com/wanglog/p/6650695.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;二分法&lt;/strong&gt;是数学领域的一种求零点的算法&lt;br&gt;&lt;br&gt;对于定义区间为$[a,b]$上的连续函数$f(x)$，若有$f(a)$与$f(b)$异号，根据介值定理，$f(x)=0$的解必在$[a,b]$上，酱紫，就可以通过二分运算逐步缩小$[a,b]
      
    
    </summary>
    
      <category term="数据结构自学史" scheme="http://yoursite.com/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E8%87%AA%E5%AD%A6%E5%8F%B2/"/>
    
    
  </entry>
  
</feed>

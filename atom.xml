<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>凡希的博客</title>
  
  <subtitle>因为喜欢，所以热爱</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-11-01T09:22:04.455Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>凡希</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>收益率</title>
    <link href="http://yoursite.com/2018/10/30/%E6%94%B6%E7%9B%8A%E7%8E%87/"/>
    <id>http://yoursite.com/2018/10/30/收益率/</id>
    <published>2018-10-30T08:11:45.000Z</published>
    <updated>2018-11-01T09:22:04.455Z</updated>
    
    <content type="html"><![CDATA[<h3 id="现金流分析"><a href="#现金流分析" class="headerlink" title="现金流分析"></a>现金流分析</h3><ul><li><strong>净现金流出</strong>：$C_t=O_t-I_t$　<br></li><li>通过现金流折现对现金流进行分析的方法称为<strong>现金流折现分析</strong>　<br></li><li>对于一般的现金流$R_t=-C_t(t=0,1,2,…,n)$，在利率为$i$时有现值$V(0)=\sum_{t=0}^{n}v^tR_t$，它可正可负.<br></li></ul><h3 id="收益率"><a href="#收益率" class="headerlink" title="收益率"></a>收益率</h3><ul><li><strong>收益率</strong>就是使<code>投资支出现值</code>与<code>投资回收现值`</code>相等`的的利率.<br></li><li>计算方法：令$V(0)=\sum_{t=0}^{n}v^tR_t$中的$V(0)$等于$0$，求解得到的$i$值就是收益率.<br></li><li>收益率可正可负<br></li></ul><h3 id="再投资收益率"><a href="#再投资收益率" class="headerlink" title="再投资收益率"></a>再投资收益率</h3><p>情形一<br><br>0时刻投资一单位货币，投资期限为$n$，本金年利率为$i$.每年产生的利息按照利率$j$再投资，即利率的年利率为$j$，则投资回收值在时刻$n$的积累值为<br>$$1+i[(1+j)^{n-1}+(1+j)^{n-2}+…+(1+j)^{0}]=1+is_{n┒j}$$</p><p>情形二<br><br>在标准期末付年金中，各次付款产生的利息的再投资收益率均为$j$，即从时刻２开始，每期都有由付款本金产生的利息，由于付款的本金之和随时间推移逐年增加，因而每期所产生的利息也就逐年增加，因此，时刻$n$的积累本息和为<br>$$n+i(Is)<em>{n-1┒j}=n+i\frac{s</em>{n┒j}-n}{j}$$</p><p>而期初付年金相应的计算公式为<br>$$n+i(Is){n┒j}$$</p><h3 id="投资额基金收益率"><a href="#投资额基金收益率" class="headerlink" title="投资额基金收益率"></a>投资额基金收益率</h3><ul><li><p>一些符号的含义</p><blockquote><p>$A$：期初基金的资本量<br><br>$B$：期末基金的本息和<br><br>$I$：投资期内基金所获得的收入<br><br>$C_t$：时刻$t$投入资金或从基金中赎回的资金量，$0&lt;=t&lt;=1$<br><br>$C$：此期间注入或赎回的资金之和，即$C=\sum_tC_t$ <br></p></blockquote></li><li><p>于是，在一个期间内，我们有<br>$$B=A+C+I$$</p><p>若假设投资期内所获得的基金收入是在期末进行支付的，则有<br>$$I=iA+\sum_tC_{t } i_{1-t,t}$$<br>注意最后一个符号　<br></p><p>于是计算收益率的公式如下：</p><p>$$i=\frac{I}{A+\sum_tC_t(1-t)}$$</p><p>若假设歌各次资金的投入与赎回在$0-1$上是均匀分布的，那么有<br>$$i=\frac{I}{A+B-I}$$</p><h3 id="时间加权收益率"><a href="#时间加权收益率" class="headerlink" title="时间加权收益率"></a>时间加权收益率</h3><p>$$1+i=(1+i_1)+(1+i_2)+…+(1+i_m)$$</p><h3 id="投资组合法和投资年法"><a href="#投资组合法和投资年法" class="headerlink" title="投资组合法和投资年法"></a>投资组合法和投资年法</h3></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;现金流分析&quot;&gt;&lt;a href=&quot;#现金流分析&quot; class=&quot;headerlink&quot; title=&quot;现金流分析&quot;&gt;&lt;/a&gt;现金流分析&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;净现金流出&lt;/strong&gt;：$C_t=O_t-I_t$　&lt;br&gt;&lt;/li&gt;
&lt;li&gt;通
      
    
    </summary>
    
      <category term="金融数学" scheme="http://yoursite.com/categories/%E9%87%91%E8%9E%8D%E6%95%B0%E5%AD%A6/"/>
    
    
  </entry>
  
  <entry>
    <title>多元统计分析期中考点梳理</title>
    <link href="http://yoursite.com/2018/10/28/%E5%A4%9A%E5%85%83%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90%E6%9C%9F%E4%B8%AD%E8%80%83%E7%82%B9%E6%A2%B3%E7%90%86/"/>
    <id>http://yoursite.com/2018/10/28/多元统计分析期中考点梳理/</id>
    <published>2018-10-28T13:37:11.000Z</published>
    <updated>2018-10-29T09:23:06.593Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Part-1．"><a href="#Part-1．" class="headerlink" title="Part 1．"></a>Part 1．</h2><p><strong>１．$What’s$ <code>统计距离</code>？$What’s$ <code>统计距离</code>与<code>欧氏距离</code>各自的<code>优缺点</code>(两者之间的<code>区别</code>与<code>联系</code>)?</strong></p><p><strong>2．$k-means$的思想、方法以及过程？</strong></p><p><strong>3．$Fisher$判别法的思想、方法以及过程推导？</strong></p><p><strong>4．<code>聚类分析</code>与<code>判别分析</code>的区别与联系？</strong></p><p><strong>5．PCA的思想与方法？</strong></p><p><strong>6．FA的思想与方法？</strong></p><p><strong>7．PCA与FA的区别与联系？</strong></p><hr><h2 id="Part-2．"><a href="#Part-2．" class="headerlink" title="Part 2．"></a>Part 2．</h2><p><strong>1．利用<code>随机向量</code>的<code>均值</code>与<code>方差</code>的性质求解<code>协方差矩阵</code></strong></p><p><strong>2．通过<code>判别分析/距离判别/计算马氏距离</code>，做判别</strong></p><p><strong>3．$PCA$求解主成分</strong></p><p><strong>4．已知$FA$的结果，求解<code>共同度</code>，<code>方差贡献率</code></strong></p><hr><h2 id="Part-1-．"><a href="#Part-1-．" class="headerlink" title="$Part 1$．"></a>$Part 1$．</h2><p><strong>１．$What’s$ <code>统计距离</code>？$What’s$ <code>统计距离</code>与<code>欧氏距离</code>各自的<code>优缺点</code>(两者之间的<code>区别</code>与<code>联系</code>)?</strong> <br></p><blockquote><p>$1^。$<code>统计距离</code>是一种可以<strong>体现各个变量在变差大小上的不同</strong>以及<strong>有时存在的相关性</strong>的距离，并且它<strong>与各变量所用的单位无关.</strong><br><br>(1)<code>优点：</code>可以<strong>体现各个变量在变差大小上的不同</strong>以及<strong>有时存在的相关性</strong>的距离，并且它<strong>与各变量所用的单位无关.</strong><br><br>(2)<code>缺点：</code>夸大了变化微小的变量的作用。受协方差矩阵不稳定的影响，并不总是能顺利计算出。<br></p><p>$2^。$<code>欧氏距离</code>欧也称欧几里得度量、欧几里得度量，是一个通常采用的距离定义，它是在$m$维空间中两个点之间的真实距离。在二维和三维空间中的欧氏距离的就是两点之间的距离。<br><br>(1)优点：欧式距离在计算方法和理解上较为简单<br><br>(2)缺点：在一些统计问题上有一些缺点，比如，在欧式距离中，每个坐标的贡献是同等的，但用测量值做为坐标值计算距离时，会带有不等的随机波动，这种情况下需要对坐标进行加权。</p></blockquote><p><strong>2．$k-means$的思想、方法以及过程？</strong><br></p><blockquote><p><code>基本思想:</code>把每个样品聚集到其最近形心(均值)类中.<br><br><code>方法:</code>对于给定的样本集，按照样本之间的距离大小，将样本集划分为K个簇。让簇内的点尽量紧密的连在一起，而让簇间的距离尽量的大。<br><br><code>过程：</code>把样品粗略分成$K$个初始类$–&gt;$进行修改，逐步分派到其最近均值的类中$–&gt;$重复上一步，直到各类无元素进出<br></p></blockquote><p><strong>3．$Fisher$判别法的思想、方法以及过程推导？</strong><br></p><blockquote><p><code>思想：</code>投影，将$k$组$p$维数据投影到某一个方向，使得组与组之间的投影尽可能地分开，然后再选择合适的判别规则，将新的样品进行分类判别。<br><br><code>方法：</code><br><br><code>过程：</code><br></p></blockquote><p><strong>4．<code>聚类分析</code>与<code>判别分析</code>的区别与联系？</strong><br></p><ul><li><p>区别：<br><br>1、基本思想不同</p><blockquote><p><code>聚类分析</code>是根据研究对象特征对研究对象进行分类的一种多元分析技术,在未知各样本类别的情况下，把性质相近的个体归为一类, 使得同一类中的个体都具有高度的同质性, 不同类之间的个体具有高度的异质性；<br><br><code>判别分析</code>是对已知分类的数据建立由数值指标构成的分类规则即判别函数, 然后把这样的规则应用到未知分类的样本去分类<br></p></blockquote><p>2、研究目的不同</p><blockquote><p>虽然都是研究分类的，但在进行<code>聚类分析</code>前，<code>对总体到底有几种类型不知道</code>（研究分几类较为合适需从计算中加以调整）。<code>判别分</code>析则是<code>在总体类型划分已知</code>，对当前新样本判断它们属于哪个总体。</p></blockquote><p>3.所具有的方法不同<br></p><blockquote><p><code>聚类分析</code>分两种：$Q$型聚类（对样本的聚类），$R$型聚类（对变量的聚类）。聚类分析需要注意的是，一般小样本数据可以用系统聚类法，大样本数据一般用快速聚类法（$K$均值聚类法），当研究因素既有分类变量又有计量变量，可以用两步聚类。<br><br><code>判别分析</code>有$Fisher$判别，$Bayes$判别和逐步判别。一般用$Fisher$判别即可，要考虑概率及误判损失最小的用$Bayes$判别，但变量较多时，一般先进行逐步判别筛选出有统计意义的变量，再结合实际情况选择用哪种判别方法.</p></blockquote></li></ul><p><strong>5．$PCA$的思想与方法？</strong><br></p><blockquote><p><code>思想：</code>PCA通过线性变换将原始数据变换为一组各维度线性无关的表示，可用于提取数据的主要特征分量<br><br><code>目的：</code>为了节省计算机在进行计算时所占用的资源，在减少需要分析的指标的同时，尽量减少原指标包含信息的损失，并用随机变量的方差来代表保留信息的比重，以达到对所收集数据进行全面分析的目的。<br><br><code>方法：</code>协方差矩阵法，相关矩阵法<br><br><code>参考：</code><a href="https://blog.csdn.net/daaikuaichuan/article/details/53444639" target="_blank" rel="noopener">https://blog.csdn.net/daaikuaichuan/article/details/53444639</a></p></blockquote><p><strong>6．$FA$的思想与方法？</strong><br></p><blockquote><p><code>思想：</code>根据相关性大小把原始变量分组，使得同组内的变量之间相关性较高，而不同组的变量间的相关性则较低<br><br><code>方法：</code>求解因子载荷的方法：主成分法，主轴因子法，极大似然法，最小二乘法，a因子提取法。</p></blockquote><p><strong>7．P$CA$与$FA$的区别与联系？</strong><br></p><ul><li>1．区别<blockquote><p>(1)都是一种多维随机变量降维的方法<br><br>(2)$FA$求解初始因子时有一种方法是$PCA$ <br></p></blockquote></li><li>2．联系<blockquote><p>(1)$PCA$是以方差度量保留的主成分的，使方差尽量大，$FA$中因子与原变量间有很高的相关性，且有很轻的解释能力<br><br>(2)形式不同.$PCA$是$Y=UX$，而$FA$为$X=AF+e$ <br><br>(3)$FA$除主成分法外还有其他方法，比如极大似然法 <br><br>(4)$FA$最后要进行因子旋转 <br></p></blockquote></li></ul><hr><h2 id="Part-2．-1"><a href="#Part-2．-1" class="headerlink" title="Part 2．"></a>Part 2．</h2><p><strong>1．利用<code>随机向量</code>的<code>均值</code>与<code>方差</code>的性质求解<code>协方差矩阵</code></strong><br><br><img src="1.jpg" alt=""> <br><br><strong>2．通过<code>判别分析/距离判别/计算马氏距离</code>，做判别</strong><br><br><img src="2.jpg" alt=""> <br><br><img src="3.jpg" alt=""> <br><br><strong>3．$PCA$求解主成分</strong><br><br><img src="4.jpg" alt=""> <br><br><strong>4．已知$FA$的结果，求解<code>共同度</code>，<code>方差贡献率</code></strong><br></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Part-1．&quot;&gt;&lt;a href=&quot;#Part-1．&quot; class=&quot;headerlink&quot; title=&quot;Part 1．&quot;&gt;&lt;/a&gt;Part 1．&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;１．$What’s$ &lt;code&gt;统计距离&lt;/code&gt;？$What’s$ &lt;cod
      
    
    </summary>
    
      <category term="R" scheme="http://yoursite.com/categories/R/"/>
    
    
  </entry>
  
  <entry>
    <title>利息的基本概念</title>
    <link href="http://yoursite.com/2018/10/26/%E5%88%A9%E6%81%AF%E7%9A%84%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/"/>
    <id>http://yoursite.com/2018/10/26/利息的基本概念/</id>
    <published>2018-10-26T06:34:37.000Z</published>
    <updated>2018-10-27T06:55:47.160Z</updated>
    
    <content type="html"><![CDATA[<h2 id="知识梳理"><a href="#知识梳理" class="headerlink" title="知识梳理"></a>知识梳理</h2><h3 id="一些基本概念："><a href="#一些基本概念：" class="headerlink" title="一些基本概念："></a>一些基本概念：<br></h3><p><strong>本金</strong>：每项业务开始时投资的金额.<br><br><strong>积累值（终值）</strong>：业务在一定时间后回收到的总金额称为该时刻的积累值，与<code>本金</code>与<code>投资日算起的时间长度</code>有关.<br><br><strong>积累函数</strong>$a(t)$又叫<code>t期积累因子</code>，与之相对的是<code>t期折现因子</code>$v(t)$ ，并且有$a(t)=v^-(t)$ .<br><br><strong>现值</strong>只与该时点以后的款项有关，<strong>积累值</strong>只与该时点以前的款项有关.<br><br><strong>第$n$期利息</strong>：$I_n=A(n)-A(n-1)$，$n&gt;=1,n$是整数.　<br></p><h3 id="实际利率"><a href="#实际利率" class="headerlink" title="实际利率"></a>实际利率</h3><p>第$n$期的实际利率$i_n=\frac{I_n}{A_{n-1}.}$.<br></p><blockquote><p>单利：$a(t)=1+it$ ，$i_n=\frac{a(n)-a(n-1)}{a(n-1)}=\frac{(1+in)-(1+i(n-1))}{1-i(n-1)}=\frac{i}{1+i(n-1)}$   <br><br>复利：$a(t)=(1+i)^t$，$i_n=\frac{a(n)-a(n-1)}{a(n-1)}=\frac{(1+i)^n-(1+i))^{n-1}}{(1+i)^{n-1}.}=i$ <br></p></blockquote><p>为使得获利最大：<br><br>当$t&gt;=1$时，$(1+i)^t&gt;=1+it$，选复利.<br><br>当$t&lt;=1$时，$(1+i)^t&lt;=1+it$，选单利.<br></p><h3 id="实际贴现率"><a href="#实际贴现率" class="headerlink" title="实际贴现率"></a>实际贴现率</h3><p>定义：一个度量期内取得的利息金额与期末的投资可回收金额的比值，用$d$表示.<br><br>第$n$期的实际贴现率$d_n=\frac{I_n}{A_n}$. <br><br>公式：$v=a^-(1)=1-d$ <br></p><blockquote><p>复利场合的贴现率：$a(n)=(1+i)^n$，$d_n=\frac{a(n)-a(n-1)}{a(n)}=\frac{i}{1+i}$，又叫做<code>复贴现</code> <br><br>单利场合的贴现率：$a(n)=1+ni$，$d_n=\frac{d}{1-(n-1)d}$  <br></p></blockquote><p><strong>名义利率</strong>：设每期付$m$次利息的名义利率为$i^{(m)}$，则实际利率（每$\frac1m$期）为$\frac{i^{(m)}.}{m}$，通过$(1+i)=(1+\frac{i^{(m)}.}{m})^m$可以求得实际利率$i$. <br><br><strong>名义贴现率</strong>：通过$1-d=(1-\frac{d^{(m)}}{m})^m$可求得每期实际贴现率$d$. <br><br><strong>公式</strong>：<br>$$(1+\frac{i^{(m)}.}{m})^m=1+i=(1-\frac{d^{(p)}}{p})^{-p}=\frac1{1-d}$$　</p><h3 id="利息力"><a href="#利息力" class="headerlink" title="利息力"></a>利息力</h3><p>利息力用来度量在每一时点上的利息<br><br>公式：$a(t)=e^{\int_0^t \delta_t dt}$ <br></p><hr><p>现在，用一个公式对以上内容做一个总结，如下：<br>$$(1+\frac{i^{(m)}.}{m})^m=1+i=v^{-1}=(1-d)^{-1}=(1-\frac{d^{(p)}.}{p})^{(-p)}=e^{\delta}$$<br>注意这里的利息力(贴现力)是常数.<br><br><img src="1.jpeg" alt=""> <br></p><hr><h2 id="课本习题"><a href="#课本习题" class="headerlink" title="课本习题"></a>课本习题</h2><p>5．已知某笔投资在三年后的积累值为为1000元，第一年的利率为$i_1=$10%，第二年的利率为$i_2=$8%，第三年的利率为$i_3=$6%，求该笔投资的原始金额.<br><br>解：$A(0)(1+i_1)(1+i_2)(1+i_3)=1000–&gt;A(0)$ <br></p><p>13．假设某人在1984年7月1日投资1000元于某基金，该基金在$t$时刻的利息力为$\delta_t=\frac{3+2t}{50}$，其中$t$为距1984年1月1日的年数，求该笔投资在1985年1月1日的积累值.<br><br>解：$1000e^{\int_\frac12^1\delta_tdt}$ <br></p><p>14．某基金$A$以每月计息一次的年名义利率12%积累，基金$B$以利息强度$\delta_t=\frac{t}6$积累，在时刻0，两笔基金存入的款项相同，请确定两基金额相等的下一时刻.<br><br>解：$1.(1+\frac{0.12}{12})^{12n}=1.e^{\int_0^n\frac{t}{6}dt}–&gt;n$ <br></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;知识梳理&quot;&gt;&lt;a href=&quot;#知识梳理&quot; class=&quot;headerlink&quot; title=&quot;知识梳理&quot;&gt;&lt;/a&gt;知识梳理&lt;/h2&gt;&lt;h3 id=&quot;一些基本概念：&quot;&gt;&lt;a href=&quot;#一些基本概念：&quot; class=&quot;headerlink&quot; title=&quot;一些基
      
    
    </summary>
    
      <category term="金融数学" scheme="http://yoursite.com/categories/%E9%87%91%E8%9E%8D%E6%95%B0%E5%AD%A6/"/>
    
    
  </entry>
  
  <entry>
    <title>求极限的一般方法</title>
    <link href="http://yoursite.com/2018/10/26/%E6%B1%82%E6%9E%81%E9%99%90%E7%9A%84%E4%B8%80%E8%88%AC%E6%96%B9%E6%B3%95/"/>
    <id>http://yoursite.com/2018/10/26/求极限的一般方法/</id>
    <published>2018-10-26T05:48:37.000Z</published>
    <updated>2018-10-26T05:53:33.373Z</updated>
    
    <content type="html"><![CDATA[<p>1、利用初等函数的连续性求极限；</p><p>2、利用极限的运算法则求极限；</p><p>3、利用左右极限求极限；</p><p>4、利用两个重要极限求极限；</p><p>5、利用无穷小与有界量的积为无穷小的性质求极限；</p><p>6、利用等价无穷小代换求极限；</p><p>7、利用单调有界性准则求极限；</p><p>8、利用夹逼准则求极限；</p><p>9、利用中值定理求极限；</p><p>10、利用洛必达法则求极限；</p><p>11、用定积分求极限；</p><p>12、利用泰勒公式求极限；</p><p>13、利用数项收敛的必要性求极限。</p><p><img src="timg.gif" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;1、利用初等函数的连续性求极限；&lt;/p&gt;
&lt;p&gt;2、利用极限的运算法则求极限；&lt;/p&gt;
&lt;p&gt;3、利用左右极限求极限；&lt;/p&gt;
&lt;p&gt;4、利用两个重要极限求极限；&lt;/p&gt;
&lt;p&gt;5、利用无穷小与有界量的积为无穷小的性质求极限；&lt;/p&gt;
&lt;p&gt;6、利用等价无穷小代换求极限；&lt;
      
    
    </summary>
    
      <category term="Mathematic Analysis" scheme="http://yoursite.com/categories/Mathematic-Analysis/"/>
    
    
  </entry>
  
  <entry>
    <title>关于０/０型极限未定式的一些疑惑</title>
    <link href="http://yoursite.com/2018/10/25/%E5%85%B3%E4%BA%8E%EF%BC%90-%EF%BC%90%E5%9E%8B%E6%9E%81%E9%99%90%E6%9C%AA%E5%AE%9A%E5%BC%8F%E7%9A%84%E4%B8%80%E4%BA%9B%E7%96%91%E6%83%91/"/>
    <id>http://yoursite.com/2018/10/25/关于０-０型极限未定式的一些疑惑/</id>
    <published>2018-10-25T13:26:17.000Z</published>
    <updated>2018-10-26T05:49:54.408Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>在进入正题之前，先来看一道题目. <br></p></blockquote><p><strong>[引例]</strong>　设$f(0)=0$，且$\lim_{h\rightarrow0}\frac{f(1-cos h)}{h^2}$存在，则$f’(0)$存在吗?<br><br><strong>解</strong>：<br><br>$\lim_{h\rightarrow0}\frac{f(1-cos h)}{h^2}=\lim_{h\rightarrow0}\frac{f(0+(1-cos h))-f(0)}{h^2}=\frac12\ lim_{h\rightarrow0}\frac{f(0+(1-cos h))-f(0)}{\frac{h^2}{2}}=lim_{h\rightarrow0}\frac{f(0+(1-cos h))-f(0)}{(1-cos h)-0}$<br>令$1-cos h=t$,<br><br>则当$h\rightarrow0$时，$cos h\rightarrow1^-$(因为$|cos h|&lt;=1$，所以只能从1的左侧趋近于１)，从而$1-cos h\rightarrow0^+$，即$t\rightarrow0^+$ <br><br>所以$\lim_{h\rightarrow0}\frac{f(1-cos h)}{h^2}=lim_{h\rightarrow0}\frac{f(0+(1-cos h))-f(0)}{(1-cos h)-0}=lim_{t\rightarrow0^+}\frac{f(0+t)-f(0)}{t-0}$存在（这是题干已知条件） <br><br>由极限的定义可知，$f(x)$在$x=0$处的右极限存在，即$f^{‘+}(0)$存在<br><br>但是，$f(x)$在$x=0$处的左极限是否存在<br>呢？根据当前已知的条件，我们无从得知，从而就不能确定$f(x)$在$x=0$处的导数是否存在（这里是根据导数的定义：只有当左右导数同时存在且相等时，才能确定该函数在该点的导数是存在的）.<br></p><blockquote><p>看完上面的引例，来进入正题！<br></p></blockquote><p><strong>[题目]</strong>计算极限$\lim_{x\rightarrow0}\frac{1-cosx\sqrt{cosx}}{x^2}$ <br><br><strong>解</strong>　<br><br>$\lim_{x\rightarrow0}\frac{1-cosx\sqrt(cosx)}{x^2}=\lim_{x\rightarrow0}\frac{(1-cosx)+(cosx-cosx\sqrt{cosx})}{x^2}=\lim_{x\rightarrow0}\frac{1-cosx}{x^2}+\lim_{x\rightarrow0}\frac{cosx(1-\sqrt{consx})}{x^2}$ <br><br>第一部分直接利用等价无穷小，得$\frac12$.<br><br>第二部分利用极限的四则运算法则，有<br><br>$\lim_{x\rightarrow0}\frac{cosx(1-\sqrt{cosx})}{x^2}=\lim_{x\rightarrow0}{cosx}.\lim_{x\rightarrow0}\frac{1-\sqrt{cosx}}{x^2}\=1.\lim_{x\rightarrow0}{\frac{1-cosx}{x^2(1+\sqrt{cosx})}=\lim_{x\rightarrow0}(\frac1{1+\sqrt{cosx})}\lim_{x\rightarrow0}\frac{1-cosx}{x^2}}\=\frac12{\lim_{x\rightarrow0}\frac{1-cosx}{x^2}}=\frac14$ <br><br>所以最终结果为$\frac12+\frac14=\frac34$ <br></p><p>现在，我们采用另外一种方法求解上题<br><br>$\lim_{x\rightarrow0}\frac{1-cosx\sqrt{cosx}}{x^2}=\frac12\lim_{x\rightarrow0}\frac{1-cosx\sqrt{cosx}}{1-cosx}$ <br><br>到这里一切正常<br><br>继续<br><br>令$cosx=t$，由于$x\rightarrow0$时，$cosx\rightarrow1^-$（因为｜cosx｜&lt;=1,所以只能从１的左侧趋近于１），即$t\rightarrow1^-$，从而上式可化为<br><br>$$\frac12\lim_{t\rightarrow1^-}\frac{1-t\sqrt{t}}{1-t}=\frac12\lim_{t\rightarrow1^-}\frac{1-t^{\frac32}}{1-t}$$ <br><br>最后利用洛必达法则计算得到结果为$\frac34$　<br></p><blockquote><p><strong>我的疑问</strong>：这里只计算出来了左极限存在为一个数，右极限不确定，类比刚开始的引例，那么以上过程是错误的吗？<br><br>这里的$\frac{1-t^{\frac32}}{1-t}$是初等函数，书上说初等函数在其定义区间（定义域内的任意一小区间，包含于定义域）上连续，而此函数的定义域为$cosx&gt;=0且cosx!=1$，也就是$t&gt;=0且t!=1$ 。<br><br>现在$t\rightarrow1^-$,而$1$是断点，不在定义区间上。（<strong>解惑：极限是在去心邻域定义的，这个不影响</strong>）<br></p></blockquote><blockquote><p>解惑：以上过程没毛病，当$t\rightarrow1^-$时，$x$可以从两侧趋近于０，$x$的双侧极限值都等于$t$的单侧极限值，可以把$x$的两侧极限都求解一下来验证这一点.：<br><br>利用第一种解法中的一步，即$\frac12{\lim_{x\rightarrow0}\frac{1-cosx}{x^2}}$，不论$x\rightarrow0^+$还是$x\rightarrow0^-$，由于它是偶函数，所以结果都是一样的。</p></blockquote><p>一般的，我们需要讨论左右侧极限的题目类型如下：<br></p><blockquote><p>1.分段函数，无论连续不连续，都一定得分左右证明 <br><br>2.连续性问题，证明连续性<br><br>3.定积分时，若是广义积分、暇积分，不得不考虑单侧极限。是积分积出来之后才考虑单侧极限。<br></p></blockquote><p>所以，快滚去看课本！</p><p><img src="1.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;在进入正题之前，先来看一道题目. &lt;br&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;[引例]&lt;/strong&gt;　设$f(0)=0$，且$\lim_{h\rightarrow0}\frac{f(1-cos h)}{h^2}$存在，则
      
    
    </summary>
    
      <category term="Mathematic Analysis" scheme="http://yoursite.com/categories/Mathematic-Analysis/"/>
    
    
  </entry>
  
  <entry>
    <title>C语言基础之数据类型与输入/输出</title>
    <link href="http://yoursite.com/2018/10/15/C%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80%E4%B9%8B%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E4%B8%8E%E8%BE%93%E5%85%A5-%E8%BE%93%E5%87%BA/"/>
    <id>http://yoursite.com/2018/10/15/C语言基础之数据类型与输入-输出/</id>
    <published>2018-10-15T14:58:33.000Z</published>
    <updated>2018-10-16T03:21:00.782Z</updated>
    
    <content type="html"><![CDATA[<h1 id="数据在计算机内部的表示"><a href="#数据在计算机内部的表示" class="headerlink" title="数据在计算机内部的表示"></a>数据在计算机内部的表示</h1><h2 id="常用的进位制"><a href="#常用的进位制" class="headerlink" title="常用的进位制"></a>常用的进位制</h2><p>R进制数只采用R个基本数字符号，R称为数制的<code>基数</code>，数制中每一固定位置对应的单位值称为<code>权</code>，进位原则是<code>逢R进1</code>.<br><br>比如二进制，那么基数就是２，数字符号就是0,1<br><br>再比如八进制，则基数为８，数字符号为0,1,…,7(共８位).<br><br><strong>注意</strong>在16进制中，基数为16很自然，但是数字符号应该是0,1,2,…,15，然而以为数只能到9，所以16进制的10用A来表示，11用B来表示，这样子一直下去，到15用F表示.<br></p><h2 id="数值与字符在计算机中的表示"><a href="#数值与字符在计算机中的表示" class="headerlink" title="数值与字符在计算机中的表示"></a>数值与字符在计算机中的表示</h2><h3 id="计算机中数值型数据的编码"><a href="#计算机中数值型数据的编码" class="headerlink" title="计算机中数值型数据的编码"></a>计算机中数值型数据的编码</h3><ul><li>机器数与真值</li></ul><p>通常一个数的最高位为<code>符号位</code>，用<code>0表示正，1表示负</code>，称为<code>数符</code>　<br><br>若机器的字长为8位，则$D_7$为符号位，$D_6$到$D_0$为数值位，比如$(11100001)<em>2=(-97)</em>{10}$ <br><br><strong>机器数</strong>：<code>机器内存放的正负符号数码化的数</code> <br><br>比如真值$(-1100001)_2$的机器数是11100001 <br><br>另外为了运算方便，机器数中的负数有3种表示方法：<code>原码</code>，<code>补码</code>，<code>反码</code>　<br></p><ul><li>定点数和浮点数</li></ul><p>定点数分为<code>定点整数</code>和<code>定点小数</code>，利用浮点数可以扩大实数的表示范围，即<code>科学计数法</code>. <br></p><h3 id="计算机中字符型数据的编码"><a href="#计算机中字符型数据的编码" class="headerlink" title="计算机中字符型数据的编码"></a>计算机中字符型数据的编码</h3><ul><li><p>ASCII码</p></li><li><p>汉字编码</p></li></ul><h1 id="字符集和保留字"><a href="#字符集和保留字" class="headerlink" title="字符集和保留字"></a>字符集和保留字</h1><h2 id="基本符号集"><a href="#基本符号集" class="headerlink" title="基本符号集"></a>基本符号集</h2><ul><li><p>数字10个 <br><br>0,1,2,…,9 <br></p></li><li><p>大小写字母各26个　<br></p></li><li><p>下划线</p></li><li>特殊符号</li></ul><h2 id="标识符"><a href="#标识符" class="headerlink" title="标识符"></a>标识符</h2><ul><li>用户标识符</li></ul><p>即用户自定义的变量名字，要求由<code>字母，数字，下划线</code>组成，且<code>不能以数字开头</code>，<code>区分大小写</code>　<br></p><ul><li>保留字</li></ul><p>自带的，比如<code>char,int</code>等，总计32个　<br></p><ul><li>预定义标识符</li></ul><p><code>#define</code>和<code>#include</code> 之类的，挺有用的　<br></p><h1 id="C语言的数据类型"><a href="#C语言的数据类型" class="headerlink" title="C语言的数据类型"></a>C语言的数据类型</h1><ul><li>基本类型</li></ul><p>整型(int，short，long)+实型(float，double)+字符型(char)<br></p><ul><li>构造类型</li></ul><p>数组类型＋结构体类型(struct)+共用体类型(union)+枚举类型(enum) <br></p><ul><li>指针类型</li></ul><p>指针类型(*) <br></p><ul><li>空类型</li></ul><p>空类型(void)</p><h1 id="常量"><a href="#常量" class="headerlink" title="常量"></a>常量</h1><h2 id="数值常量"><a href="#数值常量" class="headerlink" title="数值常量"></a>数值常量</h2><ul><li>整型常量　<br></li></ul><p>八进制整型常量必须以0开头，数码取0~7，通常是无符号数<br><br>十六进制整型常量的前缀为0X或0x，数码取值0~9，A~F或a~f <br><br>十进制整型常量无前缀，数码为0~9 <br></p><ul><li>实型常量</li></ul><p>也就是我们所说的浮点数　<br><br>小数形式＋指数形式(科学计数法)</p><h2 id="字符常量和字符串常量"><a href="#字符常量和字符串常量" class="headerlink" title="字符常量和字符串常量"></a>字符常量和字符串常量</h2><ul><li>字符常量</li></ul><p>用<code>单引号</code>括住，可作为整数常量来处理(指ASCII值)，<code>a=&#39;a&#39;+1=97+1=98</code>　<br><br>还有一些不可显示的字符，如换行，回车等，以反斜杠开头　<br><br>注意转移字符的使用，比如<code>&#39;\&quot;&#39;</code>表示双引号，<code>&#39;\\&#39;</code>表示反斜杠</p><ul><li>字符串常量</li></ul><p>用<code>双引号</code>括住，比如”I love China!”<br><br>注意转义字符的使用：比如<code>he said he is a &quot;super man&quot;</code>可以表示为<code>&quot;he said he is a \&quot;super man \&quot;&quot;</code>，程序如下：<br><br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"stdio.h"</span></span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"he said he is a \"super man\""</span>);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>输出结果：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">he said he is a <span class="string">"super man"</span></span><br></pre></td></tr></table></figure></p><p>不同于字符常量的是，字符串常量以<code>&#39;\0&#39;</code>结束，但不显示<code>&#39;\0&#39;</code>，从而C语言中字符串的长度不受限制<br></p><h2 id="符号常量"><a href="#符号常量" class="headerlink" title="符号常量"></a>符号常量</h2><p><code>#define PI 3.1415926</code>诸如此类，必须<code>大写</code></p><h1 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h1><p>变量3要素：<code>变量名＋变量的存储单元＋变量(存储单元存放)的值</code><br></p><h2 id="变量的定义"><a href="#变量的定义" class="headerlink" title="变量的定义"></a>变量的定义</h2><p>定义变量3要素：<strong>存储类型(auto,static)+数据类型＋变量名</strong>　<br></p><p>通常我们省略的存储类型，默认是<code>auto</code>，即都会把变量存放在<code>动态存储区</code>（栈区），这种变量在程序运行过程中实时分配和释放.<br><br>而<code>static</code>定义的变量的存储方式是<code>静态存储</code>的，<code>静态局部变量是放在静态存储区的</code>，在整个程序运行期间都不释放，跟全局变量一样长期占用内存。但是静态局部变量和全局变量还是不一样的，比如说，静态局部变量只能在所定义的函数内引用，静态局部变量在函数调用结束后是仍然存在的，但不能被其他函数引用。<strong>静态局部变量是在编译时赋初值的，即只赋初值一次，在程序运行时它已经有了初值，以后每次调用函数时不再对其重新复制，而只是保留上次函数调用结束时的值。</strong>在定义静态局部变量时，如不赋初值，则编译时自动赋初值为0。有时在程序设计中定义一个全局变量，只限于被所在源文件引用，而不想让其他文件引用，则可在定义全局变量前面加static，即静态全局变量。(此段引用自<a href="https://blog.csdn.net/he__yuan/article/details/79477882" target="_blank" rel="noopener">https://blog.csdn.net/he__yuan/article/details/79477882</a>)　<br></p><ul><li>整型变量</li></ul><p>基本型<code>int</code>,占4字节<br><br>短整型<code>short int</code> 或者<code>short</code>，占２字节　<br><br>长整型<code>long int</code> 或者<code>long</code>，占4字节，与基本型一样　<br><br>无符号型<code>unsigned</code></p><p>对于无符号型，由于没有符号位，所以不能表示负数　<br></p><ul><li>实型变量</li></ul><p><code>单精度类型</code>(float，占4字节)和<code>双精度类型</code>(double，占8字节) <br></p><p>-　字符变量</p><p><code>char</code>，占1字节，每个字符只能防放一个字符，由于C语言中没有字符串变量，如果需要把字符串放在变量中，就需要一个字符型数组来实现<br><br>比如下面这各个，我将<code>字符串常量</code>“china”赋给a这个<code>变量</code>，由于上述原因，a只能接受一个字符，故产生异常:<br><br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"stdio.h"</span></span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line"><span class="keyword">char</span> a=<span class="string">"china"</span>;</span><br><span class="line"></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"%d"</span>,a);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>输出：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">p0.c: In function ‘main’:</span><br><span class="line">p0.c:<span class="number">4</span>:<span class="number">8</span>: warning: initialization makes integer from pointer without a cast [-Wint-conversion]</span><br><span class="line"> <span class="keyword">char</span> a=<span class="string">"china"</span>;</span><br><span class="line">        ^~~~~~~</span><br></pre></td></tr></table></figure></p><p>而将”china”这个字符串改为字符’a’,再次运行：　<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"stdio.h"</span></span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">char</span> a = <span class="string">'a'</span>;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"%d"</span>, a);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>就能正常输出’a’的ASCII码的值了，即97.<br></p><h2 id="变量的存储"><a href="#变量的存储" class="headerlink" title="变量的存储"></a>变量的存储</h2><p><strong>存储某变量的内存空间的首地址称为变量的地址</strong>，变量地址用<code>&amp;变量名</code>来表示,<code>&amp;</code>表示取地址，即取存储空间首地址<br><br><strong>注意</strong>：虽然地址的值在形式上和整型数一样，也可以用十进制或者十六进制表示，但意义不一样.整型数有算数，关系，逻辑等运算，而<code>地址只有有限的算数运算和关系运算</code>.<br><br>还有，<code>地址的值一定是一个整型量</code>.</p><h2 id="变量的初始化"><a href="#变量的初始化" class="headerlink" title="变量的初始化"></a>变量的初始化</h2><p>直接举个栗子：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a,b;</span><br><span class="line">a=<span class="number">1</span>;</span><br><span class="line">b=<span class="number">2</span></span><br></pre></td></tr></table></figure></p><p>等价于<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a=<span class="number">1</span>,b=<span class="number">2</span>;</span><br></pre></td></tr></table></figure></p><h1 id="数据的输入和输出"><a href="#数据的输入和输出" class="headerlink" title="数据的输入和输出"></a>数据的输入和输出</h1><p>C语言的所有输入和输出操作都是调用<code>标准I/O库函数</code>　<br></p><h2 id="格式输入-输出函数"><a href="#格式输入-输出函数" class="headerlink" title="格式输入/输出函数"></a>格式输入/输出函数</h2><ul><li>格式输入函数</li></ul><p><code>printf(格式控制字符串,输出项表)</code></p><ul><li>格式输入函数</li></ul><p><code>scanf(格式控制字符串,输入项地址表)</code></p><h2 id="字符输入-输出函数"><a href="#字符输入-输出函数" class="headerlink" title="字符输入/输出函数"></a>字符输入/输出函数</h2><ul><li>字符输出函数</li></ul><p><code>putchar(ch)</code>，<code>ch</code>可以使整型变量或者字符型变量.<br><br>这将输出字符，比如：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"stdio.h"</span></span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">char</span> a = <span class="string">'a'</span>;</span><br><span class="line">    <span class="keyword">int</span> b=<span class="number">97</span>;</span><br><span class="line">    <span class="built_in">putchar</span>(<span class="string">'\n'</span>);</span><br><span class="line">    <span class="built_in">putchar</span>(a);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>这将输出两个a，各占一行，即<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a</span><br><span class="line">a</span><br></pre></td></tr></table></figure></p><p>提示：97是’a’的ASCII码.　<br></p><ul><li>字符输入函数</li></ul><p><code>getchar()</code>，无参数<br></p><p>举个例子就清楚了：<br><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"stdio.h"</span></span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   <span class="keyword">char</span> a ;</span><br><span class="line">   a=getchar();</span><br><span class="line">   <span class="built_in">putchar</span>(a);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>输入a，程序将输出a<br></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;数据在计算机内部的表示&quot;&gt;&lt;a href=&quot;#数据在计算机内部的表示&quot; class=&quot;headerlink&quot; title=&quot;数据在计算机内部的表示&quot;&gt;&lt;/a&gt;数据在计算机内部的表示&lt;/h1&gt;&lt;h2 id=&quot;常用的进位制&quot;&gt;&lt;a href=&quot;#常用的进位制&quot; cla
      
    
    </summary>
    
      <category term="C" scheme="http://yoursite.com/categories/C/"/>
    
    
  </entry>
  
  <entry>
    <title>求解线性方程组的直接方法</title>
    <link href="http://yoursite.com/2018/10/14/%E6%B1%82%E8%A7%A3%E7%BA%BF%E6%80%A7%E6%96%B9%E7%A8%8B%E7%BB%84%E7%9A%84%E7%9B%B4%E6%8E%A5%E6%96%B9%E6%B3%95/"/>
    <id>http://yoursite.com/2018/10/14/求解线性方程组的直接方法/</id>
    <published>2018-10-14T08:54:44.000Z</published>
    <updated>2018-10-14T08:54:45.010Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>考研英语词汇漫漫路</title>
    <link href="http://yoursite.com/2018/10/14/%E8%80%83%E7%A0%94%E8%8B%B1%E8%AF%AD%E8%AF%8D%E6%B1%87/"/>
    <id>http://yoursite.com/2018/10/14/考研英语词汇/</id>
    <published>2018-10-14T06:02:19.000Z</published>
    <updated>2018-10-17T14:15:49.527Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">博客将按照学习进度来更新，加油吧少年！---------To myself</span><br></pre></td></tr></table></figure><h2 id="Unit-1"><a href="#Unit-1" class="headerlink" title="Unit 1"></a>Unit 1</h2><ul><li><p>accord</p><blockquote><p>1.accord <em>v.符合，给予，何意　n.和谐；according to …是根据的意思</em><br><br>2.core<em>n.核</em><br><br>3.record<br><br>4.recoding<br><br>5.recorder<br></p></blockquote></li><li><p>take</p><blockquote><p>mistake</p></blockquote></li></ul><ul><li><p>use</p><blockquote><p>1.use<br><br>2.useful<br><br>3.misuse<br><br>4.user<br></p></blockquote></li><li><p>example</p><blockquote><p>1.sample　<em>n.样本</em><br><br>2.exemplify　<em>v.例证</em><br></p></blockquote></li><li><p>work</p><blockquote><p>1.homework<br><br>2.workforce <em>v.劳动力</em><br><br>3.framework　<em>n.框架</em><br><br>4.network　<em>n.网络</em><br><br>5.teamwork<br></p></blockquote></li><li><p>time</p><blockquote><p>1.temporary–&gt;contemporary　<em>adj.同时的，现代的</em><br><br>2.chronic <em>n.慢性病</em>–&gt;chronicle　<em>n.编年史</em>；synchronize　<em>v.同步</em><br></p></blockquote></li><li><p>through</p><blockquote><p>throughout　<em>始终，adv.各处，prep.历来</em><br></p></blockquote></li><li><p>give</p><blockquote><p>1.giver<br><br>2.gift<br><br>3.donate<br><br>4.gifted<br><br>5.given <em>prep.(表示原因)考虑到</em><br> </p></blockquote></li><li><p>suggest</p><blockquote><p>1.suggestion<br><br>2.register<br><br>3.digest　<em>v.消化</em><br><br>4.digestive　<em>adj.消化的</em><br><br>5.digestion　<em>n.消化</em><br></p></blockquote></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">axx结构表强调</span><br></pre></td></tr></table></figure><h2 id="Unit-2"><a href="#Unit-2" class="headerlink" title="Unit 2"></a>Unit 2</h2><ul><li><p>author</p><blockquote><p>1.authority <em>n. 权威（机构）</em><br><br>2.authorize <em>v.使…权威化，授权</em><br><br>3.authentic <em>adj.真实的</em>–&gt;authenticity <em>n.真伪</em><br></p></blockquote></li><li><p>social </p><blockquote><p>1.society–&gt;sociologist<br><br>2.association <em>n. 协会</em><br><br>3.associate　<em>v.关联，n.联想，adj.副的，比如副教授就是associate professor</em><br><br>4.socially <em>社交上</em><br></p></blockquote></li><li><p>just</p><blockquote><p>1.justice<em>n.正义，大法官，司法</em><br><br>2.justify<em>v.使…公平</em><br><br>3.justification<br><br>4.adjust <em>调整，适应，adjust to 是适应</em><br><br>5.adjustment <em>n. 调整</em><br><br>6.just <em>adj.公平，正义</em><br></p></blockquote></li><li><p>text</p><blockquote><p>1.context  <em>n.上下文</em><br><br>2.tissue <em>n.(身体）组织</em><br></p></blockquote></li><li><p>state <em>n.状态，国家</em></p><blockquote><p>1.statesman <em>n.政治家（为国家谋利益），而plitician是政客（为自己谋利益）</em><br><br>2.statistic <br><br>3.statistacal<br><br>4.statement–&gt;under/overstatement <em>言(不)过其实</em><br><br>5.statistics <em>n.统计学，统计数据</em></p></blockquote></li><li><p>result</p><blockquote><p>1.consult–&gt;consulting<br><br>2.consultant <em>n.咨询者</em><br></p></blockquote></li><li><p>idea</p><blockquote><p>1.ideal<br><br>2.idealize<br><br>3.idealism<br><br>4.idealist<br><br>5.ideally<br><br>6.ideological <em>adj.思想的–&gt;ideology</em><br></p></blockquote></li><li><p>point</p><blockquote><p>1.wiewpoint==view<br><br>2.appoint <em>v.委派</em><br><br>3.appointment<br></p></blockquote></li></ul><h2 id="Unit-3"><a href="#Unit-3" class="headerlink" title="Unit 3"></a>Unit 3</h2><ul><li>part</li></ul><blockquote><p>1.partial adj.部分的<br><br>2.impartial  意思是“全面的”，前缀-im代表反义，等同于in，这里有一个规定，当-im后面的单词为b/m/p时，要把in改为im,但并不是绝对的，比如unbelievable<br><br>3.partially 部分地<br><br>4.partly 部分地<br><br>5.particular 特别的<br><br>6.particularly表递进，等于especially<br><br>7.<strong>counterpart</strong> 对应物，对应者，特朗普＆习近平就是这么个对应关系<br><br>8.party 聚会，党派,方面，比如the third party 表示第三方<br><br>9.partner　伙伴，伴侣/同性恋<br><br>10.apart 隔开，距离，right here waiting ,此情可待<br><br>11.apartment 住的公寓<br><br>12.department 部门，比如NYPD<br><br>13.depart v.分开，离开；departure n.启程<br></p></blockquote><ul><li>problem</li></ul><blockquote><p>1.question,作为动词表示询问，质疑<br><br>2.require,requirement，要求<br><br>3.request,请求<br><br>4.acquire,acquisition，获得，收购<br><br>5.inquire,inquiry，询问<br></p></blockquote><ul><li>believe 认为<br></li><li>human</li></ul><blockquote><p>词根–hum表示泥土<br><br>1.humanity<br><br>2.humid adj.潮湿的<br><br>3.humidity n.湿度<br><br>4.humble adj.卑微的，谦卑的<br><br>5.humorous adj.幽默的<br></p></blockquote><ul><li>limit</li></ul><blockquote><p>1.limitation　n.限制<br><br>2.eliminate　n.根除，如eliminate poverty，消除贫困<br><br>3.preliminary adj.预选的<br></p></blockquote><ul><li>view</li></ul><blockquote><p>1.review v.复习，评论(冷静思考后的评论)，所以reviewer是指评论家<br><br>2.interview，inter指相互，比如international就是国家与国家相互的，即“国际的”，interview表示面试，还可以采访(interviewee是被面试者，interviewer是面试官，采访者)<br></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span cla
      
    
    </summary>
    
      <category term="考研英语" scheme="http://yoursite.com/categories/%E8%80%83%E7%A0%94%E8%8B%B1%E8%AF%AD/"/>
    
    
      <category term="英语" scheme="http://yoursite.com/tags/%E8%8B%B1%E8%AF%AD/"/>
    
  </entry>
  
  <entry>
    <title>多元统计第三次上机试验报告</title>
    <link href="http://yoursite.com/2018/10/12/%E5%A4%9A%E5%85%83%E7%BB%9F%E8%AE%A1%E7%AC%AC%E4%B8%89%E6%AC%A1%E4%B8%8A%E6%9C%BA%E8%AF%95%E9%AA%8C%E6%8A%A5%E5%91%8A/"/>
    <id>http://yoursite.com/2018/10/12/多元统计第三次上机试验报告/</id>
    <published>2018-10-12T04:13:24.000Z</published>
    <updated>2018-10-12T04:17:21.207Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data=iris[,1:4]</span><br><span class="line">dist.e=dist(data)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model1=hclust(dist.e)</span><br><span class="line">plot(model1)</span><br><span class="line">result0=cutree(model1,k=3)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">model2=kmeans(data,centers=3)</span><br><span class="line">model2</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">library(MASS)</span><br><span class="line">xxx=iris[,1:4]</span><br><span class="line">yyy=iris[,5]</span><br><span class="line">model2=lda(yyy~.,data=xxx,prior=c(1,1,1)/3)</span><br><span class="line">model2</span><br></pre></td></tr></table></figure><h1 id="附加题"><a href="#附加题" class="headerlink" title="附加题"></a>附加题</h1><p><strong>Prepare:首先将两个附件中数据提取到csv文件中，分别命名为file1.csv，file2.csv</strong></p><h2 id="1-利用附件1中数据对各省进行聚类分析，说明应分成几类，各类包括哪些省份"><a href="#1-利用附件1中数据对各省进行聚类分析，说明应分成几类，各类包括哪些省份" class="headerlink" title="1.利用附件1中数据对各省进行聚类分析，说明应分成几类，各类包括哪些省份."></a>1.利用附件1中数据对各省进行聚类分析，说明应分成几类，各类包括哪些省份.</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#read file1</span><br><span class="line">file1=read.csv(&apos;/home/fantasy/Desktop/task3/file1.csv&apos;)</span><br><span class="line">x=file1[,2:9]</span><br><span class="line">x</span><br></pre></td></tr></table></figure><blockquote><p>方法１：系统聚类（层次聚类）</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#calculate distance</span><br><span class="line">d=dist(x)</span><br><span class="line">#cluster</span><br><span class="line">hc=hclust(d)</span><br><span class="line">#plot</span><br><span class="line">plot(hc,hang=-1)</span><br></pre></td></tr></table></figure><p>若指定聚类数k和分类高度h,令k=3，h=200:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;result:&quot;)</span><br><span class="line">cutree(hc,k=3,h=200)</span><br></pre></td></tr></table></figure></p><blockquote><p>方法２：k-均值聚类</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kmeans(x,center=3,iter.max=100,nstart=2333)</span><br></pre></td></tr></table></figure><h2 id="2-利用附件2中数据对广东和西藏进行判别，说明其属于哪个类别"><a href="#2-利用附件2中数据对广东和西藏进行判别，说明其属于哪个类别" class="headerlink" title="2.利用附件2中数据对广东和西藏进行判别，说明其属于哪个类别."></a>2.利用附件2中数据对广东和西藏进行判别，说明其属于哪个类别.</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">file2=read.csv(&apos;/home/fantasy/Desktop/task3/file2.csv&apos;)</span><br><span class="line">xx=file2[1:29,2:9]</span><br><span class="line">yy=file2[1:29,10]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">D=dist(xx)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">library(MASS)</span><br><span class="line">l=lda(yy~.,data=file2[1:29,2:9])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot(l)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#make prediction</span><br><span class="line">predict(l,file2[30:31,2:9])</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;
      
    
    </summary>
    
      <category term="R" scheme="http://yoursite.com/categories/R/"/>
    
    
  </entry>
  
  <entry>
    <title>原来你是这样的薛之谦</title>
    <link href="http://yoursite.com/2018/10/06/xzq/"/>
    <id>http://yoursite.com/2018/10/06/xzq/</id>
    <published>2018-10-06T05:10:04.000Z</published>
    <updated>2018-10-06T05:57:57.311Z</updated>
    
    <content type="html"><![CDATA[<p>本文对薛之谦的部分歌词进行了文本挖掘处理，欢迎围观~</p><blockquote><p>目标：绘制词云图，分析词频并可视化</p></blockquote><h2 id="文本预处理"><a href="#文本预处理" class="headerlink" title="文本预处理"></a>文本预处理</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">from</span> wordcloud <span class="keyword">import</span> WordCloud</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#读取原始文本文件</span></span><br><span class="line">file=open(<span class="string">'/home/fantasy/Desktop/xzq/薛之谦.txt'</span>)</span><br><span class="line"><span class="comment">#去掉制表符和换行符</span></span><br><span class="line">c=[]</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> file:</span><br><span class="line">    line=line.strip(<span class="string">'\n'</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="string">'\t'</span> <span class="keyword">not</span> <span class="keyword">in</span> line:</span><br><span class="line">        c.append(line)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#去掉非中文字符串</span></span><br><span class="line">d=[]</span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> c:</span><br><span class="line">    s=re.sub(<span class="string">"[A-Za-z0-9\!\%\[\]\,\。]"</span>, <span class="string">""</span>, s)</span><br><span class="line">    d.append(s)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#去掉空字符串</span></span><br><span class="line">dd=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> d:</span><br><span class="line">    <span class="keyword">if</span> i <span class="keyword">is</span> <span class="keyword">not</span> <span class="string">''</span> <span class="keyword">and</span> <span class="string">' '</span> <span class="keyword">and</span> <span class="string">' '</span> :</span><br><span class="line">        dd.append(i)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#对文本进行分词处理</span></span><br><span class="line">words=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> dd:</span><br><span class="line">    w=jieba.cut(i)</span><br><span class="line">    word=<span class="string">','</span>.join(w)</span><br><span class="line">    words.append(word)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">words[<span class="number">0</span>:<span class="number">10</span>]</span><br></pre></td></tr></table></figure><pre><code>[&apos;\ufeff,夜深人静, ,那,是,爱情&apos;, &apos;偷偷地,控制,着,我,的,心&apos;, &apos;提醒,我, ,爱,你,要,随时,待命&apos;, &apos; &apos;, &apos;音乐,安静, ,还是,爱情,啊&apos;, &apos;一步,一步,吞噬,着,我,的,心&apos;, &apos;爱上你, ,我,失去,了,我,自己&apos;, &apos; &apos;, &apos;爱得,那么,认真, ,爱得,那么,认真&apos;, &apos;可,还是,听见,了,你,说,不,可能&apos;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wo=<span class="string">','</span>.join(words)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">type(wo)</span><br></pre></td></tr></table></figure><pre><code>str</code></pre><h2 id="词频统计"><a href="#词频统计" class="headerlink" title="词频统计"></a>词频统计</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">counter = &#123;&#125;</span><br><span class="line"><span class="comment"># 如果字典里有该词则加1，否则添加入字典</span></span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> words:</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> s.split(<span class="string">','</span>):</span><br><span class="line">        <span class="keyword">if</span> j <span class="keyword">not</span> <span class="keyword">in</span> counter:</span><br><span class="line">            counter[j] = <span class="number">0</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            counter[j] += <span class="number">1</span></span><br><span class="line"><span class="comment">#词频从高到低排序</span></span><br><span class="line">sorted_counter=sorted(counter.items(),key = <span class="keyword">lambda</span> x:x[<span class="number">1</span>],reverse = <span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sorted_counter</span><br></pre></td></tr></table></figure><pre><code>[(&apos; &apos;, 2424), (&apos;我&apos;, 1388), (&apos;的&apos;, 1380), (&apos;你&apos;, 1189), (&apos;了&apos;, 433), (&apos;在&apos;, 358), (&apos;：&apos;, 255), (&apos;是&apos;, 251), (&apos;都&apos;, 209), (&apos;不&apos;, 187), (&apos;着&apos;, 157), (&apos;说&apos;, 157), (&apos;就&apos;, 152), (&apos;谁&apos;, 152), (&apos;爱&apos;, 142), (&apos;薛之谦&apos;, 129), (&apos;还&apos;, 127), (&apos;会&apos;, 121), (&apos;要&apos;, 118), (&apos;我们&apos;, 111), (&apos;人&apos;, 108), (&apos;好&apos;, 102), (&apos;有&apos;, 100), (&apos;过&apos;, 97), (&apos;也&apos;, 93), (&apos;这&apos;, 92), (&apos;多&apos;, 88), (&apos;和&apos;, 87), (&apos;把&apos;, 82), (&apos;又&apos;, 82), (&apos;吗&apos;, 82), (&apos;到&apos;, 80), (&apos;自己&apos;, 79), ...]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x=sorted_counter[<span class="number">1</span>:<span class="number">35</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x</span><br></pre></td></tr></table></figure><pre><code>[(&apos;我&apos;, 1388), (&apos;的&apos;, 1380), (&apos;你&apos;, 1189), (&apos;了&apos;, 433), (&apos;在&apos;, 358), (&apos;：&apos;, 255), (&apos;是&apos;, 251), (&apos;都&apos;, 209), (&apos;不&apos;, 187), (&apos;着&apos;, 157), (&apos;说&apos;, 157), (&apos;就&apos;, 152), (&apos;谁&apos;, 152), (&apos;爱&apos;, 142), (&apos;薛之谦&apos;, 129), (&apos;还&apos;, 127), (&apos;会&apos;, 121), (&apos;要&apos;, 118), (&apos;我们&apos;, 111), (&apos;人&apos;, 108), (&apos;好&apos;, 102), (&apos;有&apos;, 100), (&apos;过&apos;, 97), (&apos;也&apos;, 93), (&apos;这&apos;, 92), (&apos;多&apos;, 88), (&apos;和&apos;, 87), (&apos;把&apos;, 82), (&apos;又&apos;, 82), (&apos;吗&apos;, 82), (&apos;到&apos;, 80), (&apos;自己&apos;, 79), (&apos;让&apos;, 79), (&apos;能&apos;, 78)]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">xlabel=[]</span><br><span class="line">ylabel=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> x:</span><br><span class="line">    xl=i[<span class="number">0</span>]</span><br><span class="line">    yl=i[<span class="number">1</span>]</span><br><span class="line">    xlabel.append(xl)</span><br><span class="line">    ylabel.append(yl)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xnames</span><br></pre></td></tr></table></figure><pre><code>[&apos;我&apos;, &apos;的&apos;, &apos;你&apos;, &apos;了&apos;, &apos;在&apos;, &apos;：&apos;, &apos;是&apos;, &apos;都&apos;, &apos;不&apos;, &apos;着&apos;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">plt.style.use(<span class="string">'ggplot'</span>)</span><br><span class="line"><span class="keyword">from</span> matplotlib.font_manager <span class="keyword">import</span> FontProperties </span><br><span class="line">font = FontProperties(fname=<span class="string">r"/home/fantasy/Desktop/xzq/华文行楷.ttf"</span>, size=<span class="number">8</span>) </span><br><span class="line">font1 = FontProperties(fname=<span class="string">r"/home/fantasy/Desktop/xzq/华文行楷.ttf"</span>, size=<span class="number">15</span>)</span><br><span class="line">xnames=xlabel</span><br><span class="line">xx= range(len(xnames))</span><br><span class="line">plt.plot(xx,ylabel,<span class="string">'o-'</span>,color=<span class="string">'red'</span>)</span><br><span class="line">plt.xticks(xx,xnames,rotation = <span class="number">0</span>,fontproperties=font)</span><br><span class="line">plt.xlabel(<span class="string">'排名前10的词'</span>,fontproperties=font1)</span><br><span class="line">plt.ylabel(<span class="string">'频数'</span>,fontproperties=font1)</span><br><span class="line">plt.title(<span class="string">'薛之谦歌词中出现字词排名靠前的几个'</span>,fontproperties=font1)</span><br></pre></td></tr></table></figure><pre><code>Text(0.5,1,&apos;薛之谦歌词中出现字词排名靠前的几个&apos;)</code></pre><p><img src="output_18_1.png" alt="png"></p><p>可见，出现最多的字词为“我”，“的”，“你”</p><h2 id="绘制词云图"><a href="#绘制词云图" class="headerlink" title="绘制词云图"></a>绘制词云图</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">fig=WordCloud(collocations=<span class="keyword">False</span>,font_path=<span class="string">'/home/fantasy/Desktop/xzq/华文行楷.ttf'</span>,width=<span class="number">1400</span>,height=<span class="number">1400</span>,min_font_size=<span class="number">3</span>,margin=<span class="number">2</span>).generate(wo)</span><br><span class="line">plt.imshow(fig)</span><br><span class="line">plt.axis(<span class="string">"off"</span>)</span><br><span class="line">plt.title(<span class="string">'薛之谦歌词词云图'</span>,fontproperties=font1)</span><br></pre></td></tr></table></figure><pre><code>Text(0.5,1,&apos;薛之谦歌词词云图&apos;)</code></pre><p><img src="output_21_1.png" alt="png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文对薛之谦的部分歌词进行了文本挖掘处理，欢迎围观~&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;目标：绘制词云图，分析词频并可视化&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;文本预处理&quot;&gt;&lt;a href=&quot;#文本预处理&quot; class=&quot;headerlink&quot; tit
      
    
    </summary>
    
      <category term="Python自然语言处理" scheme="http://yoursite.com/categories/Python%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="文本分析＆挖掘,Python" scheme="http://yoursite.com/tags/%E6%96%87%E6%9C%AC%E5%88%86%E6%9E%90%EF%BC%86%E6%8C%96%E6%8E%98-Python/"/>
    
  </entry>
  
  <entry>
    <title>解决在Anoconda环境下安装第三方库的问题</title>
    <link href="http://yoursite.com/2018/10/06/%E8%A7%A3%E5%86%B3%E5%9C%A8Anoconda%E7%8E%AF%E5%A2%83%E4%B8%8B%E5%AE%89%E8%A3%85%E7%AC%AC%E4%B8%89%E6%96%B9%E5%BA%93%E7%9A%84%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2018/10/06/解决在Anoconda环境下安装第三方库的问题/</id>
    <published>2018-10-06T02:53:21.000Z</published>
    <updated>2018-10-06T05:53:02.279Z</updated>
    
    <content type="html"><![CDATA[<p>在Anoconda环境下安装第三方库，使用其自带的pip即可　<br><br>比如安装wordcloud<br><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo /home/fantasy/anaconda3/bin/pip install wordcloud</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在Anoconda环境下安装第三方库，使用其自带的pip即可　&lt;br&gt;&lt;br&gt;比如安装wordcloud&lt;br&gt;&lt;br&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span cla
      
    
    </summary>
    
      <category term="Python日常小bug" scheme="http://yoursite.com/categories/Python%E6%97%A5%E5%B8%B8%E5%B0%8Fbug/"/>
    
    
  </entry>
  
  <entry>
    <title>Hexo+Github免费搭建炫酷个人博客</title>
    <link href="http://yoursite.com/2018/10/03/Hexo-Github%E5%85%8D%E8%B4%B9%E6%90%AD%E5%BB%BA%E7%82%AB%E9%85%B7%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    <id>http://yoursite.com/2018/10/03/Hexo-Github免费搭建炫酷个人博客/</id>
    <published>2018-10-03T04:10:04.000Z</published>
    <updated>2018-10-03T07:28:09.007Z</updated>
    
    <content type="html"><![CDATA[<p>一直有一个搭建个人博客的想法，之前折腾过Python的Django框架，但没有足够的时间和精力去系统的学习Django，最后不了了之。趁着这次国庆假期，想法又涌上心头，一顿搜索后，Hexo成功地引起了我的注意。Hexo搭配Github，可以实现免费建站，具体来说，就是将博客的一些配置文件以及博客内容同步存储到你自己的Github仓库而不需要额外购买域名。这篇文章便记录了从零开始利用Hexo+Github实现免费搭建个人博客的步骤。<br></p><h3 id="1-安装Node-js"><a href="#1-安装Node-js" class="headerlink" title="1.安装Node.js"></a>1.安装Node.js</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">访问　http://nodejs.cn/download/，下载系统对应的版本安装即可</span><br></pre></td></tr></table></figure><h3 id="2-安装Git"><a href="#2-安装Git" class="headerlink" title="2.安装Git"></a>2.安装Git</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">访问 https://git-scm.com/downloads，同上</span><br></pre></td></tr></table></figure><h3 id="3-安装Hexo"><a href="#3-安装Hexo" class="headerlink" title="3.安装Hexo"></a>3.安装Hexo</h3><p>使用npm来安装Hexo，所以先安装npm<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install npm</span><br></pre></td></tr></table></figure></p><p>然后就可以安装Hexo了<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-cli g</span><br></pre></td></tr></table></figure></p><p>注意：使用npm网速较慢，建议多试几次，或者直接切换为国内源：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm config set registry &quot;https://registry.npm.taobao.org&quot;</span><br></pre></td></tr></table></figure></p><h3 id="4-使用Hexo"><a href="#4-使用Hexo" class="headerlink" title="4.使用Hexo"></a>4.使用Hexo</h3><p>首先创建一个用来存放博客的文件夹，比如我创建了一个名字为”myblog”的文件夹，那么就进入该文件夹目录下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd /home/fantasy/Desktop/myblog</span><br></pre></td></tr></table></figure></p><p>注意：cd 后面的是你的文件夹的完整路径<br></p><p>接下来初始化你的文件夹<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo init myblog</span><br></pre></td></tr></table></figure></p><p>之后你就会看到文件夹出现了一堆文件，如图<br><br><img src="1.png" alt=""> <br><br>现在基本的操作就完成了<br></p><h3 id="5-在本地部署博客"><a href="#5-在本地部署博客" class="headerlink" title="5.在本地部署博客"></a>5.在本地部署博客</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo claen</span><br><span class="line">hexo generate</span><br><span class="line">hexo deploy</span><br></pre></td></tr></table></figure><p>首先安装server<br><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-server --save</span><br></pre></td></tr></table></figure></p><p>然后开启本地服务器<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo sever</span><br></pre></td></tr></table></figure></p><p>会输出一个网址，如图<br><br><img src="2.png" alt=""> <br><br>复制输出的网址，在浏览器中打开即可访问博客，或者也可以直接复制以下网址<br><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://localhost:4000/</span><br></pre></td></tr></table></figure></p><h3 id="6-将博客部署到Github上"><a href="#6-将博客部署到Github上" class="headerlink" title="6.将博客部署到Github上"></a>6.将博客部署到Github上</h3><p>首先去官网注册一个Github账号，然后登入你的账号<br><br>现在来创建一个新的Repositories,配置如图<br><br><img src="3.png" alt="">　<br><br>然后配置生成ssh key <br></p><h5 id="配置SSH-key"><a href="#配置SSH-key" class="headerlink" title="配置SSH key"></a>配置SSH key</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir ~/.ssh </span><br><span class="line">cd ~/.ssh</span><br><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure><p>接着是一路回车<br><br>然后输入以下代码<br><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat ~/.ssh/id_rsa.pub</span><br></pre></td></tr></table></figure></p><p>你会看到输出了一些字符串，这就是SSH密钥，复制它们<br><br>接下来进入你的Github主页，如图<br><br><img src="4.png" alt=""> <br><br>接下来的步骤都写在图片里了，看图<br><br><img src="5.png" alt=""> <br><br><img src="6.png" alt=""><br><br>点击左侧的SSH and GPG keys，继续看图<br><br><img src="7.png" alt=""> <br><br><img src="8.png" alt=""> <br><br>现在，将之前复制的内容粘贴到上图所示位置的Key框里面，点击Add SHH key，SSH key配置完毕<br></p><h4 id="配置Git"><a href="#配置Git" class="headerlink" title="配置Git"></a>配置Git</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config –global user.ame “xxx” </span><br><span class="line">git config –global user.mail “xxx@xxx.com”</span><br></pre></td></tr></table></figure><p>其中，第一个xxx填写你的Github用户名，也就是之前创建Repositories的时候在用的那个；<br><br>第二个需要把双引号里面的内容替换为你注册Github时所使用的邮箱<br><br>这样子就配置完成了<code>Git</code>，可以输入下面的代码做一个测试<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh -T git@github.com</span><br></pre></td></tr></table></figure></p><h4 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h4><p>进入之前你创建的文件夹,打开<code>config.yml</code>文件，拉到最下面，修改deploy的配置为如下代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">## Docs: https://hexo.io/docs/deployment.html</span><br><span class="line">deploy:</span><br><span class="line">  # 类型</span><br><span class="line">  type: git</span><br><span class="line">  # 仓库</span><br><span class="line">  repo: git@github.com:username/username.github.io.git</span><br><span class="line">  # 分支</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure></p><p>其中的usename改为你自己的Github用户名即可，点击保存<br></p><h4 id="部署到Github"><a href="#部署到Github" class="headerlink" title="部署到Github"></a>部署到Github</h4><p>终于到这一步了<br><br>依下次输入以下3行代码<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo clean </span><br><span class="line">hexo generate</span><br><span class="line">hexo deploy</span><br></pre></td></tr></table></figure></p><p>这样子部署工作就彻底完成了<br></p><h3 id="8-测试访问你的博客"><a href="#8-测试访问你的博客" class="headerlink" title="8.测试访问你的博客"></a>8.测试访问你的博客</h3><p>在浏览器的网址输入框输入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">usename.github.io</span><br></pre></td></tr></table></figure></p><p>其中,usename依旧是你的Github用户名<br></p><p>以上便是搭建博客的全部步骤</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;一直有一个搭建个人博客的想法，之前折腾过Python的Django框架，但没有足够的时间和精力去系统的学习Django，最后不了了之。趁着这次国庆假期，想法又涌上心头，一顿搜索后，Hexo成功地引起了我的注意。Hexo搭配Github，可以实现免费建站，具体来说，就是将博客
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>年金</title>
    <link href="http://yoursite.com/2018/10/01/%E5%B9%B4%E9%87%91/"/>
    <id>http://yoursite.com/2018/10/01/年金/</id>
    <published>2018-10-01T13:20:01.000Z</published>
    <updated>2018-10-29T14:53:11.249Z</updated>
    
    <content type="html"><![CDATA[<h3 id="标准型年金"><a href="#标准型年金" class="headerlink" title="标准型年金"></a>标准型年金</h3><p>在此之前，先来谈一下关于折现到零时刻和积累到某（n）时刻的意思.<br><br>以期末付年金为例：<br><br>所谓零时刻，就是第一期的开始时刻，即1期初<br><br>所谓积累到某时刻，就是说某（n）时刻末尾，也就是第(n+1)期初.<br><br>以期末付标准型年金为例，如下图所示：<br><br><img src="1.jpg" alt=""> <br><br>另外，期初付年金核算公式推导如下<br><br><img src="2.jpg" alt=""></p><h3 id="一般型年金"><a href="#一般型年金" class="headerlink" title="一般型年金"></a>一般型年金</h3><h4 id="变利率年金"><a href="#变利率年金" class="headerlink" title="变利率年金"></a>变利率年金</h4><blockquote><p>１. 各付款期间段的利率不同，就是说在每个期间段都按照该期间段的利率来核算，所以年金现值为$a_{n┒}=(1+i_1)^{-1}+(1+i_1)^{-1}(1+i_2)^{-1}+…+(1+i_1)^{-1}(1+i_2)^{-1}…(1+i_n)^{-1}=\sum_{t=1}^n\prod_{s=1}^{t}(1+i_s)^{-1}$ <br>年金积累值为$s_n┒=1+(1+i_n)+(1+i_n)(1+i_{n-1})+…+(1+i_n)…(1+i_2)=\sum_{t=1}^{n}\prod_{s=0}^{t-1}(1+i_{n-s+1})$,令$i_{n+1}=0$ .<br><br>２. 各次付款所依据的利率不同，就是说利率是按照每次付款来计算的，当把某一次付款折现到０时刻或者积累到某时刻时，利率都是按照该次付款的利率，而不是随着付款的期间而变动，这样子，年金现值为$a_{n┒}=(1+i_1)^{-1}+(1+i_2)^{-2}+…+(1+i_n)^{-n}=\sum_{t=1}^{n}(1+i_t)^{-t}$，积累值为$s_{n┒}=(1+i_1)^{n-1}+(1+i_2)^{n-2}+…+(1+i_n)+1=\sum_{t=1}^{n}(1+i_t)^{n-t}$</p></blockquote><p><img src="3.jpg" alt=""> <br><br><img src="4.jpg" alt=""> <br></p><h4 id="付款频率与计息频率不同的年金"><a href="#付款频率与计息频率不同的年金" class="headerlink" title="付款频率与计息频率不同的年金"></a>付款频率与计息频率不同的年金</h4><p><img src="05.jpg" alt=""> <br><br><img src="06.jpg" alt=""> <br><br><img src="07.jpg" alt=""> <br></p><h4 id="连续年金"><a href="#连续年金" class="headerlink" title="连续年金"></a>连续年金</h4><p>连续年金，即付款频率无限大的年金，是付款频率大于计息频率的特例,就是在一个计息期内无限次付款<br><br><img src="08.jpg" alt=""> <br></p><h4 id="基本变化年金"><a href="#基本变化年金" class="headerlink" title="基本变化年金"></a>基本变化年金</h4><p><img src="9.jpg" alt=""> <br></p><h4 id="更一般变化年金"><a href="#更一般变化年金" class="headerlink" title="更一般变化年金"></a>更一般变化年金</h4><p><img src="10.jpg" alt=""><br><br><img src="11.jpg" alt=""><br><br><img src="12.jpg" alt=""><br></p><h4 id="连续变化年金"><a href="#连续变化年金" class="headerlink" title="连续变化年金"></a>连续变化年金</h4><p><img src="13.jpg" alt=""> <br></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;标准型年金&quot;&gt;&lt;a href=&quot;#标准型年金&quot; class=&quot;headerlink&quot; title=&quot;标准型年金&quot;&gt;&lt;/a&gt;标准型年金&lt;/h3&gt;&lt;p&gt;在此之前，先来谈一下关于折现到零时刻和积累到某（n）时刻的意思.&lt;br&gt;&lt;br&gt;以期末付年金为例：&lt;br&gt;&lt;br&gt;所
      
    
    </summary>
    
      <category term="金融数学" scheme="http://yoursite.com/categories/%E9%87%91%E8%9E%8D%E6%95%B0%E5%AD%A6/"/>
    
    
  </entry>
  
  <entry>
    <title>多元统计与R语言第二次上机作业</title>
    <link href="http://yoursite.com/2018/10/01/%E5%A4%9A%E5%85%83%E7%BB%9F%E8%AE%A1%E4%B8%8ER%E8%AF%AD%E8%A8%80%E7%AC%AC%E4%BA%8C%E6%AC%A1%E4%B8%8A%E6%9C%BA%E4%BD%9C%E4%B8%9A/"/>
    <id>http://yoursite.com/2018/10/01/多元统计与R语言第二次上机作业/</id>
    <published>2018-10-01T09:05:04.000Z</published>
    <updated>2018-10-01T09:11:34.556Z</updated>
    
    <content type="html"><![CDATA[<figure class="highlight plain"><figcaption><span>setup, include</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">knitr::opts_chunk$set(echo = TRUE)</span><br></pre></td></tr></table></figure><p>1.1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data1=read.csv(&apos;/home/fantasy/Desktop/r2/data/data1.csv&apos;,sep=&apos;,&apos;,header =F )</span><br><span class="line">print(data1)</span><br></pre></td></tr></table></figure></p><p>1.2<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data2=read.csv(&apos;/home/fantasy/Desktop/r2/data/data2.csv&apos;)</span><br><span class="line">print(data2)</span><br></pre></td></tr></table></figure></p><p>1.3<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data3=read.table(&apos;/home/fantasy/Desktop/r2/data/data3.csv&apos;,sep=&apos;,&apos;,header=F)</span><br><span class="line">print(data3)</span><br></pre></td></tr></table></figure></p><p>1.4<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mydata=read.csv(&apos;/home/fantasy/Desktop/train.csv&apos;)</span><br><span class="line">print(mydata)</span><br></pre></td></tr></table></figure></p><p>2.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x=seq(0,100,1)</span><br><span class="line">plot(x,x+1/x)</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x=seq(1,100,0.001)</span><br><span class="line">plot(x,exp(-(x^2)))</span><br></pre></td></tr></table></figure><p>3.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x=rnorm(100,60,10)</span><br><span class="line">x</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.test(x,conf.level=0.95)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.test(x,mu=60)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.test(x,mu=60,alternative = &apos;greater&apos;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.test(x,mu=60,alternative = &apos;less&apos;)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">y=rnorm(100,0,1)</span><br><span class="line">y</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">var.test(x,y,conf.level = 0.95)</span><br></pre></td></tr></table></figure><p>4.1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">binom.test(x=656,n=1000,p=1/2)</span><br></pre></td></tr></table></figure></p><p>4.2<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">s=rexp(100,2)</span><br><span class="line">s</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">t.test(s)</span><br></pre></td></tr></table></figure><p>5.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data=read.csv(&apos;/home/fantasy/Desktop/r2/data/成绩单.csv&apos;)</span><br><span class="line">data</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">chinese=c(data[,3])</span><br><span class="line">math=data[,4]</span><br><span class="line">english=data[,5]</span><br><span class="line">mean(chinese)</span><br><span class="line">mean(math)</span><br><span class="line">mean(english)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">var(chinese)</span><br><span class="line">var(math)</span><br><span class="line">var(english)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hist(chinese)</span><br><span class="line">hist(math)</span><br><span class="line">hist(english)</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data[,2][data[,5][data[,3]&gt;120 &amp; data[,4]&gt;120]]</span><br><span class="line">mean(data[,5][data[,3]&gt;120 &amp; data[,4]&gt;120])</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">total_score=data[,3]+data[,4]+data[,5]</span><br><span class="line">data$total_score=total_score</span><br><span class="line">data</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">eng1=data[,5][data[,&apos;total_score&apos;]&gt;300]</span><br><span class="line">chinese1=data[,3][data[,&apos;total_score&apos;]&gt;300]</span><br><span class="line">eng2=data[,5][data[,&apos;total_score&apos;]&lt;300]</span><br><span class="line">chinese2=data[,3][data[,&apos;total_score&apos;]&lt;300]</span><br><span class="line">print(cor.test(eng1,chinese1))</span><br><span class="line">print(cor.test(eng2,chinese2))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;figure class=&quot;highlight plain&quot;&gt;&lt;figcaption&gt;&lt;span&gt;setup, include&lt;/span&gt;&lt;/figcaption&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;
      
    
    </summary>
    
      <category term="R" scheme="http://yoursite.com/categories/R/"/>
    
    
  </entry>
  
  <entry>
    <title>多元统计与R语言第一次上机作业</title>
    <link href="http://yoursite.com/2018/10/01/%E5%A4%9A%E5%85%83%E7%BB%9F%E8%AE%A1%E4%B8%8ER%E8%AF%AD%E8%A8%80%E7%AC%AC%E4%B8%80%E6%AC%A1%E4%B8%8A%E6%9C%BA%E4%BD%9C%E4%B8%9A/"/>
    <id>http://yoursite.com/2018/10/01/多元统计与R语言第一次上机作业/</id>
    <published>2018-10-01T08:51:43.000Z</published>
    <updated>2018-10-01T09:13:04.064Z</updated>
    
    <content type="html"><![CDATA[<p>2.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">c(1,2,3)</span><br><span class="line">s&lt;-c</span><br><span class="line">print(s)</span><br></pre></td></tr></table></figure><p>3.1<br><figure class="highlight plain"><figcaption><span>&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">((-27)*12)/21</span><br></pre></td></tr></table></figure></p><p>3.2<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sqrt(10)</span><br></pre></td></tr></table></figure></p><p>3.3<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">log(10)</span><br></pre></td></tr></table></figure></p><p>3.4<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">log10(2+3*pi)</span><br></pre></td></tr></table></figure></p><p>3.5<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">exp(2.7689)</span><br></pre></td></tr></table></figure></p><p>3.6<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(25-5)^3</span><br></pre></td></tr></table></figure></p><p>3.7<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cos(pi)</span><br></pre></td></tr></table></figure></p><p>3.8.1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prod(2:3)</span><br></pre></td></tr></table></figure></p><p>3.8.2<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">10*9*8*7*6*5*4</span><br></pre></td></tr></table></figure></p><p>3.8.3<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(10*9*8*7*6*5*4)/(40*39*38*37*36)</span><br></pre></td></tr></table></figure></p><p>3.9.1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">choose(5,2)</span><br></pre></td></tr></table></figure></p><p>3.9.2<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">choose(7,3)</span><br></pre></td></tr></table></figure></p><p>4.rep<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a=c(1,2)</span><br><span class="line">rep(a,2)</span><br></pre></td></tr></table></figure></p><p>4.seq<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">seq(from=1,to=10,by=2)</span><br></pre></td></tr></table></figure></p><p>4.matrix<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data=c(1,2,3,4,5,6,7,8)</span><br><span class="line">matrix(data,nrow=4,ncol=2,byrow=T)</span><br></pre></td></tr></table></figure></p><p>5.1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dnorm(1)</span><br></pre></td></tr></table></figure></p><p>5.2<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">qnorm(0.95)</span><br></pre></td></tr></table></figure></p><p>5.3<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpois(100,5)</span><br></pre></td></tr></table></figure></p><p>5.4<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rnorm(100,60,10)</span><br></pre></td></tr></table></figure></p><p>6.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x=c(rnorm(100,0,1))</span><br><span class="line">print(x)</span><br></pre></td></tr></table></figure></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">print(sum(x))</span><br><span class="line">print(mean(x))</span><br><span class="line">print(var(x))</span><br><span class="line">print(sd(x))</span><br><span class="line">print(min(x))</span><br><span class="line">print(max(x))</span><br><span class="line">print(median(x))</span><br><span class="line">print(summary(x))</span><br></pre></td></tr></table></figure><p>7.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x=seq(1,10,1)</span><br><span class="line">plot(x,exp(x))</span><br></pre></td></tr></table></figure></p><p>7.2<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x=runif(100,1,10)</span><br><span class="line">print(x)</span><br><span class="line">hist(x)</span><br></pre></td></tr></table></figure></p><p>8.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">library(MASS)</span><br></pre></td></tr></table></figure></p><p>9.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ID=c(&apos;01&apos;,&apos;02&apos;,&apos;03&apos;)</span><br><span class="line">class=c(1,2,3)</span><br><span class="line">body_long=c(188,189,190)</span><br><span class="line">weight=c(66,67,65)</span><br><span class="line">df=data.frame(ID,class,body_long,weight)</span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure></p><p>10.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(df[,2])</span><br><span class="line">print(df[2,2])</span><br></pre></td></tr></table></figure></p><p>11.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data0=read.csv(&apos;path+1.csv&apos;)</span><br><span class="line">data1=read.table(&apos;path+1.txt&apos;,header=F)</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;2.&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;
      
    
    </summary>
    
      <category term="R" scheme="http://yoursite.com/categories/R/"/>
    
    
  </entry>
  
  <entry>
    <title>$n$级行列式</title>
    <link href="http://yoursite.com/2018/10/01/gdds2/"/>
    <id>http://yoursite.com/2018/10/01/gdds2/</id>
    <published>2018-10-01T07:24:05.000Z</published>
    <updated>2018-10-01T07:35:58.993Z</updated>
    
    <content type="html"><![CDATA[<p><strong>二、行列式(8-18)</strong><br><br><code>排列</code> <code>行列式性质</code><br><br>关于<strong>数域</strong>：<code>有理数域是最小的数域，复数域是最大的数域</code><br><br>现在，再次回顾最开头的证明的判断线性方程组的解的个数的定理，我们发现，判别的前提都是得先把线性方程组的增广矩阵经过初等行变换化成阶梯型，但这样子太麻烦了，而且都马上能解出来了，如果仅仅是判断线性方程组的解的个数，这种方法实在是$low$爆了，所以现在咱们要寻找更简单的判断方法<br><br>先来看一个二元线性方程组的例子：<br><img src="5.jpg" alt=""><br><br>没错，我们可以看到，<strong>当该二元线性方程组的系数矩阵行列式不为零时，该线性方程组有唯一解</strong>，事实证明，这个结论对于更高阶的系数矩阵行列式（也可以说是对$n$元线性方程组）是<strong>完全$ok$的，</strong>那么我们该如何证明呢？<br><br>此处先留个悬念，接下来先看一下关于$n$阶行列式的一些知识：<br></p><h3 id="排列"><a href="#排列" class="headerlink" title="排列"></a>排列</h3><ul><li><blockquote><p><strong>逆序数</strong>：一个排列（由$1,2,…,n$组成的一个<strong>有序数组</strong>称为一个<strong>$n$级排列</strong>）中逆序（在一个排列中，如果<strong>一对数</strong>的前后位置与大小顺序相反，那么它们就称为一个<strong>逆序</strong>）的总数就称为这个排列的逆序数.<br></p></blockquote></li><li><blockquote><p>排列的奇偶性：逆序数为偶（奇）数的排列叫做偶（奇）排列.<br></p></blockquote></li><li><blockquote><p><strong>对换</strong>：把一个排列中<strong>某两个数</strong>的位置互换，而其余的数不动，就得到另一个排列，这样的一个变换称为一个对换.（<code>注意：是任意两个数，不一定相邻哦，下面的定理１是为了证明方便才进行的两个相邻元素之间的对换，不要搞混，那只是为了证明.</code>）<br></p></blockquote></li><li><blockquote><p><strong>Th1. 对换改变排列的奇偶性</strong><br><br><strong>证明</strong>：<br><br>先来看两个数相邻的情况：<br><br><img src="06.jpg" alt=""><br><br>排列（１）与排列（２）逆序数相差１，从而两者的奇偶性相反；<br><br>再来看一般情况：<br><br><img src="07.jpg" alt=""><br><br>由排列（３）变换到排列（４），总共经过了$(S+1)+S=2S+1$次相邻两个元素之间的对换，其中$i$移动到$j$的右边需要与它相邻的元素做$S+1$次对换，而$j$在$i$移动到其右侧后，需要再向左与其相邻元素做$S$次对换方可到达原来$i$所在的位置，而$2S+1$是奇数，也就是说总共做了奇数次相邻两元素之间的对换，从而排列（３）与排列（４）的奇偶性相反，证明完毕.<br></p></blockquote></li><li><blockquote><p> Th2. 任意一个$n$级排列$j_1j_2…j_n$与自然序列$123…n$可以经过一系列的对换互变，且所做对换的次数与原来的排列$j_1j_2…j_n$的奇偶性相同.<br><br><strong>证明</strong>：<br><br><img src="8.jpg" alt=""><br><br>假设$j_1j_2…j_n$是奇排列，由于$123…n$　，也就是说排列的奇偶性在经过$S$次对换后变化了，所以一定做了奇数次对换，故$S$为奇数.同理若假设一开始的$j_1j_2…j_n$是偶排列，由于$j_1j_2…j_n$也是偶排列，所以做了偶数次对换，即$S$此时为偶数，证明完毕，很简单吧.<br></p></blockquote></li></ul><h3 id="n-级行列式"><a href="#n-级行列式" class="headerlink" title="$n$级行列式"></a>$n$级行列式</h3><ul><li><blockquote><p>$n$级行列式是$n!$项的代数和<br><br><img src="09.png" alt="ｎ级行列式的定义"><br></p></blockquote></li></ul><ul><li>对于上述$n$级行列式，我们可以取出其中任意一项$(-1)^{ι<br>(j_1j_2…j_n)}a_{1j_1}a_{2j_2}…a_{nj_n}$（从全部的$n!$项中取,注意到这里还是按照行指标顺序排列，列指标进行全排列的那种）可以得到以下结论：<br><br><img src="9.jpg" alt=""><br><br>证明如下：<br><br><img src="10.jpg" alt=""><br><br>那么这个结论有什么用处呢？继续往下看你就明白了<br><br>我们把之前任取的$(-1)^{ι<br>(j_1j_2…j_n)}a_{1j_1}a_{2j_2}…a_{nj_n}$<br>搬过来先，然后按照证明里面的套路分别将行指标和列指标做对换，和证明中唯一不同之处在于，我们现在让列指标对换成自然排列（$12…n$），那么其逆序数为零，即$ι<br>(12…n)=0$.好，现在开始代上述结论公式，则有<br>$detA=\sum{(-1)^{ι<br>(j_1j_2…j_n)}a_{1j_1}a_{2j_2}…a_{nj_n}}=\sum{(-1)^{ι<br>(i_1i_2…i_n)+ι(12…n)}a_{i_11}a_{i_22}…a_{i_nn}}$<br>$=\sum{(-1)^{ι<br>(i_1i_2…i_n)}a_{i_11}a_{i_22}…a_{i_nn}.}$ <br><br>观察最右侧的式子，可以看出，它是把列指标按照自然顺序排列，而且符号由行指标的逆序数决定，这和我们之前（第一个等号右面的那一项）先将行指标按自然顺序排列，而符号由列指标的逆序数决定正好相反，但是，它们却是相等的，都代表着行列式$A$，即上式中的$det  Ａ$.<br><br>这就不禁让我们思考，行和列之间似乎有着某种对称的关系.事实上的确如此，并且据此我们还可以得到关于行列式的一些性质，下面就来看一下这些有用的性质都是何方神圣.<br></li></ul><p>行列式的性质：</p><ul><li><blockquote><p>性质１． 行列互换，行列式不变（也就是说行列式的转置的值和行列式的值本身是等的）；<br><br><img src="11.jpg" alt=""><br></p></blockquote></li><li><blockquote><p>性质２． 以一数乘行列式的一行就相当于用这个数乘次行列式.（如果行列式中一行为０，那么行列式为零)；<br><br><img src="12.jpg" alt=""><br></p></blockquote></li><li><blockquote><p>性质３． 如果某一行是两组数的和，那么这个行列式就等于两个行列式的和，而这两个行列式除这一行以外全与原来行列式的对应行一样；<br></p></blockquote></li><li><blockquote><p>性质４．　如果行列式中有两行相同，那么行列式为零．所谓两行相同就是说两行的对应元素都相等；<br><br><img src="14.jpg" alt=""><br></p></blockquote></li><li><blockquote><p>性质５．　如果行列式的两行成比例，那么行列式为零；<br><br><img src="15.jpg" alt=""><br></p></blockquote></li><li><blockquote><p><strong>性质６．　把一行的倍数加到另一行，行列式不变；</strong><br><br><img src="16.jpg" alt=""><br></p></blockquote></li><li><blockquote><p>性质７．　对换行列式中两行的位置，行列式反号．<br><br><img src="13.jpg" alt=""><br></p></blockquote></li></ul><p><em>应用行列式的性质，可以解决大部分的行列式计算，这里不再列举<br></em></p><ul><li>$n$级矩阵$A=(a_{ij})$，划去$A$的$(i,j)$元所在的第$i$行和第$j$列,剩下的元素按照原来的顺序构成一个$n-1$阶行列式，称之为$A$的$(i,j)$元的<strong>余子式</strong>，记做$M_{ij}$，令$A_{ij}=(-1)^{i+j}M_{ij}$，称之为$A$的$(i,j)$元的<strong>代数余子式</strong>.<br><br>有了代数余子式的概念之后，计算行列式就变得简单起来，或者说我们又找到了一个新的计算行列式的方法，继续向下看.<br><br><strong>下面惊现４个定理！</strong></li><li><strong> Theory</strong>：$n$级矩阵$A=(a_{ij})$的行列式$|A|=a_{i1}A_{i1}+a_{i2}A_{i2}+…+a_{in}A_{in}$=$\sum_{j=1}^n{a_{ij}A_{ij}}$，其中$i=1,2,…,n$.<br><br>证明：取$A$的第$i$行<br><br>将第$i$行的元素排在第一个位置，其他的还是从小到大按照行指标成自然顺序排列，即$|A|＝\sum_{jk_1…k_{i-1}k_{i+1}…k_n} (-1)^{.{(i-1)+(j-1)}+ι(k_1…k_{i-1}k_{i+1}…k_n)}a_{ij}a_{1k_1}…a_{i-1,k_{i-1}}a_{i+1,k_{i+1}}…a_{nk_n}$<br>＝$\sum_{j=1}^n (-1)^{i+j}a_{ij}*\sum_{k_1…k_{i-1}k_{i+1}…k_n}(-1)^{ι(k_1…k_{i-1}k_{i+1}…k_n)}a_{1k_1}…a_{i-1,k-1}a_{i+1,k+1}…a_{nk_n}$<br>＝$\sum_{j=1}^n (-1)^{i+j}a_{ij}M_{ij}$<br>＝$\sum_{j=1}^n a_{ij}A_{ij}$，这样子完成了上述定定理的证明.<br><br>什么？没太懂？好吧，来解释一下：<br><br>第一个等式是第$i$行的元素排在第一个位置，其他的还是从小到大按照行指标成自然顺序排列，所以可以先写出后面的那些以及求和符号的底部那些，关键是逆序数的确定，这里又用到了前面刚证明的结论（我指的是行列式性质上面的那个手写证明哦），也就是（－１）的幂次等于行指标的逆序数＋列指标的逆序数，可以向上翻一下，那咱们现在就来看看：$(i-1)$是行指标的逆序数（因为原来的行指标是按照自然顺序排列，逆序数是０，后来咱们把第$i$行放在了最前面，所以从第二行往右（后），比$i$小的行指标有$1,2,…,i-1$，一共$i-1$个），同理$(j-1)$是此时列指标中的$j$的逆序数，而后面的$ι(k_1…k_{i-1}k_{i+1}…k_n)$是列指标中除去$j$之外的其他元素的逆序数，相当于把列指标拆成了两项，你可能会有疑问，这里为什要把列指标拆开而不直接写成$ι(jk_1…k_{i-1}k_{i+1}…k_n)$呢？那是因为，我们要证明的目标式是含有划掉第$i$行第$j$列，其余的元素按照原来的顺序所构成的行列式（即余子式）.<br><br>第二个式子是提取公因式，把所有公因式都放在左边，由于取定了行之后，要求和遍历每一列，因此提取出$\sum_{j=1}^n$，由于我们取定了第$i$行，所以首先可以把$a_{ij}$提出来，最后是$(-1)$的幂次，这个$(i-1)+(j-1)=i+j-2$可以直接写成$i+j$(因为2是偶数)，$i$确定了，$j$可以遍历，所以提取出来了$(-1)^{i+j}$，剩余的就统统放在右边了，这也就是第二个式子.<br><br>第三个式子就不必多说了.<br><br><code>这个定理也就称为n阶行列式按一行展开</code><br><br><code>同样的，对于列也有一样的性质</code><br></li><li>$n$级矩阵$A=(a_{ij})$的行列式$|A|=a_{1j}A_{1j}+a_{2j}A_{2j}+…+a_{nj}A_{nj}$=$\sum_{i=1}^n{a_{ij}A_{ij}}$，其中$j=1,2,…,n$.<br><br>证明：<br><br>按第$j$列展开，<br>$|A|=|A’|=a_{1j}A_{1j}+a_{2j}A_{2j}+…+a_{nj}A_{nj}$=$\sum_{i=1}^n{a_{ij}A_{ij}.}$,下面的图形象的说明了可以这样做的原因<br><br><img src="17.jpg" alt=""><br><br><br><code>这个定理也就称为n阶行列式按一列展开</code><br></li></ul><p>以上定理统称为<strong>行列式按一行(列)展开</strong>.<br></p><p>接下来我们想一下，以上的定理都是某元素乘以自身的代数余子式，结果等于行列式的值，那么如果我们用该元素乘以与该元素在同一行或同一列的某个元素的代数余子式而不是去乘以该元素自身的代数余子式，那么结果会是多少呢？答案是零！我们可以来证明一下：<br><br><img src="18.jpg" alt=""><br><br>这个定理对于列也同样成立，即$n$级矩阵$A=(a_{ij})$，当$i!=j$时，$a_{1j}A_{1l}+a_{2j}A_{2l}+…+a_{nj}A_{nl}=0$</p><p><strong>以上总共４个定理，前两个称为行列式按一行（列）展开，结果仍是行列式本身的值，后两个是变形，不再乘以自身的代数余子式了，结果是０，这些在后面都有很重要的应用</strong></p><p>下面介绍<strong>范德蒙德行列式</strong><br></p><ul><li>形如<br><br><img src="19.jpg" alt=""><br><br>的行列式称之为”$n$级的范德蒙德行列式”,并且该行列式的结果为<br><br><img src="20.jpg" alt=""><br><br>也就是说“对任意的$n$(n&gt;=2)，$n$级范德蒙德行列式等于$a_1,a_2,…,a_n$这$n$个数的所有可能的差$a_i-a_j$($1&lt;=j&lt;i&lt;=n$)的乘积”，用数学归纳法即可证明之.<br></li></ul><p>现在，是时候回到本章最开始提出的问题了：<strong>$n$元线性方程组的系数矩阵行列式不为零时，该线性方程组有唯一解</strong>的证明：<br><br><img src="21.jpg" alt=""><br><br><img src="22.jpg" alt=""><br><br><strong>吐槽</strong>：绕了这么一大圈，终于证出来了，主要用到了之前证明的行列式的那些性质，按照一行（列）展开以及第一章提到的比较笨的方法（直接化增广矩阵为阶梯型，都快计算出来了）.<br></p><ul><li>而对于$n$阶齐次线性方程组来说，应用上面证明的定理，自然有较好的性质，这也就是克莱默法则的第一部分<br></li></ul><blockquote><p>若只有零解（即只有唯一解），则其系数矩阵行列式不为０；<br><br>若有非零解（即有无穷多个解），则其系数矩阵行列式等于０.<br></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;strong&gt;二、行列式(8-18)&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;排列&lt;/code&gt; &lt;code&gt;行列式性质&lt;/code&gt;&lt;br&gt;&lt;br&gt;关于&lt;strong&gt;数域&lt;/strong&gt;：&lt;code&gt;有理数域是最小的数域，复数域是最大的数域&lt;/code&gt;&lt;br&gt;
      
    
    </summary>
    
      <category term="MathA" scheme="http://yoursite.com/categories/MathA/"/>
    
    
  </entry>
  
  <entry>
    <title>解线性方程组</title>
    <link href="http://yoursite.com/2018/10/01/gdds1/"/>
    <id>http://yoursite.com/2018/10/01/gdds1/</id>
    <published>2018-10-01T07:20:38.910Z</published>
    <updated>2018-10-01T07:20:38.910Z</updated>
    
    <content type="html"><![CDATA[<h3 id="解线性方程组"><a href="#解线性方程组" class="headerlink" title="解线性方程组"></a>解线性方程组</h3><p><strong>一、解线性方程组(3-7)</strong><br><br><code>矩阵消元法</code>　<code>阶梯型</code>　<code>主元</code></p><blockquote><p>1.对n个线性方程组成的线性方程组(即<code>n元线性方程组</code>)的增广矩阵做初等行变换得到一个阶梯型矩阵并记为J（J有$n+1$列，因为包含了<code>常数项</code>那一列），记J的非零行的个数为r. <br><br>证明：</p><ul><li>当出现<code>等号左边全是０，而右边非０</code>时（<em>这显然不可能的事儿</em>），该线性方程组<code>无解</code>；<br></li><li>当$r＝n$时，该线性方程组有<code>唯一解</code>；<br></li><li>当$r&lt;n$时,该线性方程组有<code>无穷解</code>；<br></li></ul></blockquote><p>证：<br></p><ul><li>第一条显然成立；<br></li><li><strong>在证明后面两个结论之前，我们先来证明$r&lt;=n$,即阶梯型矩阵非零行的数目$r$不可能大于（超过）未知量$n$的数目:</strong><br><br><img src="1.jpg" alt="矩阵J"><br><br>首先可以明确的是，J的第r个主元不能位于第$n+1$列，至多也只能在第$n$列，因此$t&lt;=n$，稍微想一下就知道，主元是指每一行第一个非零元素，上图中的元素$b$为第$r$行的主元，且位于第$t$列，主元不可能跑到常数项的那个第$n+1$列去，所以有$t&lt;=n$.<br><br>再来想想，每一行的主元所在列数不一定正好是对应的行数，比如在第二行中，$x_２$的系数在经过初等行变换之后为０，而$x_３$的系数非零.类似情况有很多，因此主元所在列数$t$往往是靠右，即主元所在列数$t$往往大于主元所在行数，而图中主元$b$所在的行数被我们设定为$r$，那就是说主元$b$所在列数$t$往往大于主元所在行数$r$，注意一点，我们这里的<code>往往大于</code>是指一般情况，可以取等号，即　$t&gt;=r$，从而有$r&lt;=n$，这就证明了<code>阶梯型矩阵非零行的数目r不可能大于（超过）未知量n的数目</code>.<br><br>等一下，再补充一些（更形象的解释一下）：在证明$t&gt;=r$时，在矩阵J中，b是最后一个主元，或者换句话说，b所在行的下边的行（我们也不知道具体有多少行，也有可能是０行，如果$r=n$），全部是零元素，那么上面的主元所在列由于是要成阶梯型的，可以脑补画面（阶梯型），这个阶梯，每一凳的砖块数不一样，最少是一块，那么也有可能下一凳（下一个主元）用了两块甚至更多块砖，就像Python的缩进一样，当每凳（每个主元）都只用１块砖时，就是最节省的情况（从左到右，一凳一凳的一个阶梯），此时恰好有$r=t$ ，而在有浪费的情况（每凳所用砖头数大于１）下，会<code>向右推进</code>，使得$t$变大，从而就有了$t&gt;=r$.　<br>总结下就是：<br><br><strong>$J$中主元所在列数$t$不可能跑到常数项所在列（第$n+1$列），所以有$t&lt;=n$；<br></strong><br><strong>$J$中主元所在行数$r$在不浪费的情况下也只能等于t,否则小于$t$，所以有$r&lt;=t$；<br></strong><br><strong>综上，有</strong><br>$$r&lt;=n$$</li><li>现在来证明当$r=n$时，该$n$元线性方程组有唯一解.<br><br>将阶梯型矩阵$J$继续进行变换，化为简化的阶梯型，记做$J_1$，如下图<br><center><img src="2.jpg" alt="矩阵J_1"></center><br><br>此时是$r=n$的，那么很明显，<br>$$(C_1,C_2，…，C_n)$$<br>就是该线性方程组的唯一解啦.<br></li><li>现在来证明当$r&lt;n$时，该$n$元线性方程组有无穷多个解.<br><br>经过初等行变换，第一行的主元总是可以在第一列位置处，而其他行的主元所在列位置则不一定正好与其对应的行数,如下图<br><center><img src="03.jpg" alt="矩阵J_1"></center><br>这里要注意啦，第$２$行的主元不一定在第２列，我们不妨标记其为$J_2$列，$J_2$不一定是$２$.<br><br>将所有的主变量（以主元为系数的变量）系数化为$１$，并移到等号左边，将自由未知量（所有$n$个变量除去主变量）移到等号右边，如下图<br><img src="4.jpg" alt="矩阵J_1"><br><br>由于$r&lt;n$，并且左边只有r个主变量，那么右边肯定有$n-r$个自由未知量.<br><br>自由未知量的取值不同，对应的该线性方程组的一组解也不同，从而证明了<code>当r&lt;n时，n元线性方程组有无穷多个解.</code><br><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">以上，便是全部的证明过程,Over~~~~~~~~~~~~~~~</span><br></pre></td></tr></table></figure></li></ul><p>现在，我们来讨论一下<strong>齐次线性方程组</strong>（常数项全为零的线性方程组）<br><br>显然，$(0，0，…　，0)$是原方程组的一组解，称为<strong>零解</strong>；<br><br>其余的解（如果存在）则叫做<strong>非零解</strong>.<br><br><em>$n$元齐次线性方程组有非零解的充分必要条件是：系数矩阵经过初等行变换化成的阶梯型矩阵的非零行数目$r&lt;n$.<br></em><br>但如果仅仅是来判断一个$n$元齐次线性方程组是否有非零解，还得每次进行初等行变换，好不麻烦，于是，针对于<strong>齐次</strong>的特殊性，我们有更简单的判别方法，那就是：<br></p><ul><li><strong><center>$n$元<code>齐次</code>线性方程组<code>有非零解</code>的<code>充分条件</code>是方程组中<code>方程的个数</code>$s$<code>小于</code>未知量的个数$n$.</center></strong></li><li>证明很简单：前面已经证过，当$r=n$时，该$n$元齐次线性方程组有唯一解；我们又知道，在$n$元<strong>齐次</strong>线性方程组中，由于常数项全为０，所以$(0，0，…　，0)$是原方程组的一组解（叫做零解），那么这个零解就是该$n$元齐次线性方程组出现的唯一解的情况.去除这种唯一解的情况，那就只剩下了无穷多个解的情况（不可能出现无解的情况，因为不管怎么样，都至少有一组零解了，怎能再无解？）了.按照之前的套路，对该方程组做初等行变换，记住，一共有$s$个方程，那么经过初等行变换之后，所得到的方程的个数$r$肯定小于或等于$s$，即<strong>$r&lt;=s$</strong>，又由于已知（条件）$s&lt;n$，因此我们得到$r&lt;n$，而这个结论正好是前面证过的关于$n$元线性方程组有无穷多个解的条件，这里的无穷多个解肯定全是非零解，因为对于齐次线性方程组来说，零解是必然存在的，而且这种情况已经被我们划分到方程组有唯一解的类别之中，那么另外一种情况，即有无穷多个解中，这无穷多个解肯定全是非零解了，要不然就矛盾了.这样子我们就完成了上述结论的简单证明.<br></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">再次提醒，我们刚才的证明的结论是针对于n元齐次线性方程组，齐次，齐次，齐次!而且只是**充分条件**</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;解线性方程组&quot;&gt;&lt;a href=&quot;#解线性方程组&quot; class=&quot;headerlink&quot; title=&quot;解线性方程组&quot;&gt;&lt;/a&gt;解线性方程组&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;一、解线性方程组(3-7)&lt;/strong&gt;&lt;br&gt;&lt;br&gt;&lt;code&gt;矩阵消元法&lt;/cod
      
    
    </summary>
    
      <category term="MathA" scheme="http://yoursite.com/categories/MathA/"/>
    
    
  </entry>
  
  <entry>
    <title>Python数据预处理</title>
    <link href="http://yoursite.com/2018/09/30/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/"/>
    <id>http://yoursite.com/2018/09/30/数据预处理/</id>
    <published>2018-09-30T10:29:25.740Z</published>
    <updated>2018-09-30T10:29:25.740Z</updated>
    
    <content type="html"><![CDATA[<p>标准化：数据的标准化（normalization）是将数据按比例缩放，使之落入一个小的特定区间。在某些比较和评价的指标处理中经常会用到，去除数据的单位限制，将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权。</p><p>标准化基于正态分布的假设，将数据变换为均值为0、标准差为1的标准正态分布。但即使数据不服从正态分布，也可以用此法。特别适用于数据的最大值和最小值未知，或存在孤立点。</p><p>主要方法：<br>z-score标准化，即零-均值标准化（常用方法）</p><p>$$y=\frac{x-μ}σ$$</p><p><del>~</del>~~ 下面看看在Python中的实现</p><p>方法１.<strong>scale</strong>可以直接对数组进行标准化，请看下例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line">X_train=np.array([[<span class="number">1</span>,<span class="number">50</span>,<span class="number">500</span>],[<span class="number">2</span>,<span class="number">40</span>,<span class="number">400</span>],[<span class="number">5</span>,<span class="number">55</span>,<span class="number">666</span>]])</span><br><span class="line">X_scaled=preprocessing.scale(X_train,axis=<span class="number">0</span>)<span class="comment">#axis默认值就是０，所以也可以不写</span></span><br><span class="line"><span class="keyword">print</span> X_scaled       <span class="comment">#标准化后的数据</span></span><br></pre></td></tr></table></figure><pre><code>[[-0.98058068  0.26726124 -0.20054214] [-0.39223227 -1.33630621 -1.11209733] [ 1.37281295  1.06904497  1.31263947]]</code></pre><p>咱们可以检验一下这个X_scaled的均值和方差</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> X_scaled.mean(axis=<span class="number">0</span>)<span class="comment">#均值</span></span><br><span class="line"><span class="keyword">print</span> X_scaled.std(axis=<span class="number">0</span>)<span class="comment">#方差</span></span><br></pre></td></tr></table></figure><pre><code>[  7.40148683e-17  -2.96059473e-16   0.00000000e+00][ 1.  1.  1.]</code></pre><p>注意这里的axis=0代表按行处理，也就是把行压缩，也就是对每一列进行标准化，常用！</p><p>方法２．<strong>from skelearn.preprocessing import StandardScaler</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scaler = preprocessing.StandardScaler()</span><br><span class="line">scaler</span><br></pre></td></tr></table></figure><pre><code>StandardScaler(copy=True, with_mean=True, with_std=True)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scaler.fit(X_train)</span><br></pre></td></tr></table></figure><pre><code>StandardScaler(copy=True, with_mean=True, with_std=True)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scaler.transform(X_train)</span><br></pre></td></tr></table></figure><pre><code>array([[-0.98058068,  0.26726124, -0.20054214],       [-0.39223227, -1.33630621, -1.11209733],       [ 1.37281295,  1.06904497,  1.31263947]])</code></pre><p>以上是把fit和transform两步分开进行的，我们也可以直接一步完成，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scaler.fit_transform(X_train)</span><br></pre></td></tr></table></figure><pre><code>array([[-0.98058068,  0.26726124, -0.20054214],       [-0.39223227, -1.33630621, -1.11209733],       [ 1.37281295,  1.06904497,  1.31263947]])</code></pre><p>但是要注意，在实际的建模过程中，我们通常将数据集划分为训练数据集和测试数据集，这时候我们应该分两步进行，先fit训练数据集，并将其定义为一个变量，比如ss,然后用ss来transform训练数据集从而进行模型的拟合，之后在检验模型的拟合度时，首先也要对测试数据集进行transform，这是就要用之前fit好的ss来transform测试数据集了，当然，这里只针对于变量数据，不包括target</p><p>同样可以用均值和方差来进行验证：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> scaler.fit_transform(X_train).mean(axis=<span class="number">0</span>)</span><br><span class="line"><span class="keyword">print</span> scaler.fit_transform(X_train).std(axis=<span class="number">0</span>)</span><br></pre></td></tr></table></figure><pre><code>[  7.40148683e-17  -2.96059473e-16   0.00000000e+00][ 1.  1.  1.]</code></pre><p>我们一般采用方法２，因为它可以保存训练集中的参数（均值、方差）直接使用其对象转换测试集数据.</p><p>其实，对数据进行标准化的数学方法不止上面这一个，还有以下几个：</p><ul><li>离差标准化</li></ul><p>则是对原始数据的一个线性变换，公式如下：</p><p>$$y=\frac{x-x_{min}}{x_{max}-x_{min}}$$</p><p>这种方法有个缺陷就是当有新数据加入时，可能导致$x_{max}$和$x_{min}$的变化，需要重新定义。</p><p>下面来编程模拟实现一个实例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x=np.array([[<span class="number">1.5</span>,<span class="number">8.8</span>,<span class="number">2.3</span>],[<span class="number">5.8</span>,<span class="number">5.0</span>,<span class="number">6.2</span>],[<span class="number">7.2</span>,<span class="number">8.3</span>,<span class="number">9.6</span>],[<span class="number">4.4</span>,<span class="number">5.5</span>,<span class="number">6.6</span>]])</span><br><span class="line">x</span><br></pre></td></tr></table></figure><pre><code>array([[ 1.5,  8.8,  2.3],       [ 5.8,  5. ,  6.2],       [ 7.2,  8.3,  9.6],       [ 4.4,  5.5,  6.6]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.shape</span><br></pre></td></tr></table></figure><pre><code>(4, 3)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_NEW=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>,x.shape[<span class="number">1</span>]):</span><br><span class="line">    x_new=(x[:,i]-min(x[:,i]))/(max(x[:,i])-min(x[:,i]))</span><br><span class="line">    <span class="keyword">print</span> x_new</span><br><span class="line">    X_NEW.append(x_new)</span><br></pre></td></tr></table></figure><pre><code>[ 0.          0.75438596  1.          0.50877193][ 1.          0.          0.86842105  0.13157895][ 0.          0.53424658  1.          0.5890411 ]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.array(X_NEW).transpose()<span class="comment">#最终数据</span></span><br></pre></td></tr></table></figure><pre><code>array([[ 0.        ,  1.        ,  0.        ],       [ 0.75438596,  0.        ,  0.53424658],       [ 1.        ,  0.86842105,  1.        ],       [ 0.50877193,  0.13157895,  0.5890411 ]])</code></pre><p>当然，我们也可以直接调用sklearn中的<strong>MinMaxScaler()</strong>来实现上述功能：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing   </span><br><span class="line">min_max_scaler = preprocessing.MinMaxScaler()  </span><br><span class="line">X_minMax = min_max_scaler.fit_transform(x)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X_minMax<span class="comment">#最终数据</span></span><br></pre></td></tr></table></figure><pre><code>array([[ 0.        ,  1.        ,  0.        ],       [ 0.75438596,  0.        ,  0.53424658],       [ 1.        ,  0.86842105,  1.        ],       [ 0.50877193,  0.13157895,  0.5890411 ]])</code></pre><p>结果是一模一样的！</p><p>为了方便起见，我们今后就直接调用MinMaxScaler() 就好了.</p><p>离差标准化可以扩展一下，比如我们想要把数据映射到－１和１之间，那么就采用以下数学公式：</p><p>$$x_{new}=\frac{x-x_{mean}}{x_{max}-x_{min}}$$</p><p>编程模拟一下，直接对之前的代码做一些改动就可以了，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x=np.array([[<span class="number">1.0</span>,<span class="number">2.2</span>,<span class="number">3.3</span>],[<span class="number">5.2</span>,<span class="number">3.3</span>,<span class="number">2.2</span>],[<span class="number">1.3</span>,<span class="number">2.5</span>,<span class="number">6.8</span>]])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X_NEW=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(x.shape[<span class="number">1</span>]):</span><br><span class="line">    x_new=(x[:,i]-np.mean(x[:,i]))/(max(x[:,i])-min(x[:,i]))</span><br><span class="line">    <span class="keyword">print</span> x_new</span><br><span class="line">    X_NEW.append(x_new)</span><br></pre></td></tr></table></figure><pre><code>[-0.56578947  0.18859649  0.43421053 -0.05701754][ 0.5        -0.5         0.36842105 -0.36842105][-0.53082192  0.00342466  0.46917808  0.05821918]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.array(X_NEW).transpose()<span class="comment">#最终数据</span></span><br></pre></td></tr></table></figure><pre><code>array([[-0.56578947,  0.5       , -0.53082192],       [ 0.18859649, -0.5       ,  0.00342466],       [ 0.43421053,  0.36842105,  0.46917808],       [-0.05701754, -0.36842105,  0.05821918]])</code></pre><p>＊＊＊</p><p>以上都是些常用的数据标准化方法，还有一些不太常用的方法，比如：</p><ul><li>对数Logistic模式：</li></ul><p>$$X_{new}=\frac{1}{1+e^{-X_{old}}}$$</p><p>得出的数都在０和１之间</p><p>最后来说一下<strong>数据正则化</strong></p><p>正则化主要是用于解决过拟合，正则性衡量了函数光滑的程度，正则性越高，函数越光滑。（光滑衡量了函数的可导性，如果一个函数是光滑函数，则该函数无穷可导，即任意n阶可导）.<br><br>采用正则化方法会自动削弱不重要的特征变量，自动从许多的特征变量中”提取“重要的特征变量，减小特征变量的数量级。这个方法非常有效，当我们有很多特征变量时，其中每一个变量都能对预测产生一点影响。我们可以有很多特征变量，其中每一个变量都是有用的，因此我们不希望把它们删掉，这就导致了正则化概念的发生。</p><p>看一下在sklearn中的调用方法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> normalize</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x=np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]])</span><br><span class="line">x</span><br></pre></td></tr></table></figure><pre><code>array([[1, 2, 3],       [4, 5, 6],       [7, 8, 9]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">normalizer = preprocessing.Normalizer().fit(x)  <span class="comment"># fit does nothing</span></span><br><span class="line">normalizer</span><br></pre></td></tr></table></figure><pre><code>Normalizer(copy=True, norm=&apos;l2&apos;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">normalizer.transform(x)<span class="comment">#最终结果</span></span><br></pre></td></tr></table></figure><pre><code>array([[ 0.26726124,  0.53452248,  0.80178373],       [ 0.45584231,  0.56980288,  0.68376346],       [ 0.50257071,  0.57436653,  0.64616234]])</code></pre><p>今天就写到这儿吧，有时间继续，如果能帮到你，还请关注下微信公众号“我将在南极找寻你”，更多干货尽在其中！</p><p>参考：<br> <a href="https://blog.csdn.net/gshgsh1228/article/details/52199870/" target="_blank" rel="noopener">https://blog.csdn.net/gshgsh1228/article/details/52199870/</a>　<br><br><a href="https://www.jianshu.com/p/0d8bb02f98fb" target="_blank" rel="noopener">https://www.jianshu.com/p/0d8bb02f98fb</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;标准化：数据的标准化（normalization）是将数据按比例缩放，使之落入一个小的特定区间。在某些比较和评价的指标处理中经常会用到，去除数据的单位限制，将其转化为无量纲的纯数值，便于不同单位或量级的指标能够进行比较和加权。&lt;/p&gt;
&lt;p&gt;标准化基于正态分布的假设，将数据
      
    
    </summary>
    
      <category term="Data Analysis&amp;Mining with Python" scheme="http://yoursite.com/categories/Data-Analysis-Mining-with-Python/"/>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Python回归拟合</title>
    <link href="http://yoursite.com/2018/09/30/reg/"/>
    <id>http://yoursite.com/2018/09/30/reg/</id>
    <published>2018-09-30T10:24:21.282Z</published>
    <updated>2018-09-30T10:24:21.282Z</updated>
    
    <content type="html"><![CDATA[<ul><li>线性回归</li></ul><p>线性回归的经典解法是最小二乘法，关于最小二乘法的原理网络上都有介绍，咱们这里只看在sklearn中的实现</p><p>我们将采用sklearn自带的美国波斯顿房价数据集进行演示</p><p>首先导入数据并查看数据的基本信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset=load_boston()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">type(dataset)<span class="comment">#数据类型是sklearn的数据集类型</span></span><br></pre></td></tr></table></figure><pre><code>sklearn.datasets.base.Bunch</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset.data.shape<span class="comment">#自变量的维度</span></span><br></pre></td></tr></table></figure><pre><code>(506, 13)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataset.target.shape<span class="comment">#因变量的维度</span></span><br></pre></td></tr></table></figure><pre><code>(506,)</code></pre><p>现在来分割数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment"># 随机采样25%的数据构建测试样本，其余作为训练样本。</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target,random_state=<span class="number">33</span>, test_size=<span class="number">0.25</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分析回归目标值的差异。</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"The max target value is"</span>, np.max(dataset.target)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"The min target value is"</span>, np.min(dataset.target)</span><br><span class="line"><span class="keyword">print</span> <span class="string">"The average target value is"</span>, np.mean(dataset.target)</span><br></pre></td></tr></table></figure><pre><code>The max target value is 50.0The min target value is 5.0The average target value is 22.5328063241</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.hist(dataset.target)</span><br><span class="line">plt.show()</span><br><span class="line">plt.hist(dataset.data)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_13_0.png" alt="png"></p><p><img src="output_13_1.png" alt="png"></p><p>发现差异较大，所以先进行标准化处理，关于标准化的方法，已经在上一篇文章中讲过，忘记的朋友可以去翻翻看</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#标准化数据</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line">ss_x=StandardScaler()</span><br><span class="line">ss_y=StandardScaler()</span><br><span class="line">X_train=ss_x.fit_transform(X_train)</span><br><span class="line">X_test=ss_x.transform(X_test)</span><br><span class="line">y_train=ss_y.fit_transform(y_train)</span><br><span class="line">y_test=ss_y.transform(y_test)</span><br></pre></td></tr></table></figure><pre><code>/home/fantasy/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:586: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)/home/fantasy/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)/home/fantasy/anaconda2/lib/python2.7/site-packages/sklearn/preprocessing/data.py:649: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.  warnings.warn(DEPRECATION_MSG_1D, DeprecationWarning)</code></pre><p>标准化之后，就要开始拟合模型了</p><p>基于最小二乘法的LinearRegression：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">lr = LinearRegression()</span><br><span class="line">lr.fit(X_train, y_train)<span class="comment">#拟合模型</span></span><br><span class="line">lr_y_predict = lr.predict(X_test)<span class="comment">#做预测</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#评估模型</span></span><br><span class="line"><span class="comment"># 使用LinearRegression模型自带的评估模块，并输出评估结果。</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'The value of default measurement of LinearRegression is'</span>, lr.score(X_test, y_test)</span><br></pre></td></tr></table></figure><pre><code>The value of default measurement of LinearRegression is 0.6763403831</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从sklearn.metrics依次导入r2_score、mean_squared_error以及mean_absoluate_error用于回归性能的评估。</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score, mean_squared_error, mean_absolute_error</span><br><span class="line"><span class="comment"># 使用r2_score模块，并输出评估结果。</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'The value of R-squared of LinearRegression is'</span>, r2_score(y_test, lr_y_predict)</span><br></pre></td></tr></table></figure><pre><code>The value of R-squared of LinearRegression is 0.6763403831</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#可视化因变量</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">plt.scatter(np.arange(len(y_test)),y_test,color = <span class="string">'red'</span>,label=<span class="string">'y_test'</span>)</span><br><span class="line">plt.scatter(np.arange(len(lr_y_predict)),lr_y_predict,color = <span class="string">'blue'</span>,label=<span class="string">'y_pred'</span>)</span><br><span class="line">plt.legend(loc=<span class="number">2</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_21_0.png" alt="png"></p><p>拟合效果还不错</p><p>在模型评估时，两种方式是一样的，以后直接用第一种，即模型自带的score就可以了</p><p>但是，一个拟合出来的模型并不是直接可以拿来用的。还需要对其统计性质进行检验</p><p>主要有以下四个检验：<br>（数值型）自变量要与因变量有线性关系；<br>残差基本呈正态分布；<br>残差方差基本不变（同方差性）；<br>残差（样本）间相关独立。</p><p>第一个可以直接绘制每隔变量与因变量之间的散点图（子图）,还是以波斯顿房价为例进行演示，如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">xlabel=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">13</span>):</span><br><span class="line">    x_i=np.array(dataset.data[:,i])</span><br><span class="line">    xlabel.append(x_i)</span><br><span class="line">    plt.style.use(<span class="string">'seaborn'</span>)</span><br><span class="line">    figurei=plt.subplot(<span class="number">2</span>,<span class="number">7</span>,i+<span class="number">1</span>)</span><br><span class="line">    <span class="comment">#figurei.patch.set_facecolor('blue')</span></span><br><span class="line">    figurei.scatter(x_i,dataset.target)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_27_0.png" alt="png"></p><p>检验残差是否基本上呈正态分布也建议直接Spss</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#不确定,建议SPSS</span></span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line">stats.probplot(dataset.target,dist=<span class="string">"norm"</span>, plot=plt)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_29_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#不确定，建议SPSS</span></span><br><span class="line">d= dataset.target</span><br><span class="line">sorted_ = np.sort(d)</span><br><span class="line">yvals = np.arange(len(sorted_))/float(len(sorted_))</span><br><span class="line">plt.plot(sorted_, yvals)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_30_0.png" alt="png"></p><p>共线性检验可直接上Spss,看VIF,简单粗暴</p><hr><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#这个是绘制VIF的程序，没看懂，以后再研究</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">vif2=np.zeros((<span class="number">15</span>,<span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">15</span>):</span><br><span class="line">    tmp=[k <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">13</span>) <span class="keyword">if</span> k!=i]</span><br><span class="line">    <span class="comment">#clf.fit(X2[:,tmp],X2[:,i])</span></span><br><span class="line">    vifi=<span class="number">1</span>/(<span class="number">1</span>-lr.score(X_test, y_test))</span><br><span class="line">    vif2[i]=vifi</span><br><span class="line"></span><br><span class="line">vif3=np.zeros((<span class="number">15</span>,<span class="number">1</span>))</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">15</span>):</span><br><span class="line">    tmp=[k <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">15</span>) <span class="keyword">if</span> k!=i]</span><br><span class="line">    <span class="comment">#clf.fit(X3[:,tmp],X3[:,i])</span></span><br><span class="line">    vifi=<span class="number">1</span>/(<span class="number">1</span>-lr.score(X_test, y_test))</span><br><span class="line">    vif3[i]=vifi  </span><br><span class="line">    </span><br><span class="line">plt.figure()</span><br><span class="line">ax = plt.gca()</span><br><span class="line">ax.plot(vif2)</span><br><span class="line">ax.plot(vif3)</span><br><span class="line">plt.xlabel(<span class="string">'feature'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'VIF'</span>)</span><br><span class="line">plt.title(<span class="string">'VIF coefficients of the features'</span>)</span><br><span class="line">plt.axis(<span class="string">'tight'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_33_0.png" alt="png"></p><p>说完了基于最小二乘法的线性回归，咱们接下来看一个<strong>随机梯度下降原理</strong>拟合的线性回归模型</p><p>所谓梯度下降法，就是利用沿着梯度下降最快的方向求偏导数，得到损失函数的全局最小值时的参数θ，涉及到偏导数、学习速度、更新、收敛等问题。</p><p>不过这里我们并不讨论这些，具体的可以看这篇文章<a href="https://github.com/endymecy/spark-ml-source-analysis/blob/master/%E6%9C%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/gradient-descent.md" target="_blank" rel="noopener">https://github.com/endymecy/spark-ml-source-analysis/blob/master/%E6%9C%80%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D/gradient-descent.md</a>　而是在sklearn中实现它，如下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line">model = SGDRegressor()</span><br><span class="line">model.fit(X_train,y_train)<span class="comment">#拟合模型</span></span><br></pre></td></tr></table></figure><pre><code>SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,       fit_intercept=True, l1_ratio=0.15, learning_rate=&apos;invscaling&apos;,       loss=&apos;squared_loss&apos;, n_iter=5, penalty=&apos;l2&apos;, power_t=0.25,       random_state=None, shuffle=True, verbose=0, warm_start=False)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sgdr_y_predict=model.predict(X_test)<span class="comment">#做预测</span></span><br></pre></td></tr></table></figure><p>可视化结果y的真实值和预测值之间的差距：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(np.arange(len(y_test)),y_test,color = <span class="string">'red'</span>,label=<span class="string">'y_test'</span>)</span><br><span class="line">plt.scatter(np.arange(len(sgdr_y_predict)),sgdr_y_predict,color = <span class="string">'blue'</span>,label=<span class="string">'y_pre'</span>)</span><br><span class="line">plt.legend(loc=<span class="number">2</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_40_0.png" alt="png"></p><p>看一下R方：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#模型model自带的</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"R_square:"</span>,model.score(X_test,y_test)</span><br></pre></td></tr></table></figure><pre><code>R_square: 0.66058562575</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从sklearn.metrics依次导入r2_score、mean_squared_error以及mean_absoluate_error用于回归性能的评估。</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score, mean_squared_error, mean_absolute_error</span><br><span class="line"><span class="comment"># 使用r2_score模块，并输出评估结果。</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'The value of R-squared of LinearRegression is'</span>, r2_score(y_test, sgdr_y_predict)</span><br></pre></td></tr></table></figure><pre><code>The value of R-squared of LinearRegression is 0.66058562575</code></pre><p>还有一种方法，就是用<strong>岭回归</strong></p><p>岭回归是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法，对病态数据的拟合要强于最小二乘法。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> linear_model</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge,RidgeCV   <span class="comment"># Ridge岭回归,RidgeCV带有广义交叉验证的岭回归</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ========岭回归========</span></span><br><span class="line">model = Ridge(alpha=<span class="number">0.5</span>)</span><br><span class="line">model = RidgeCV(alphas=[<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">10.0</span>])  <span class="comment"># 通过RidgeCV可以设置多个参数值，算法使用交叉验证获取最佳参数值</span></span><br><span class="line">model.fit(X_train, y_train)   <span class="comment"># 线性回归建模</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'系数矩阵:\n'</span>,model.coef_,model.intercept_</span><br><span class="line"><span class="keyword">print</span> <span class="string">'线性回归模型:\n'</span>,model</span><br><span class="line"><span class="comment"># print('交叉验证最佳alpha值',model.alpha_)  # 只有在使用RidgeCV算法时才有效</span></span><br><span class="line"><span class="comment"># 使用模型预测</span></span><br><span class="line">predicted = model.predict(X_test)</span><br></pre></td></tr></table></figure><pre><code>系数矩阵:[-0.10354081  0.11293307 -0.01049108  0.09295071 -0.15094031  0.32557661 -0.02033021 -0.2991313   0.20061662 -0.15572242 -0.19759762  0.05583187 -0.39404276] 5.52785513551e-15线性回归模型:RidgeCV(alphas=[0.1, 1.0, 10.0], cv=None, fit_intercept=True, gcv_mode=None,    normalize=False, scoring=None, store_cv_values=False)</code></pre><p>结果可视化</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(np.arange(len(y_test)),y_test,color = <span class="string">'red'</span>,label=<span class="string">'y_test'</span>)</span><br><span class="line">plt.scatter(np.arange(len(sgdr_y_predict)),sgdr_y_predict,color = <span class="string">'blue'</span>,label=<span class="string">'y_pre'</span>)</span><br><span class="line">plt.legend(loc=<span class="number">2</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_50_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#模型model自带的</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"R_square:"</span>,model.score(X_test,y_test)</span><br></pre></td></tr></table></figure><pre><code>R_square: 0.67691092236</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从sklearn.metrics依次导入r2_score、mean_squared_error以及mean_absoluate_error用于回归性能的评估。</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score, mean_squared_error, mean_absolute_error</span><br><span class="line"><span class="comment"># 使用r2_score模块，并输出评估结果。</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'The value of R-squared of LinearRegression is'</span>, r2_score(y_test, predicted)</span><br></pre></td></tr></table></figure><pre><code>The value of R-squared of LinearRegression is 0.67691092236</code></pre><p>综合上面三种方法的比较，发现岭回归的效果最好</p><p>线性模型掌握这三个完全够用了，下面来看一下非线性模型的回归拟合，主要是关于多项式拟合的，其余的对数，指数拟合这里不再讨论</p><ul><li>多项式拟合</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入线性模型和多项式特征构造模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span>  PolynomialFeatures</span><br><span class="line">poly_reg_x =PolynomialFeatures(degree=<span class="number">2</span>)</span><br><span class="line"><span class="comment">#poly_reg_y =PolynomialFeatures(degree=2)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> X_train.shape<span class="comment">#之前的ｓｉｚｅ</span></span><br><span class="line"><span class="keyword">print</span> X_test.shape</span><br></pre></td></tr></table></figure><pre><code>(379, 13)(127, 13)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">X_train=poly_reg_x.fit_transform(X_train)<span class="comment">#每个训练集都被transform</span></span><br><span class="line">X_test=poly_reg_x.transform(X_test)</span><br><span class="line"><span class="comment">#y_train=poly_reg_y.fit_transform(y_train)</span></span><br><span class="line"><span class="comment">#y_test=poly_reg_y.transform(y_test)</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> X_train.shape<span class="comment">#之后的ｓｉｚｅ</span></span><br><span class="line"><span class="keyword">print</span> X_test.shape</span><br></pre></td></tr></table></figure><pre><code>(379, 105)(127, 105)</code></pre><p>在构造完多项式特征之后，就可以用之前的线性回归lr来操作了</p><p>注意：在先对数据标准化之后再构造多项式特征与先构造多项式特征再标准化的结果差距很大，就本例而言，前者似乎更有效</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">lr=LinearRegression()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">modeler=lr.fit(X_train,y_train)<span class="comment">#模型拟合</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poly_y_predict=modeler.predict(X_test)<span class="comment">#做预测</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#模型model自带的</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"R_square:"</span>,modeler.score(X_test,y_test)</span><br></pre></td></tr></table></figure><pre><code>R_square: 0.842818486817</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从sklearn.metrics依次导入r2_score、mean_squared_error以及mean_absoluate_error用于回归性能的评估。</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score, mean_squared_error, mean_absolute_error</span><br><span class="line"><span class="comment"># 使用mean_squared_error模块，并输出评估结果。</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'The value of mean_squared_error of LinearRegression is'</span>, mean_squared_error(y_test, predicted)</span><br></pre></td></tr></table></figure><pre><code>The value of mean_squared_error of LinearRegression is 0.290920352888</code></pre><p>均方误差如此小，模型堪称完美</p><p>模型效果可视化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(np.arange(len(y_test)),y_test,color = <span class="string">'red'</span>,label=<span class="string">'y_test'</span>)</span><br><span class="line">plt.scatter(np.arange(len(sgdr_y_predict)),sgdr_y_predict,color = <span class="string">'blue'</span>,label=<span class="string">'y_pre'</span>)</span><br><span class="line">plt.legend(loc=<span class="number">2</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_69_0.png" alt="png"></p><p>以上是在sklearn中的多项式拟合方法，我们可以查看下模型的系数，比较多,这算是一个缺点了（模型难写，容易过拟合）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> modeler.coef_<span class="comment">#由低阶到高阶</span></span><br></pre></td></tr></table></figure><pre><code>[  9.83736707e-13  -2.66737655e-03   3.16462828e-01   1.25375928e+00   3.17151400e+12  -1.61912385e-01   3.80770585e-01  -2.70605062e-01  -2.30644623e-01   6.36550903e-01  -1.23194122e+00   1.82800293e-01   1.47033691e-01  -3.51562500e-01   9.76562500e-03   1.60988998e+00   2.96385193e+00   5.17631531e-01  -2.71759033e-02   5.75256348e-02  -1.39862061e-01  -3.11294556e-01   1.75088501e+00  -4.04202271e+00   9.93446350e-01  -9.21630859e-03   1.07109070e-01  -4.19921875e-02  -4.53796387e-02  -1.73645020e-02  -2.53723145e-01   2.30712891e-02  -3.18298340e-02   1.45568848e-02  -5.79681396e-02   1.94564819e-01  -5.81054688e-02   2.40783691e-02  -1.19384766e-01   1.56875610e-01  -2.15034485e-02   2.81250000e-01   1.61010742e-01   3.10821533e-02   3.52600098e-01   6.44836426e-02  -4.64248657e-02   1.86462402e-02   7.94677734e-02  -3.62548828e-02  -9.95393091e+11  -1.26373291e-01  -9.04617310e-02   6.21032715e-03  -2.34451294e-02  -4.63104248e-02   8.61663818e-02  -5.27343750e-02   3.11126709e-02  -4.30259705e-02  -1.50436401e-01   7.12280273e-02  -1.96792603e-01   1.67648315e-01  -1.43829346e-01   2.98084259e-01  -2.63671875e-01   3.46069336e-02   9.91134644e-02   4.61425781e-02  -1.51935577e-01  -1.54113770e-03  -6.87255859e-02  -2.09899902e-01  -5.36499023e-02  -7.35473633e-03  -7.93457031e-02   1.32598877e-02  -2.28881836e-03   4.61242676e-01  -2.49618530e-01  -2.85339355e-02  -1.33331299e-01  -1.42181396e-01   1.50909424e-01  -7.42797852e-02  -1.14502907e-01  -5.12084961e-02   4.06494141e-02   9.94567871e-02  -8.89060974e-01   8.14544678e-01  -1.85592651e-01  -5.57861328e-02  -2.31964111e-01  -5.03234863e-02   1.87805176e-01   2.02636719e-02  -1.73187256e-02   4.75559235e-02   2.38952637e-02   3.66210938e-03  -3.41796875e-03  -2.86254883e-02   6.39343262e-02]</code></pre><p>以上也是基于最小二乘原理的，因为我们只是用sklearn的多项式构造模块将原来的线性数据通过列方向的扩充，变成了多项式的形式，但还是用的LinearRegression来拟合模型的，那么，我们可以试一下别的原理，比如下面的<strong>岭回归</strong>拟合多项式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line">ridge=Ridge(alpha=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> X_train.shape<span class="comment">#之前的ｓｉｚｅ</span></span><br><span class="line"><span class="keyword">print</span> X_test.shape</span><br></pre></td></tr></table></figure><pre><code>(379, 13)(127, 13)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#导入构造多项式特征模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span>  PolynomialFeatures</span><br><span class="line">poly_reg =PolynomialFeatures(degree=<span class="number">2</span>)</span><br></pre></td></tr></table></figure><h3 id="在下一步之前对原始数据进行了标准化！！！"><a href="#在下一步之前对原始数据进行了标准化！！！" class="headerlink" title="在下一步之前对原始数据进行了标准化！！！"></a>在下一步之前对原始数据进行了标准化！！！</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在这一步之前对原始数据进行了标准化！！！</span></span><br><span class="line">X_train=poly_reg.fit_transform(X_train)<span class="comment">#每个训练集都被transform</span></span><br><span class="line">X_test=poly_reg.transform(X_test)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> X_train.shape<span class="comment">#之后的ｓｉｚｅ</span></span><br><span class="line"><span class="keyword">print</span> X_test.shape</span><br></pre></td></tr></table></figure><pre><code>(379, 105)(127, 105)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poly_ridge=ridge.fit(X_train,y_train)<span class="comment">#模型拟合</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">poly_ridge_y_predict=ridge.predict(X_test)<span class="comment">#做预测</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#模型ridge自带的</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">"R_square:"</span>,ridge.score(X_test,y_test)</span><br></pre></td></tr></table></figure><pre><code>R_square: 0.846155705955</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#从sklearn.metrics依次导入r2_score、mean_squared_error以及mean_absoluate_error用于回归性能的评估。</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> r2_score, mean_squared_error, mean_absolute_error</span><br><span class="line"><span class="comment"># 使用mean_squared_error模块，并输出评估结果。</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'The value of mean_squared_error of RidgeRegression is'</span>, mean_squared_error(y_test, poly_ridge_y_predict)</span><br></pre></td></tr></table></figure><pre><code>The value of mean_squared_error of RidgeRegression is 0.138526615137</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看模型的系数</span></span><br><span class="line"><span class="keyword">print</span> ridge.coef_,ridge.intercept_</span><br></pre></td></tr></table></figure><pre><code>[ 0.         -0.01515184 -0.10580862  0.27932288  0.01645974 -0.14657861  0.36744518 -0.22397917 -0.21912044  0.05965385 -0.04161497 -0.08866449  0.11792374 -0.3637897   0.01224963  0.04046505  0.16591023  0.47025105 -0.0426397   0.06610476 -0.07187838 -0.14978614 -0.23375497 -0.01411628  0.05016413 -0.00793163  0.09939217 -0.0134973  -0.02031623  0.00222154 -0.13674295  0.02549065 -0.02315901  0.00183563 -0.00664953  0.17951566 -0.02818604 -0.03342595 -0.10510401  0.10889808 -0.00633295  0.33583991  0.14526388  0.04291548  0.32826641  0.07628581  0.00221103 -0.0020726  0.03954039 -0.02489515  0.05244391 -0.11941144 -0.08827233  0.01151196 -0.028727   -0.0410782   0.06641088 -0.0236821  -0.00505518 -0.04825191 -0.12339398  0.0680945  -0.1614648   0.13523431 -0.08524669  0.11271328 -0.182551    0.03326487  0.10387014  0.04437453 -0.14262386  0.00168108 -0.06360327 -0.20487222 -0.06044155 -0.01195337 -0.08105273  0.01500186  0.01720694  0.32904656 -0.16341483 -0.03929378 -0.13649985 -0.14039058  0.14996113 -0.11682082 -0.09929801 -0.06146238  0.0137472   0.07554982 -0.50475006  0.39750343 -0.098317   -0.06266169 -0.16932652 -0.04422031  0.18347525  0.04147819 -0.10451011  0.0364601   0.0112839   0.02664297 -0.00190007 -0.02998467  0.07018101] -0.190718574726</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#可视化效果</span></span><br><span class="line">plt.scatter(np.arange(len(y_test)),y_test,color = <span class="string">'red'</span>,label=<span class="string">'y_test'</span>)</span><br><span class="line">plt.scatter(np.arange(len(poly_ridge_y_predict)),poly_ridge_y_predict,color = <span class="string">'blue'</span>,label=<span class="string">'y_pre'</span>)</span><br><span class="line">plt.legend(loc=<span class="number">2</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_84_0.png" alt="png"></p><p>总结一下关于sklearn中的PolynomialFeatures的用法，就是<strong>最好在构造多项式特征之前对原始的数据（x和y）进行标准化处理</strong>，然后就可以使用基于最小二乘法的LinearRegression或者基于别的原理的RidgeRegression了.</p><hr><p>其实，在numpy中也有多项式拟合的模块，只是只能拟合一元的多项式，即一个自变量和一个因变量，下面就一起来看一下</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">z1 = np.polyfit(X_train[:,<span class="number">1</span>], y_train, <span class="number">1</span>)  <span class="comment">#一次多项式拟合，相当于线性拟合,返回的是[k,b]，即模型的系数</span></span><br><span class="line">p1 = np.poly1d(z1)<span class="comment">#给出模型表达式，真ｔｍ人性化</span></span><br><span class="line"><span class="keyword">print</span> z1  <span class="comment">#[ 1.          1.49333333]</span></span><br><span class="line"><span class="keyword">print</span> p1  <span class="comment"># 1 x + 1.493</span></span><br></pre></td></tr></table></figure><pre><code>[  0.13493869  21.35130147]0.1349 x + 21.35</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z = np.polyval(z1, X_train[:,<span class="number">1</span>])<span class="comment">#用刚刚拟合处理的模型z1来代入X_train[:,1]求得预模型的测值并保存在z中</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">z</span><br></pre></td></tr></table></figure><pre><code>array([ 24.05007536,  21.35130147,  21.35130147,  28.77292967,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  34.17047745,  24.05007536,  23.03803515,        21.35130147,  21.35130147,  21.35130147,  24.05007536,        25.93921708,  32.14639703,  21.35130147,  25.80427839,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  32.14639703,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  24.31995275,  21.35130147,        21.35130147,  26.07415578,  21.35130147,  21.35130147,        32.14639703,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  24.31995275,  21.35130147,        24.72476883,  24.05007536,  21.35130147,  21.35130147,        21.35130147,  24.05007536,  25.12958491,  21.35130147,        21.35130147,  24.72476883,  21.35130147,  32.82109051,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  32.14639703,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  26.07415578,  28.43558293,        21.35130147,  31.47170356,  34.17047745,  24.18501405,        24.05007536,  21.35130147,  21.35130147,  24.05007536,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        32.14639703,  25.3994623 ,  21.35130147,  21.35130147,        21.35130147,  24.05007536,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        24.05007536,  24.72476883,  24.05007536,  21.35130147,        24.72476883,  23.03803515,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        32.14639703,  21.35130147,  25.93921708,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  23.03803515,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  26.74884925,  21.35130147,  28.43558293,        21.35130147,  21.35130147,  29.44762314,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  25.80427839,        24.05007536,  25.3994623 ,  21.35130147,  21.35130147,        21.35130147,  23.03803515,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  25.12958491,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        32.14639703,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  25.12958491,  24.72476883,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  28.77292967,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  26.74884925,  24.05007536,  26.74884925,        21.35130147,  21.35130147,  21.35130147,  24.05007536,        23.03803515,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  27.42354272,  21.35130147,  21.35130147,        24.31995275,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  24.05007536,        27.42354272,  21.35130147,  24.31995275,  21.35130147,        27.42354272,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  28.43558293,  21.35130147,  32.14639703,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  24.05007536,  21.35130147,        31.47170356,  25.80427839,  24.72476883,  26.74884925,        21.35130147,  21.35130147,  24.05007536,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  24.31995275,  21.35130147,  21.35130147,        24.18501405,  33.49578398,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  30.79701009,        21.35130147,  29.44762314,  21.35130147,  32.48374377,        21.35130147,  21.35130147,  21.35130147,  32.14639703,        32.14639703,  21.35130147,  24.31995275,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  25.3994623 ,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        33.49578398,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  23.03803515,  31.47170356,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        27.42354272,  24.05007536,  24.72476883,  34.17047745,        33.49578398,  24.18501405,  21.35130147,  21.35130147,        21.35130147,  24.05007536,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  24.31995275,        21.35130147,  21.35130147,  23.03803515,  21.35130147,        23.03803515,  24.72476883,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  28.77292967,        21.35130147,  21.35130147,  24.31995275,  21.35130147,        21.35130147,  24.72476883,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        32.14639703,  25.93921708,  21.35130147,  21.35130147,        21.35130147,  24.72476883,  21.35130147,  32.14639703,        34.84517093,  32.48374377,  21.35130147,  32.14639703,        21.35130147,  21.35130147,  21.35130147])</code></pre><p>或者我们直接把自变量的值代入拟合好的方程里面,得到的结果和上面的一样.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_pre=p1(X_train[:,<span class="number">1</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_pre</span><br></pre></td></tr></table></figure><pre><code>array([ 24.05007536,  21.35130147,  21.35130147,  28.77292967,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  34.17047745,  24.05007536,  23.03803515,        21.35130147,  21.35130147,  21.35130147,  24.05007536,        25.93921708,  32.14639703,  21.35130147,  25.80427839,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  32.14639703,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  24.31995275,  21.35130147,        21.35130147,  26.07415578,  21.35130147,  21.35130147,        32.14639703,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  24.31995275,  21.35130147,        24.72476883,  24.05007536,  21.35130147,  21.35130147,        21.35130147,  24.05007536,  25.12958491,  21.35130147,        21.35130147,  24.72476883,  21.35130147,  32.82109051,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  32.14639703,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  26.07415578,  28.43558293,        21.35130147,  31.47170356,  34.17047745,  24.18501405,        24.05007536,  21.35130147,  21.35130147,  24.05007536,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        32.14639703,  25.3994623 ,  21.35130147,  21.35130147,        21.35130147,  24.05007536,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        24.05007536,  24.72476883,  24.05007536,  21.35130147,        24.72476883,  23.03803515,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        32.14639703,  21.35130147,  25.93921708,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  23.03803515,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  26.74884925,  21.35130147,  28.43558293,        21.35130147,  21.35130147,  29.44762314,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  25.80427839,        24.05007536,  25.3994623 ,  21.35130147,  21.35130147,        21.35130147,  23.03803515,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  25.12958491,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        32.14639703,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  25.12958491,  24.72476883,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  28.77292967,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  26.74884925,  24.05007536,  26.74884925,        21.35130147,  21.35130147,  21.35130147,  24.05007536,        23.03803515,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  27.42354272,  21.35130147,  21.35130147,        24.31995275,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  24.05007536,        27.42354272,  21.35130147,  24.31995275,  21.35130147,        27.42354272,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  28.43558293,  21.35130147,  32.14639703,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  24.05007536,  21.35130147,        31.47170356,  25.80427839,  24.72476883,  26.74884925,        21.35130147,  21.35130147,  24.05007536,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  24.31995275,  21.35130147,  21.35130147,        24.18501405,  33.49578398,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  30.79701009,        21.35130147,  29.44762314,  21.35130147,  32.48374377,        21.35130147,  21.35130147,  21.35130147,  32.14639703,        32.14639703,  21.35130147,  24.31995275,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  25.3994623 ,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        33.49578398,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  23.03803515,  31.47170356,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        27.42354272,  24.05007536,  24.72476883,  34.17047745,        33.49578398,  24.18501405,  21.35130147,  21.35130147,        21.35130147,  24.05007536,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  24.31995275,        21.35130147,  21.35130147,  23.03803515,  21.35130147,        23.03803515,  24.72476883,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  28.77292967,        21.35130147,  21.35130147,  24.31995275,  21.35130147,        21.35130147,  24.72476883,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        21.35130147,  21.35130147,  21.35130147,  21.35130147,        32.14639703,  25.93921708,  21.35130147,  21.35130147,        21.35130147,  24.72476883,  21.35130147,  32.14639703,        34.84517093,  32.48374377,  21.35130147,  32.14639703,        21.35130147,  21.35130147,  21.35130147])</code></pre><p>这种就可以直观的可视化真实值与预测曲线之间的关系了，如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(X_train[:,<span class="number">1</span>], y_train,color=<span class="string">'red'</span>,label=<span class="string">'true'</span>)</span><br><span class="line">plt.plot(X_train[:,<span class="number">1</span>],y_pre,color=<span class="string">'blue'</span>,label=<span class="string">'pre'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'best'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_95_0.png" alt="png"></p><p>这里我在网上找了一个numpy拟合多项式的例子，贴在下面了，供大家参考</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#多项式拟合(从给定的x,y中解析出最接近数据的方程式)</span></span><br><span class="line"><span class="comment">#要拟合的x,y数据</span></span><br><span class="line">x = np.arange(<span class="number">1</span>, <span class="number">17</span>, <span class="number">1</span>)</span><br><span class="line">y = np.array([<span class="number">4.00</span>, <span class="number">6.40</span>, <span class="number">8.00</span>, <span class="number">8.80</span>, <span class="number">9.22</span>, <span class="number">9.50</span>, <span class="number">9.70</span>, <span class="number">9.86</span>, <span class="number">10.00</span>, <span class="number">10.20</span>, <span class="number">10.32</span>, <span class="number">10.42</span>, <span class="number">10.50</span>, <span class="number">10.55</span>, <span class="number">10.58</span>, <span class="number">10.60</span>])</span><br><span class="line">z1 = np.polyfit(x, y, <span class="number">4</span>)<span class="comment">#3为多项式最高次幂，结果为多项式的各个系数</span></span><br><span class="line"><span class="comment">#最高次幂3，得到4个系数,从高次到低次排列</span></span><br><span class="line"><span class="comment">#最高次幂取几要视情况而定</span></span><br><span class="line">p1 = np.poly1d(z1)<span class="comment">#将系数代入方程，得到函式p1</span></span><br><span class="line">print(z1)<span class="comment">#多项式系数</span></span><br><span class="line">print(p1)<span class="comment">#多项式方程</span></span><br><span class="line">print(p1(<span class="number">18</span>))<span class="comment">#调用，输入x值，得到y</span></span><br><span class="line">x1=np.linspace(x.min(),x.max(),<span class="number">100</span>)<span class="comment">#x给定数据太少，方程曲线不光滑，多取x值得到光滑曲线</span></span><br><span class="line">pp1=p1(x1)<span class="comment">#x1代入多项式，得到pp1,代入matplotlib中画多项式曲线</span></span><br><span class="line">plt.rcParams[<span class="string">'font.sans-serif'</span>]=[<span class="string">'SimHei'</span>]<span class="comment">#显示中文</span></span><br><span class="line">plt.scatter(x,y,color=<span class="string">'g'</span>)<span class="comment">#x，y散点图</span></span><br><span class="line">plt.plot(x,y,color=<span class="string">'r'</span>)<span class="comment">#x,y线形图</span></span><br><span class="line">plt.plot(x1,pp1,color=<span class="string">'b'</span>)<span class="comment">#100个x及对应y值绘制的曲线</span></span><br><span class="line"><span class="comment">#可应用于各个行业的数值预估</span></span><br><span class="line">plt.legend(loc=<span class="string">'best'</span>)</span><br><span class="line"><span class="comment">#plt.savefig('polyfit.png',dpi=400,bbox_inches='tight')</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>[ -9.24538084e-04   3.76792011e-02  -5.54639386e-01   3.60545597e+00   1.03629808e+00]            4           3          2-0.0009245 x + 0.03768 x - 0.5546 x + 3.605 x + 1.0368.922135181</code></pre><p><img src="output_97_1.png" alt="png"></p><p>关于回归拟合的问题就说这么多，在用到的时候直接拿以上代码稍微修改一下便可使用了，更多干货请关注微信公众号“我将在南极找寻你”！</p><p>下课！</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;线性回归&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;线性回归的经典解法是最小二乘法，关于最小二乘法的原理网络上都有介绍，咱们这里只看在sklearn中的实现&lt;/p&gt;
&lt;p&gt;我们将采用sklearn自带的美国波斯顿房价数据集进行演示&lt;/p&gt;
&lt;p&gt;首先导入数据并查看数据的基本信
      
    
    </summary>
    
      <category term="Data Analysis&amp;Mining with Python" scheme="http://yoursite.com/categories/Data-Analysis-Mining-with-Python/"/>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>Python聚类分析</title>
    <link href="http://yoursite.com/2018/09/30/%E8%81%9A%E7%B1%BB/"/>
    <id>http://yoursite.com/2018/09/30/聚类/</id>
    <published>2018-09-30T10:01:36.205Z</published>
    <updated>2018-09-30T10:01:36.205Z</updated>
    
    <content type="html"><![CDATA[<ul><li>Kmean聚类</li></ul><p>以下使用的是sklearn自带的鸢尾花数据集</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  </span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##加载数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line">iris=load_iris()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.array(iris)</span><br></pre></td></tr></table></figure><pre><code>array({&apos;target_names&apos;: array([&apos;setosa&apos;, &apos;versicolor&apos;, &apos;virginica&apos;],       dtype=&apos;|S10&apos;), &apos;data&apos;: array([[ 5.1,  3.5,  1.4,  0.2],       [ 4.9,  3. ,  1.4,  0.2],       [ 4.7,  3.2,  1.3,  0.2],       [ 4.6,  3.1,  1.5,  0.2],       [ 5. ,  3.6,  1.4,  0.2],       [ 5.4,  3.9,  1.7,  0.4],       [ 4.6,  3.4,  1.4,  0.3],       [ 5. ,  3.4,  1.5,  0.2],       [ 4.4,  2.9,  1.4,  0.2],       [ 4.9,  3.1,  1.5,  0.1],       [ 5.4,  3.7,  1.5,  0.2],       [ 4.8,  3.4,  1.6,  0.2],       [ 4.8,  3. ,  1.4,  0.1],       [ 4.3,  3. ,  1.1,  0.1],       [ 5.8,  4. ,  1.2,  0.2],       [ 5.7,  4.4,  1.5,  0.4],       [ 5.4,  3.9,  1.3,  0.4],       [ 5.1,  3.5,  1.4,  0.3],       [ 5.7,  3.8,  1.7,  0.3],       [ 5.1,  3.8,  1.5,  0.3],       [ 5.4,  3.4,  1.7,  0.2],       [ 5.1,  3.7,  1.5,  0.4],       [ 4.6,  3.6,  1. ,  0.2],       [ 5.1,  3.3,  1.7,  0.5],       [ 4.8,  3.4,  1.9,  0.2],       [ 5. ,  3. ,  1.6,  0.2],       [ 5. ,  3.4,  1.6,  0.4],       [ 5.2,  3.5,  1.5,  0.2],       [ 5.2,  3.4,  1.4,  0.2],       [ 4.7,  3.2,  1.6,  0.2],       [ 4.8,  3.1,  1.6,  0.2],       [ 5.4,  3.4,  1.5,  0.4],       [ 5.2,  4.1,  1.5,  0.1],       [ 5.5,  4.2,  1.4,  0.2],       [ 4.9,  3.1,  1.5,  0.1],       [ 5. ,  3.2,  1.2,  0.2],       [ 5.5,  3.5,  1.3,  0.2],       [ 4.9,  3.1,  1.5,  0.1],       [ 4.4,  3. ,  1.3,  0.2],       [ 5.1,  3.4,  1.5,  0.2],       [ 5. ,  3.5,  1.3,  0.3],       [ 4.5,  2.3,  1.3,  0.3],       [ 4.4,  3.2,  1.3,  0.2],       [ 5. ,  3.5,  1.6,  0.6],       [ 5.1,  3.8,  1.9,  0.4],       [ 4.8,  3. ,  1.4,  0.3],       [ 5.1,  3.8,  1.6,  0.2],       [ 4.6,  3.2,  1.4,  0.2],       [ 5.3,  3.7,  1.5,  0.2],       [ 5. ,  3.3,  1.4,  0.2],       [ 7. ,  3.2,  4.7,  1.4],       [ 6.4,  3.2,  4.5,  1.5],       [ 6.9,  3.1,  4.9,  1.5],       [ 5.5,  2.3,  4. ,  1.3],       [ 6.5,  2.8,  4.6,  1.5],       [ 5.7,  2.8,  4.5,  1.3],       [ 6.3,  3.3,  4.7,  1.6],       [ 4.9,  2.4,  3.3,  1. ],       [ 6.6,  2.9,  4.6,  1.3],       [ 5.2,  2.7,  3.9,  1.4],       [ 5. ,  2. ,  3.5,  1. ],       [ 5.9,  3. ,  4.2,  1.5],       [ 6. ,  2.2,  4. ,  1. ],       [ 6.1,  2.9,  4.7,  1.4],       [ 5.6,  2.9,  3.6,  1.3],       [ 6.7,  3.1,  4.4,  1.4],       [ 5.6,  3. ,  4.5,  1.5],       [ 5.8,  2.7,  4.1,  1. ],       [ 6.2,  2.2,  4.5,  1.5],       [ 5.6,  2.5,  3.9,  1.1],       [ 5.9,  3.2,  4.8,  1.8],       [ 6.1,  2.8,  4. ,  1.3],       [ 6.3,  2.5,  4.9,  1.5],       [ 6.1,  2.8,  4.7,  1.2],       [ 6.4,  2.9,  4.3,  1.3],       [ 6.6,  3. ,  4.4,  1.4],       [ 6.8,  2.8,  4.8,  1.4],       [ 6.7,  3. ,  5. ,  1.7],       [ 6. ,  2.9,  4.5,  1.5],       [ 5.7,  2.6,  3.5,  1. ],       [ 5.5,  2.4,  3.8,  1.1],       [ 5.5,  2.4,  3.7,  1. ],       [ 5.8,  2.7,  3.9,  1.2],       [ 6. ,  2.7,  5.1,  1.6],       [ 5.4,  3. ,  4.5,  1.5],       [ 6. ,  3.4,  4.5,  1.6],       [ 6.7,  3.1,  4.7,  1.5],       [ 6.3,  2.3,  4.4,  1.3],       [ 5.6,  3. ,  4.1,  1.3],       [ 5.5,  2.5,  4. ,  1.3],       [ 5.5,  2.6,  4.4,  1.2],       [ 6.1,  3. ,  4.6,  1.4],       [ 5.8,  2.6,  4. ,  1.2],       [ 5. ,  2.3,  3.3,  1. ],       [ 5.6,  2.7,  4.2,  1.3],       [ 5.7,  3. ,  4.2,  1.2],       [ 5.7,  2.9,  4.2,  1.3],       [ 6.2,  2.9,  4.3,  1.3],       [ 5.1,  2.5,  3. ,  1.1],       [ 5.7,  2.8,  4.1,  1.3],       [ 6.3,  3.3,  6. ,  2.5],       [ 5.8,  2.7,  5.1,  1.9],       [ 7.1,  3. ,  5.9,  2.1],       [ 6.3,  2.9,  5.6,  1.8],       [ 6.5,  3. ,  5.8,  2.2],       [ 7.6,  3. ,  6.6,  2.1],       [ 4.9,  2.5,  4.5,  1.7],       [ 7.3,  2.9,  6.3,  1.8],       [ 6.7,  2.5,  5.8,  1.8],       [ 7.2,  3.6,  6.1,  2.5],       [ 6.5,  3.2,  5.1,  2. ],       [ 6.4,  2.7,  5.3,  1.9],       [ 6.8,  3. ,  5.5,  2.1],       [ 5.7,  2.5,  5. ,  2. ],       [ 5.8,  2.8,  5.1,  2.4],       [ 6.4,  3.2,  5.3,  2.3],       [ 6.5,  3. ,  5.5,  1.8],       [ 7.7,  3.8,  6.7,  2.2],       [ 7.7,  2.6,  6.9,  2.3],       [ 6. ,  2.2,  5. ,  1.5],       [ 6.9,  3.2,  5.7,  2.3],       [ 5.6,  2.8,  4.9,  2. ],       [ 7.7,  2.8,  6.7,  2. ],       [ 6.3,  2.7,  4.9,  1.8],       [ 6.7,  3.3,  5.7,  2.1],       [ 7.2,  3.2,  6. ,  1.8],       [ 6.2,  2.8,  4.8,  1.8],       [ 6.1,  3. ,  4.9,  1.8],       [ 6.4,  2.8,  5.6,  2.1],       [ 7.2,  3. ,  5.8,  1.6],       [ 7.4,  2.8,  6.1,  1.9],       [ 7.9,  3.8,  6.4,  2. ],       [ 6.4,  2.8,  5.6,  2.2],       [ 6.3,  2.8,  5.1,  1.5],       [ 6.1,  2.6,  5.6,  1.4],       [ 7.7,  3. ,  6.1,  2.3],       [ 6.3,  3.4,  5.6,  2.4],       [ 6.4,  3.1,  5.5,  1.8],       [ 6. ,  3. ,  4.8,  1.8],       [ 6.9,  3.1,  5.4,  2.1],       [ 6.7,  3.1,  5.6,  2.4],       [ 6.9,  3.1,  5.1,  2.3],       [ 5.8,  2.7,  5.1,  1.9],       [ 6.8,  3.2,  5.9,  2.3],       [ 6.7,  3.3,  5.7,  2.5],       [ 6.7,  3. ,  5.2,  2.3],       [ 6.3,  2.5,  5. ,  1.9],       [ 6.5,  3. ,  5.2,  2. ],       [ 6.2,  3.4,  5.4,  2.3],       [ 5.9,  3. ,  5.1,  1.8]]), &apos;target&apos;: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,       1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), &apos;DESCR&apos;: &apos;Iris Plants Database\n====================\n\nNotes\n-----\nData Set Characteristics:\n    :Number of Instances: 150 (50 in each of three classes)\n    :Number of Attributes: 4 numeric, predictive attributes and the class\n    :Attribute Information:\n        - sepal length in cm\n        - sepal width in cm\n        - petal length in cm\n        - petal width in cm\n        - class:\n                - Iris-Setosa\n                - Iris-Versicolour\n                - Iris-Virginica\n    :Summary Statistics:\n\n    ============== ==== ==== ======= ===== ====================\n                    Min  Max   Mean    SD   Class Correlation\n    ============== ==== ==== ======= ===== ====================\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n    petal width:    0.1  2.5   1.20  0.76     0.9565  (high!)\n    ============== ==== ==== ======= ===== ====================\n\n    :Missing Attribute Values: None\n    :Class Distribution: 33.3% for each of 3 classes.\n    :Creator: R.A. Fisher\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n    :Date: July, 1988\n\nThis is a copy of UCI ML iris datasets.\nhttp://archive.ics.uci.edu/ml/datasets/Iris\n\nThe famous Iris database, first used by Sir R.A Fisher\n\nThis is perhaps the best known database to be found in the\npattern recognition literature.  Fisher\&apos;s paper is a classic in the field and\nis referenced frequently to this day.  (See Duda &amp; Hart, for example.)  The\ndata set contains 3 classes of 50 instances each, where each class refers to a\ntype of iris plant.  One class is linearly separable from the other 2; the\nlatter are NOT linearly separable from each other.\n\nReferences\n----------\n   - Fisher,R.A. &quot;The use of multiple measurements in taxonomic problems&quot;\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in &quot;Contributions to\n     Mathematical Statistics&quot; (John Wiley, NY, 1950).\n   - Duda,R.O., &amp; Hart,P.E. (1973) Pattern Classification and Scene Analysis.\n     (Q327.D83) John Wiley &amp; Sons.  ISBN 0-471-22361-1.  See page 218.\n   - Dasarathy, B.V. (1980) &quot;Nosing Around the Neighborhood: A New System\n     Structure and Classification Rule for Recognition in Partially Exposed\n     Environments&quot;.  IEEE Transactions on Pattern Analysis and Machine\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n   - Gates, G.W. (1972) &quot;The Reduced Nearest Neighbor Rule&quot;.  IEEE Transactions\n     on Information Theory, May 1972, 431-433.\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al&quot;s AUTOCLASS II\n     conceptual clustering system finds 3 classes in the data.\n   - Many, many more ...\n&apos;, &apos;feature_names&apos;: [&apos;sepal length (cm)&apos;, &apos;sepal width (cm)&apos;, &apos;petal length (cm)&apos;, &apos;petal width (cm)&apos;]}, dtype=object)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X = iris.data[:, <span class="number">2</span>:<span class="number">4</span>] <span class="comment">##表示我们只取特征空间中的后两个维度</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X</span><br></pre></td></tr></table></figure><pre><code>array([[ 1.4,  0.2],       [ 1.4,  0.2],       [ 1.3,  0.2],       [ 1.5,  0.2],       [ 1.4,  0.2],       [ 1.7,  0.4],       [ 1.4,  0.3],       [ 1.5,  0.2],       [ 1.4,  0.2],       [ 1.5,  0.1],       [ 1.5,  0.2],       [ 1.6,  0.2],       [ 1.4,  0.1],       [ 1.1,  0.1],       [ 1.2,  0.2],       [ 1.5,  0.4],       [ 1.3,  0.4],       [ 1.4,  0.3],       [ 1.7,  0.3],       [ 1.5,  0.3],       [ 1.7,  0.2],       [ 1.5,  0.4],       [ 1. ,  0.2],       [ 1.7,  0.5],       [ 1.9,  0.2],       [ 1.6,  0.2],       [ 1.6,  0.4],       [ 1.5,  0.2],       [ 1.4,  0.2],       [ 1.6,  0.2],       [ 1.6,  0.2],       [ 1.5,  0.4],       [ 1.5,  0.1],       [ 1.4,  0.2],       [ 1.5,  0.1],       [ 1.2,  0.2],       [ 1.3,  0.2],       [ 1.5,  0.1],       [ 1.3,  0.2],       [ 1.5,  0.2],       [ 1.3,  0.3],       [ 1.3,  0.3],       [ 1.3,  0.2],       [ 1.6,  0.6],       [ 1.9,  0.4],       [ 1.4,  0.3],       [ 1.6,  0.2],       [ 1.4,  0.2],       [ 1.5,  0.2],       [ 1.4,  0.2],       [ 4.7,  1.4],       [ 4.5,  1.5],       [ 4.9,  1.5],       [ 4. ,  1.3],       [ 4.6,  1.5],       [ 4.5,  1.3],       [ 4.7,  1.6],       [ 3.3,  1. ],       [ 4.6,  1.3],       [ 3.9,  1.4],       [ 3.5,  1. ],       [ 4.2,  1.5],       [ 4. ,  1. ],       [ 4.7,  1.4],       [ 3.6,  1.3],       [ 4.4,  1.4],       [ 4.5,  1.5],       [ 4.1,  1. ],       [ 4.5,  1.5],       [ 3.9,  1.1],       [ 4.8,  1.8],       [ 4. ,  1.3],       [ 4.9,  1.5],       [ 4.7,  1.2],       [ 4.3,  1.3],       [ 4.4,  1.4],       [ 4.8,  1.4],       [ 5. ,  1.7],       [ 4.5,  1.5],       [ 3.5,  1. ],       [ 3.8,  1.1],       [ 3.7,  1. ],       [ 3.9,  1.2],       [ 5.1,  1.6],       [ 4.5,  1.5],       [ 4.5,  1.6],       [ 4.7,  1.5],       [ 4.4,  1.3],       [ 4.1,  1.3],       [ 4. ,  1.3],       [ 4.4,  1.2],       [ 4.6,  1.4],       [ 4. ,  1.2],       [ 3.3,  1. ],       [ 4.2,  1.3],       [ 4.2,  1.2],       [ 4.2,  1.3],       [ 4.3,  1.3],       [ 3. ,  1.1],       [ 4.1,  1.3],       [ 6. ,  2.5],       [ 5.1,  1.9],       [ 5.9,  2.1],       [ 5.6,  1.8],       [ 5.8,  2.2],       [ 6.6,  2.1],       [ 4.5,  1.7],       [ 6.3,  1.8],       [ 5.8,  1.8],       [ 6.1,  2.5],       [ 5.1,  2. ],       [ 5.3,  1.9],       [ 5.5,  2.1],       [ 5. ,  2. ],       [ 5.1,  2.4],       [ 5.3,  2.3],       [ 5.5,  1.8],       [ 6.7,  2.2],       [ 6.9,  2.3],       [ 5. ,  1.5],       [ 5.7,  2.3],       [ 4.9,  2. ],       [ 6.7,  2. ],       [ 4.9,  1.8],       [ 5.7,  2.1],       [ 6. ,  1.8],       [ 4.8,  1.8],       [ 4.9,  1.8],       [ 5.6,  2.1],       [ 5.8,  1.6],       [ 6.1,  1.9],       [ 6.4,  2. ],       [ 5.6,  2.2],       [ 5.1,  1.5],       [ 5.6,  1.4],       [ 6.1,  2.3],       [ 5.6,  2.4],       [ 5.5,  1.8],       [ 4.8,  1.8],       [ 5.4,  2.1],       [ 5.6,  2.4],       [ 5.1,  2.3],       [ 5.1,  1.9],       [ 5.9,  2.3],       [ 5.7,  2.5],       [ 5.2,  2.3],       [ 5. ,  1.9],       [ 5.2,  2. ],       [ 5.4,  2.3],       [ 5.1,  1.8]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#绘制数据分布图</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c = <span class="string">"green"</span>, marker=<span class="string">'*'</span>, label=<span class="string">'point'</span>)  </span><br><span class="line">plt.xlabel(<span class="string">'petal length'</span>)  </span><br><span class="line">plt.ylabel(<span class="string">'petal width'</span>)  </span><br><span class="line">plt.legend(loc=<span class="number">2</span>)  </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_9_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">estimator = KMeans(n_clusters=<span class="number">3</span>)<span class="comment">#构造聚类器</span></span><br><span class="line">estimator.fit(X)<span class="comment">#聚类</span></span><br><span class="line">label_pred = estimator.labels_ <span class="comment">#获取聚类标签</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">label_pred</span><br></pre></td></tr></table></figure><pre><code>array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,       0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,       2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2,       2, 2, 2, 2, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1,       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,       2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#绘制k-means结果</span></span><br><span class="line">x0 = X[label_pred == <span class="number">0</span>]</span><br><span class="line">x1 = X[label_pred == <span class="number">1</span>]</span><br><span class="line">x2 = X[label_pred == <span class="number">2</span>]</span><br><span class="line">plt.scatter(x0[:, <span class="number">0</span>], x0[:, <span class="number">1</span>], c = <span class="string">"red"</span>, marker=<span class="string">'o'</span>, label=<span class="string">'label0'</span>)  </span><br><span class="line">plt.scatter(x1[:, <span class="number">0</span>], x1[:, <span class="number">1</span>], c = <span class="string">"green"</span>, marker=<span class="string">'*'</span>, label=<span class="string">'label1'</span>)  </span><br><span class="line">plt.scatter(x2[:, <span class="number">0</span>], x2[:, <span class="number">1</span>], c = <span class="string">"blue"</span>, marker=<span class="string">'+'</span>, label=<span class="string">'label2'</span>)  </span><br><span class="line">plt.xlabel(<span class="string">'petal length'</span>)  </span><br><span class="line">plt.ylabel(<span class="string">'petal width'</span>)  </span><br><span class="line">plt.legend(loc=<span class="number">2</span>)  </span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p><img src="output_12_0.png" alt="png"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X.shape</span><br></pre></td></tr></table></figure><pre><code>(150, 2)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X=X.tolist()</span><br><span class="line">label_pred=label_pred.tolist()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X</span><br></pre></td></tr></table></figure><pre><code>[[1.4, 0.2], [1.4, 0.2], [1.3, 0.2], [1.5, 0.2], [1.4, 0.2], [1.7, 0.4], [1.4, 0.3], [1.5, 0.2], [1.4, 0.2], [1.5, 0.1], [1.5, 0.2], [1.6, 0.2], [1.4, 0.1], [1.1, 0.1], [1.2, 0.2], [1.5, 0.4], [1.3, 0.4], [1.4, 0.3], [1.7, 0.3], [1.5, 0.3], [1.7, 0.2], [1.5, 0.4], [1.0, 0.2], [1.7, 0.5], [1.9, 0.2], [1.6, 0.2], [1.6, 0.4], [1.5, 0.2], [1.4, 0.2], [1.6, 0.2], [1.6, 0.2], [1.5, 0.4], [1.5, 0.1], [1.4, 0.2], [1.5, 0.1], [1.2, 0.2], [1.3, 0.2], [1.5, 0.1], [1.3, 0.2], [1.5, 0.2], [1.3, 0.3], [1.3, 0.3], [1.3, 0.2], [1.6, 0.6], [1.9, 0.4], [1.4, 0.3], [1.6, 0.2], [1.4, 0.2], [1.5, 0.2], [1.4, 0.2], [4.7, 1.4], [4.5, 1.5], [4.9, 1.5], [4.0, 1.3], [4.6, 1.5], [4.5, 1.3], [4.7, 1.6], [3.3, 1.0], [4.6, 1.3], [3.9, 1.4], [3.5, 1.0], [4.2, 1.5], [4.0, 1.0], [4.7, 1.4], [3.6, 1.3], [4.4, 1.4], [4.5, 1.5], [4.1, 1.0], [4.5, 1.5], [3.9, 1.1], [4.8, 1.8], [4.0, 1.3], [4.9, 1.5], [4.7, 1.2], [4.3, 1.3], [4.4, 1.4], [4.8, 1.4], [5.0, 1.7], [4.5, 1.5], [3.5, 1.0], [3.8, 1.1], [3.7, 1.0], [3.9, 1.2], [5.1, 1.6], [4.5, 1.5], [4.5, 1.6], [4.7, 1.5], [4.4, 1.3], [4.1, 1.3], [4.0, 1.3], [4.4, 1.2], [4.6, 1.4], [4.0, 1.2], [3.3, 1.0], [4.2, 1.3], [4.2, 1.2], [4.2, 1.3], [4.3, 1.3], [3.0, 1.1], [4.1, 1.3], [6.0, 2.5], [5.1, 1.9], [5.9, 2.1], [5.6, 1.8], [5.8, 2.2], [6.6, 2.1], [4.5, 1.7], [6.3, 1.8], [5.8, 1.8], [6.1, 2.5], [5.1, 2.0], [5.3, 1.9], [5.5, 2.1], [5.0, 2.0], [5.1, 2.4], [5.3, 2.3], [5.5, 1.8], [6.7, 2.2], [6.9, 2.3], [5.0, 1.5], [5.7, 2.3], [4.9, 2.0], [6.7, 2.0], [4.9, 1.8], [5.7, 2.1], [6.0, 1.8], [4.8, 1.8], [4.9, 1.8], [5.6, 2.1], [5.8, 1.6], [6.1, 1.9], [6.4, 2.0], [5.6, 2.2], [5.1, 1.5], [5.6, 1.4], [6.1, 2.3], [5.6, 2.4], [5.5, 1.8], [4.8, 1.8], [5.4, 2.1], [5.6, 2.4], [5.1, 2.3], [5.1, 1.9], [5.9, 2.3], [5.7, 2.5], [5.2, 2.3], [5.0, 1.9], [5.2, 2.0], [5.4, 2.3], [5.1, 1.8]]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cluster_result=[]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> zip(X,label_pred):</span><br><span class="line">    i[<span class="number">0</span>].append(i[<span class="number">1</span>])</span><br><span class="line">    cluster_result.append(i[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cluster_result</span><br></pre></td></tr></table></figure><pre><code>[[1.4, 0.2, 0], [1.4, 0.2, 0], [1.3, 0.2, 0], [1.5, 0.2, 0], [1.4, 0.2, 0], [1.7, 0.4, 0], [1.4, 0.3, 0], [1.5, 0.2, 0], [1.4, 0.2, 0], [1.5, 0.1, 0], [1.5, 0.2, 0], [1.6, 0.2, 0], [1.4, 0.1, 0], [1.1, 0.1, 0], [1.2, 0.2, 0], [1.5, 0.4, 0], [1.3, 0.4, 0], [1.4, 0.3, 0], [1.7, 0.3, 0], [1.5, 0.3, 0], [1.7, 0.2, 0], [1.5, 0.4, 0], [1.0, 0.2, 0], [1.7, 0.5, 0], [1.9, 0.2, 0], [1.6, 0.2, 0], [1.6, 0.4, 0], [1.5, 0.2, 0], [1.4, 0.2, 0], [1.6, 0.2, 0], [1.6, 0.2, 0], [1.5, 0.4, 0], [1.5, 0.1, 0], [1.4, 0.2, 0], [1.5, 0.1, 0], [1.2, 0.2, 0], [1.3, 0.2, 0], [1.5, 0.1, 0], [1.3, 0.2, 0], [1.5, 0.2, 0], [1.3, 0.3, 0], [1.3, 0.3, 0], [1.3, 0.2, 0], [1.6, 0.6, 0], [1.9, 0.4, 0], [1.4, 0.3, 0], [1.6, 0.2, 0], [1.4, 0.2, 0], [1.5, 0.2, 0], [1.4, 0.2, 0], [4.7, 1.4, 2], [4.5, 1.5, 2], [4.9, 1.5, 2], [4.0, 1.3, 2], [4.6, 1.5, 2], [4.5, 1.3, 2], [4.7, 1.6, 2], [3.3, 1.0, 2], [4.6, 1.3, 2], [3.9, 1.4, 2], [3.5, 1.0, 2], [4.2, 1.5, 2], [4.0, 1.0, 2], [4.7, 1.4, 2], [3.6, 1.3, 2], [4.4, 1.4, 2], [4.5, 1.5, 2], [4.1, 1.0, 2], [4.5, 1.5, 2], [3.9, 1.1, 2], [4.8, 1.8, 2], [4.0, 1.3, 2], [4.9, 1.5, 2], [4.7, 1.2, 2], [4.3, 1.3, 2], [4.4, 1.4, 2], [4.8, 1.4, 2], [5.0, 1.7, 1], [4.5, 1.5, 2], [3.5, 1.0, 2], [3.8, 1.1, 2], [3.7, 1.0, 2], [3.9, 1.2, 2], [5.1, 1.6, 1], [4.5, 1.5, 2], [4.5, 1.6, 2], [4.7, 1.5, 2], [4.4, 1.3, 2], [4.1, 1.3, 2], [4.0, 1.3, 2], [4.4, 1.2, 2], [4.6, 1.4, 2], [4.0, 1.2, 2], [3.3, 1.0, 2], [4.2, 1.3, 2], [4.2, 1.2, 2], [4.2, 1.3, 2], [4.3, 1.3, 2], [3.0, 1.1, 2], [4.1, 1.3, 2], [6.0, 2.5, 1], [5.1, 1.9, 1], [5.9, 2.1, 1], [5.6, 1.8, 1], [5.8, 2.2, 1], [6.6, 2.1, 1], [4.5, 1.7, 2], [6.3, 1.8, 1], [5.8, 1.8, 1], [6.1, 2.5, 1], [5.1, 2.0, 1], [5.3, 1.9, 1], [5.5, 2.1, 1], [5.0, 2.0, 1], [5.1, 2.4, 1], [5.3, 2.3, 1], [5.5, 1.8, 1], [6.7, 2.2, 1], [6.9, 2.3, 1], [5.0, 1.5, 2], [5.7, 2.3, 1], [4.9, 2.0, 1], [6.7, 2.0, 1], [4.9, 1.8, 1], [5.7, 2.1, 1], [6.0, 1.8, 1], [4.8, 1.8, 2], [4.9, 1.8, 1], [5.6, 2.1, 1], [5.8, 1.6, 1], [6.1, 1.9, 1], [6.4, 2.0, 1], [5.6, 2.2, 1], [5.1, 1.5, 1], [5.6, 1.4, 1], [6.1, 2.3, 1], [5.6, 2.4, 1], [5.5, 1.8, 1], [4.8, 1.8, 2], [5.4, 2.1, 1], [5.6, 2.4, 1], [5.1, 2.3, 1], [5.1, 1.9, 1], [5.9, 2.3, 1], [5.7, 2.5, 1], [5.2, 2.3, 1], [5.0, 1.9, 1], [5.2, 2.0, 1], [5.4, 2.3, 1], [5.1, 1.8, 1]]</code></pre><p>接下来将３类数据点分别导出到csv文件</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#分类整合</span></span><br><span class="line">label0=[]<span class="comment">#第０类</span></span><br><span class="line">label1=[]<span class="comment">#第１类</span></span><br><span class="line">label2=[]<span class="comment">##第2类</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> cluster_result:</span><br><span class="line">    <span class="keyword">if</span> i[<span class="number">2</span>]==<span class="number">0</span>:</span><br><span class="line">        label0.append(i)</span><br><span class="line">    <span class="keyword">elif</span> i[<span class="number">2</span>]==<span class="number">1</span>:</span><br><span class="line">        label1.append(i)</span><br><span class="line">    <span class="keyword">elif</span> i[<span class="number">2</span>]==<span class="number">2</span>:</span><br><span class="line">        label2.append(i)</span><br></pre></td></tr></table></figure><p>现在得到的是３个list，我们将先把list转换成array，再进行导出</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#转化成array</span></span><br><span class="line">label0=np.array(label0)</span><br><span class="line">label1=np.array(label1)</span><br><span class="line">label2=np.array(label2)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#预先创建一个空的数据框</span></span><br><span class="line">label0_csv=pd.DataFrame()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#将第０类样本信息进行填充到之前的看破那个数据框</span></span><br><span class="line">label0_csv[<span class="string">'feature1'</span>]=label0[:,<span class="number">0</span>]</span><br><span class="line">label0_csv[<span class="string">'feature2'</span>]=label0[:,<span class="number">1</span>]</span><br><span class="line">label0_csv[<span class="string">'kind'</span>]=label0[:,<span class="number">2</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#整合之后的样子</span></span><br><span class="line">label0_csv</span><br></pre></td></tr></table></figure><div><br><table border="1" class="dataframe"><br>  <thead><br>    <tr style="text-align: right;"><br>      <th></th><br>      <th>feature1</th><br>      <th>feature2</th><br>      <th>kind</th><br>    </tr><br>  </thead><br>  <tbody><br>    <tr><br>      <th>0</th><br>      <td>1.4</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>1</th><br>      <td>1.4</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>2</th><br>      <td>1.3</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>3</th><br>      <td>1.5</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>4</th><br>      <td>1.4</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>5</th><br>      <td>1.7</td><br>      <td>0.4</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>6</th><br>      <td>1.4</td><br>      <td>0.3</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>7</th><br>      <td>1.5</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>8</th><br>      <td>1.4</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>9</th><br>      <td>1.5</td><br>      <td>0.1</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>10</th><br>      <td>1.5</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>11</th><br>      <td>1.6</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>12</th><br>      <td>1.4</td><br>      <td>0.1</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>13</th><br>      <td>1.1</td><br>      <td>0.1</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>14</th><br>      <td>1.2</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>15</th><br>      <td>1.5</td><br>      <td>0.4</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>16</th><br>      <td>1.3</td><br>      <td>0.4</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>17</th><br>      <td>1.4</td><br>      <td>0.3</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>18</th><br>      <td>1.7</td><br>      <td>0.3</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>19</th><br>      <td>1.5</td><br>      <td>0.3</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>20</th><br>      <td>1.7</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>21</th><br>      <td>1.5</td><br>      <td>0.4</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>22</th><br>      <td>1.0</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>23</th><br>      <td>1.7</td><br>      <td>0.5</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>24</th><br>      <td>1.9</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>25</th><br>      <td>1.6</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>26</th><br>      <td>1.6</td><br>      <td>0.4</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>27</th><br>      <td>1.5</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>28</th><br>      <td>1.4</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>29</th><br>      <td>1.6</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>30</th><br>      <td>1.6</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>31</th><br>      <td>1.5</td><br>      <td>0.4</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>32</th><br>      <td>1.5</td><br>      <td>0.1</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>33</th><br>      <td>1.4</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>34</th><br>      <td>1.5</td><br>      <td>0.1</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>35</th><br>      <td>1.2</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>36</th><br>      <td>1.3</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>37</th><br>      <td>1.5</td><br>      <td>0.1</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>38</th><br>      <td>1.3</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>39</th><br>      <td>1.5</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>40</th><br>      <td>1.3</td><br>      <td>0.3</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>41</th><br>      <td>1.3</td><br>      <td>0.3</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>42</th><br>      <td>1.3</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>43</th><br>      <td>1.6</td><br>      <td>0.6</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>44</th><br>      <td>1.9</td><br>      <td>0.4</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>45</th><br>      <td>1.4</td><br>      <td>0.3</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>46</th><br>      <td>1.6</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>47</th><br>      <td>1.4</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>48</th><br>      <td>1.5</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>    <tr><br>      <th>49</th><br>      <td>1.4</td><br>      <td>0.2</td><br>      <td>0.0</td><br>    </tr><br>  </tbody><br></table><br></div><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#保存第０类样本信息文件</span></span><br><span class="line">label0_csv.to_csv(<span class="string">r'/home/fantasy/Desktop/数学建模Python/聚类/鸢尾花聚类结果csv/label0.csv'</span>,index=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><p>至于其他两类的数据导出方法也一样，这里不再赘述。<br>其实，可以把这个导出功能封装成一个函数，传入保存路径和第几类就可以了，当然也可以直接来个for循环解决.</p><p>到现在，我们都做了些什么呢？来总结一下：</p><p>首先，我们导入了sklearn自带的鸢尾花数据集并选取了其中两个特征(feature)，拟用这两个特征做聚类.</p><p>接着，我们调用了sklearn的聚类方法做了聚类（聚成了３类），并将样本的特征与所属类别（int）整合在一个list里面，并由外围的list包裹住，然后再将这所有的list按照所属聚类数的不同而归类存储.</p><p>最后，将归类的数据先转化成数组形式，然后做成csv文件，导出到指定目录下.</p><p>有一点值得注意的是，在可视化的时候只能用二维数据，即两个特征，受维度限制.</p><hr><p>接下来我们再来看一个例子，同样是使用上面的数据，只不过这次采用dbscan算法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> DBSCAN</span><br><span class="line">db=DBSCAN(eps=<span class="number">1</span>,min_samples=<span class="number">10</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#这次使用全部特征进行聚类</span></span><br><span class="line">x=iris.data</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x.shape</span><br></pre></td></tr></table></figure><pre><code>(150, 4)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">db.fit(x)<span class="comment">#训练数据集，构建模型</span></span><br></pre></td></tr></table></figure><pre><code>DBSCAN(algorithm=&apos;auto&apos;, eps=1, leaf_size=30, metric=&apos;euclidean&apos;,    min_samples=10, n_jobs=1, p=None)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">labels=db.labels_</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">labels</span><br></pre></td></tr></table></figure><pre><code>array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,       0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#噪声比率</span></span><br><span class="line">ratio=len(labels[labels[:]==<span class="number">-1</span>])*<span class="number">1.0</span>/len(labels)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> <span class="string">"噪声比率:"</span>,ratio</span><br></pre></td></tr></table></figure><pre><code>噪声比率: 0.0</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">n_clusters_=len(set(labels)) - (<span class="number">1</span> <span class="keyword">if</span> <span class="number">-1</span> <span class="keyword">in</span> labels <span class="keyword">else</span> <span class="number">0</span>)</span><br><span class="line"><span class="keyword">print</span> <span class="string">'聚类总数为：'</span>,n_clusters_</span><br></pre></td></tr></table></figure><pre><code>聚类总数为： 2</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="keyword">print</span> <span class="string">'聚类效果评价指标：'</span>,metrics.silhouette_score(X,labels)<span class="comment">#【-1,1】,越接近１越好</span></span><br></pre></td></tr></table></figure><pre><code>聚类效果评价指标： 0.766723428068</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#总结</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_clusters_):</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'Cluter'</span>,i+<span class="number">1</span>,<span class="string">':'</span></span><br><span class="line">    count=len(x[labels==i])</span><br><span class="line">    mean=np.mean(x[labels==i][:,<span class="number">1</span>])</span><br><span class="line">    std=np.std(x[labels==i][:,<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'计数：'</span>,count</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'平均值'</span>,mean</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'标准差'</span>,std</span><br></pre></td></tr></table></figure><pre><code>Cluter 1 :计数： 50平均值 3.418标准差 0.377194909828Cluter 2 :计数： 100平均值 2.872标准差 0.331083071147</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#可视化聚类结果，这里只选取前两个进行绘制,不太准确，只是拿来说明一下绘图做法</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n_clusters_):</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'簇 '</span>, i, <span class="string">'的所有样本:'</span></span><br><span class="line">    one_cluster = x[labels == i]</span><br><span class="line">    <span class="keyword">print</span> one_cluster</span><br><span class="line">    plt.plot(one_cluster[:,<span class="number">0</span>],one_cluster[:,<span class="number">1</span>],<span class="string">'o'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><pre><code>簇  0 的所有样本:[[ 5.1  3.5  1.4  0.2] [ 4.9  3.   1.4  0.2] [ 4.7  3.2  1.3  0.2] [ 4.6  3.1  1.5  0.2] [ 5.   3.6  1.4  0.2] [ 5.4  3.9  1.7  0.4] [ 4.6  3.4  1.4  0.3] [ 5.   3.4  1.5  0.2] [ 4.4  2.9  1.4  0.2] [ 4.9  3.1  1.5  0.1] [ 5.4  3.7  1.5  0.2] [ 4.8  3.4  1.6  0.2] [ 4.8  3.   1.4  0.1] [ 4.3  3.   1.1  0.1] [ 5.8  4.   1.2  0.2] [ 5.7  4.4  1.5  0.4] [ 5.4  3.9  1.3  0.4] [ 5.1  3.5  1.4  0.3] [ 5.7  3.8  1.7  0.3] [ 5.1  3.8  1.5  0.3] [ 5.4  3.4  1.7  0.2] [ 5.1  3.7  1.5  0.4] [ 4.6  3.6  1.   0.2] [ 5.1  3.3  1.7  0.5] [ 4.8  3.4  1.9  0.2] [ 5.   3.   1.6  0.2] [ 5.   3.4  1.6  0.4] [ 5.2  3.5  1.5  0.2] [ 5.2  3.4  1.4  0.2] [ 4.7  3.2  1.6  0.2] [ 4.8  3.1  1.6  0.2] [ 5.4  3.4  1.5  0.4] [ 5.2  4.1  1.5  0.1] [ 5.5  4.2  1.4  0.2] [ 4.9  3.1  1.5  0.1] [ 5.   3.2  1.2  0.2] [ 5.5  3.5  1.3  0.2] [ 4.9  3.1  1.5  0.1] [ 4.4  3.   1.3  0.2] [ 5.1  3.4  1.5  0.2] [ 5.   3.5  1.3  0.3] [ 4.5  2.3  1.3  0.3] [ 4.4  3.2  1.3  0.2] [ 5.   3.5  1.6  0.6] [ 5.1  3.8  1.9  0.4] [ 4.8  3.   1.4  0.3] [ 5.1  3.8  1.6  0.2] [ 4.6  3.2  1.4  0.2] [ 5.3  3.7  1.5  0.2] [ 5.   3.3  1.4  0.2]]簇  1 的所有样本:[[ 7.   3.2  4.7  1.4] [ 6.4  3.2  4.5  1.5] [ 6.9  3.1  4.9  1.5] [ 5.5  2.3  4.   1.3] [ 6.5  2.8  4.6  1.5] [ 5.7  2.8  4.5  1.3] [ 6.3  3.3  4.7  1.6] [ 4.9  2.4  3.3  1. ] [ 6.6  2.9  4.6  1.3] [ 5.2  2.7  3.9  1.4] [ 5.   2.   3.5  1. ] [ 5.9  3.   4.2  1.5] [ 6.   2.2  4.   1. ] [ 6.1  2.9  4.7  1.4] [ 5.6  2.9  3.6  1.3] [ 6.7  3.1  4.4  1.4] [ 5.6  3.   4.5  1.5] [ 5.8  2.7  4.1  1. ] [ 6.2  2.2  4.5  1.5] [ 5.6  2.5  3.9  1.1] [ 5.9  3.2  4.8  1.8] [ 6.1  2.8  4.   1.3] [ 6.3  2.5  4.9  1.5] [ 6.1  2.8  4.7  1.2] [ 6.4  2.9  4.3  1.3] [ 6.6  3.   4.4  1.4] [ 6.8  2.8  4.8  1.4] [ 6.7  3.   5.   1.7] [ 6.   2.9  4.5  1.5] [ 5.7  2.6  3.5  1. ] [ 5.5  2.4  3.8  1.1] [ 5.5  2.4  3.7  1. ] [ 5.8  2.7  3.9  1.2] [ 6.   2.7  5.1  1.6] [ 5.4  3.   4.5  1.5] [ 6.   3.4  4.5  1.6] [ 6.7  3.1  4.7  1.5] [ 6.3  2.3  4.4  1.3] [ 5.6  3.   4.1  1.3] [ 5.5  2.5  4.   1.3] [ 5.5  2.6  4.4  1.2] [ 6.1  3.   4.6  1.4] [ 5.8  2.6  4.   1.2] [ 5.   2.3  3.3  1. ] [ 5.6  2.7  4.2  1.3] [ 5.7  3.   4.2  1.2] [ 5.7  2.9  4.2  1.3] [ 6.2  2.9  4.3  1.3] [ 5.1  2.5  3.   1.1] [ 5.7  2.8  4.1  1.3] [ 6.3  3.3  6.   2.5] [ 5.8  2.7  5.1  1.9] [ 7.1  3.   5.9  2.1] [ 6.3  2.9  5.6  1.8] [ 6.5  3.   5.8  2.2] [ 7.6  3.   6.6  2.1] [ 4.9  2.5  4.5  1.7] [ 7.3  2.9  6.3  1.8] [ 6.7  2.5  5.8  1.8] [ 7.2  3.6  6.1  2.5] [ 6.5  3.2  5.1  2. ] [ 6.4  2.7  5.3  1.9] [ 6.8  3.   5.5  2.1] [ 5.7  2.5  5.   2. ] [ 5.8  2.8  5.1  2.4] [ 6.4  3.2  5.3  2.3] [ 6.5  3.   5.5  1.8] [ 7.7  3.8  6.7  2.2] [ 7.7  2.6  6.9  2.3] [ 6.   2.2  5.   1.5] [ 6.9  3.2  5.7  2.3] [ 5.6  2.8  4.9  2. ] [ 7.7  2.8  6.7  2. ] [ 6.3  2.7  4.9  1.8] [ 6.7  3.3  5.7  2.1] [ 7.2  3.2  6.   1.8] [ 6.2  2.8  4.8  1.8] [ 6.1  3.   4.9  1.8] [ 6.4  2.8  5.6  2.1] [ 7.2  3.   5.8  1.6] [ 7.4  2.8  6.1  1.9] [ 7.9  3.8  6.4  2. ] [ 6.4  2.8  5.6  2.2] [ 6.3  2.8  5.1  1.5] [ 6.1  2.6  5.6  1.4] [ 7.7  3.   6.1  2.3] [ 6.3  3.4  5.6  2.4] [ 6.4  3.1  5.5  1.8] [ 6.   3.   4.8  1.8] [ 6.9  3.1  5.4  2.1] [ 6.7  3.1  5.6  2.4] [ 6.9  3.1  5.1  2.3] [ 5.8  2.7  5.1  1.9] [ 6.8  3.2  5.9  2.3] [ 6.7  3.3  5.7  2.5] [ 6.7  3.   5.2  2.3] [ 6.3  2.5  5.   1.9] [ 6.5  3.   5.2  2. ] [ 6.2  3.4  5.4  2.3] [ 5.9  3.   5.1  1.8]]</code></pre><p><img src="output_45_1.png" alt="png"></p><p>参考：<br><a href="https://blog.csdn.net/luanpeng825485697/article/details/79443512" target="_blank" rel="noopener">https://blog.csdn.net/luanpeng825485697/article/details/79443512</a><br><a href="https://blog.csdn.net/linzch3/article/details/76038172" target="_blank" rel="noopener">https://blog.csdn.net/linzch3/article/details/76038172</a><br><a href="https://blog.csdn.net/u010159842/article/details/78624135" target="_blank" rel="noopener">https://blog.csdn.net/u010159842/article/details/78624135</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;Kmean聚类&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;以下使用的是sklearn自带的鸢尾花数据集&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;l
      
    
    </summary>
    
      <category term="Data Analysis&amp;Mining with Python" scheme="http://yoursite.com/categories/Data-Analysis-Mining-with-Python/"/>
    
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
</feed>
